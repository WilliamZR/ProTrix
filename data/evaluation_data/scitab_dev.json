[
  {
    "id": "068fc4e5-d40c-4788-a5a7-bad858cbd7c7",
    "input": "## Claim\nHere is a claim: The models using BoC outperform models using BoW as well as ASM features. Does the following context support or refute the claim?\n\n## Table\nPaper title: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data\nTable caption: Table 1: Performance of supervised learning models with different features.\nFeature | LR P | LR R | LR F1 | SVM P | SVM R | SVM F1 | ANN P | ANN R | ANN F1\n+BoW | 0.93 | 0.91 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+BoC (Wiki-PubMed-PMC) | 0.94 | 0.92 | [BOLD] 0.93 | 0.94 | 0.92 | [BOLD] 0.93 | 0.91 | 0.91 | [BOLD] 0.91\n+BoC (GloVe) | 0.93 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+ASM | 0.90 | 0.85 | 0.88 | 0.90 | 0.86 | 0.88 | 0.89 | 0.89 | 0.89\n+Sentence Embeddings(SEs) | 0.89 | 0.89 | 0.89 | 0.90 | 0.86 | 0.88 | 0.88 | 0.88 | 0.88\n+BoC(Wiki-PubMed-PMC)+SEs | 0.92 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "4ab2d01c-b861-49be-9fd3-b87ffdc7ac20",
    "input": "## Claim\nHere is a claim: [CONTINUE] OD significantly outperforms OD-parse: We observe that compared to OD-parse, OD is much more accurate. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.\n[EMPTY] | Difference Function | Seanad Abolition | Video Games | Pornography\nOD-parse | Absolute | 0.01 | -0.01 | 0.07\nOD-parse | JS div. | 0.01 | -0.01 | -0.01\nOD-parse | EMD | 0.07 | 0.01 | -0.01\nOD | Absolute | [BOLD] 0.54 | [BOLD] 0.56 | [BOLD] 0.41\nOD | JS div. | 0.07 | -0.01 | -0.02\nOD | EMD | 0.26 | -0.01 | 0.01\nOD (no polarity shifters) | Absolute | 0.23 | 0.08 | 0.04\nOD (no polarity shifters) | JS div. | 0.09 | -0.01 | -0.02\nOD (no polarity shifters) | EMD | 0.10 | 0.01 | -0.01\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5315f378-f575-4846-80da-cacca49cb54a",
    "input": "## Claim\nHere is a claim: Table 4: Comparison of per-document accuracy (% ) by different systems for top 1, 3 and 5 words of abstractive sentences. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "cc5ee69b-76a7-4812-b9f9-9bfbdb654101",
    "input": "## Claim\nHere is a claim: The UnsupEmb baseline performs rather poorly on both POS and SEM tagging. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks\nTable caption: Table 2: POS and SEM tagging accuracy with baselines and an upper bound. MFT: most frequent tag; UnsupEmb: classifier using unsupervised word embeddings; Word2Tag: upper bound encoder-decoder.\n[EMPTY] | MFT | UnsupEmb | Word2Tag\nPOS | 91.95 | 87.06 | 95.55\nSEM | 82.00 | 81.11 | 91.41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d3c5de39-44f7-4eff-93d4-8295045a1db1",
    "input": "## Claim\nHere is a claim: In particular, we see that hate speech and harassment are particularly difficult to detect. Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 1: Classifier performance\nDataset | Class | Precision | Recall | F1\n[ITALIC] W. & H. | Racism | 0.73 | 0.79 | 0.76\n[EMPTY] | Sexism | 0.69 | 0.73 | 0.71\n[EMPTY] | Neither | 0.88 | 0.85 | 0.86\n[ITALIC] W. | Racism | 0.56 | 0.77 | 0.65\n[EMPTY] | Sexism | 0.62 | 0.73 | 0.67\n[EMPTY] | R. & S. | 0.56 | 0.62 | 0.59\n[EMPTY] | Neither | 0.95 | 0.92 | 0.94\n[ITALIC] D. et al. | Hate | 0.32 | 0.53 | 0.4\n[EMPTY] | Offensive | 0.96 | 0.88 | 0.92\n[EMPTY] | Neither | 0.81 | 0.95 | 0.87\n[ITALIC] G. et al. | Harass. | 0.41 | 0.19 | 0.26\n[EMPTY] | Non. | 0.75 | 0.9 | 0.82\n[ITALIC] F. et al. | Hate | 0.33 | 0.42 | 0.37\n[EMPTY] | Abusive | 0.87 | 0.88 | 0.88\n[EMPTY] | Spam | 0.5 | 0.7 | 0.58\n[EMPTY] | Neither | 0.88 | 0.77 | 0.82\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3addae78-e7aa-4757-a6e9-dc12dac74787",
    "input": "## Claim\nHere is a claim: The results prove the effectiveness of word-level attention to exploit the local interactions in link prediction task. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\n-Word-ATT | 0.648 | 0.515 | 0.395 | 0.389\n-Capsule | 0.635 | 0.507 | 0.413 | 0.386\nOur Model | 0.650 | 0.519 | 0.422 | 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "47261858-023d-413f-aad0-4d850bd3ffb3",
    "input": "## Claim\nHere is a claim: The average number of tokens per tweet is not 22.3, per sentence is not 13.6 and average scope length is not 2.9. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 3: Cue and token distribution in the conversational negation corpus.\nTotal negation cues | 2921\nTrue negation cues | 2674\nFalse negation cues | 247\nAverage scope length | 2.9\nAverage sentence length | 13.6\nAverage tweet length | 22.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "b4c1d97b-4782-4575-a319-1b40d0ece452",
    "input": "## Claim\nHere is a claim: [CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\n[EMPTY] | Acc | Sim | PP | GM\nM0: shen-1 | 0.818 | 0.719 | 37.3 | 10.0\nM1: M0 [ITALIC] +para | 0.819 | 0.734 | 26.3 | 14.2\nM2: M0 [ITALIC] +cyc | 0.813 | 0.770 | 36.4 | 18.8\nM3: M0 [ITALIC] +cyc+lang | 0.807 | 0.796 | 28.4 | 21.5\nM4: M0 [ITALIC] +cyc+para | 0.798 | 0.783 | 39.7 | 19.2\nM5: M0 [ITALIC] +cyc+para+lang | 0.804 | 0.785 | 27.1 | 20.3\nM6: M0 [ITALIC] +cyc+2d | 0.805 | [BOLD] 0.817 | 43.3 | 21.6\nM7: M6+ [ITALIC] para+lang | 0.818 | 0.805 | [BOLD] 29.0 | [BOLD] 22.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5fcf5218-c00d-4fb7-a5bd-babf7d5687d0",
    "input": "## Claim\nHere is a claim: 2018) or reinforcement learning with additional dataset-specific heuristics (Kryscinski et\\xa0al. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "bfbfa38c-d46e-475e-817c-e5161057ae5f",
    "input": "## Claim\nHere is a claim: Table 1: In all language pairs, the best correlation is achieved by our word mover metrics that use a BERT pretrained on MNLI as the embedding generator and PMeans to aggregate the embeddings from different BERT layers, i.e., WMD-1/2+BERT+MNLI+PMeans. Does the following context support or refute the claim?\n\n## Table\nPaper title: MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance\nTable caption: Table 1: Absolute Pearson correlations with segment-level human judgments in 7 language pairs on WMT17 dataset.\nSetting | Metrics | <bold>Direct Assessment</bold> cs-en | <bold>Direct Assessment</bold> de-en | <bold>Direct Assessment</bold> fi-en | <bold>Direct Assessment</bold> lv-en | <bold>Direct Assessment</bold> ru-en | <bold>Direct Assessment</bold> tr-en | <bold>Direct Assessment</bold> zh-en | <bold>Direct Assessment</bold> Average\nBaselines | METEOR++ | 0.552 | 0.538 | 0.720 | 0.563 | 0.627 | 0.626 | 0.646 | 0.610\nBaselines | RUSE(*) | 0.624 | 0.644 | 0.750 | 0.697 | 0.673 | 0.716 | 0.691 | 0.685\nBaselines | BERTScore-F1 | 0.670 | 0.686 | 0.820 | 0.710 | 0.729 | 0.714 | 0.704 | 0.719\nSent-Mover | Smd + W2V | 0.438 | 0.505 | 0.540 | 0.442 | 0.514 | 0.456 | 0.494 | 0.484\nSent-Mover | Smd + ELMO + PMeans | 0.569 | 0.558 | 0.732 | 0.525 | 0.581 | 0.620 | 0.584 | 0.595\nSent-Mover | Smd + BERT + PMeans | 0.607 | 0.623 | 0.770 | 0.639 | 0.667 | 0.641 | 0.619 | 0.652\nSent-Mover | Smd + BERT + MNLI + PMeans | 0.616 | 0.643 | 0.785 | 0.660 | 0.664 | 0.668 | 0.633 | 0.667\nWord-Mover | Wmd-1 + W2V | 0.392 | 0.463 | 0.558 | 0.463 | 0.456 | 0.485 | 0.481 | 0.471\nWord-Mover | Wmd-1 + ELMO + PMeans | 0.579 | 0.588 | 0.753 | 0.559 | 0.617 | 0.679 | 0.645 | 0.631\nWord-Mover | Wmd-1 + BERT + PMeans | 0.662 | 0.687 | 0.823 | 0.714 | 0.735 | 0.734 | 0.719 | 0.725\nWord-Mover | Wmd-1 + BERT + MNLI + PMeans | 0.670 | 0.708 | <bold>0.835</bold> | <bold>0.746</bold> | <bold>0.738</bold> | 0.762 | <bold>0.744</bold> | <bold>0.743</bold>\nWord-Mover | Wmd-2 + BERT + MNLI + PMeans | <bold>0.679</bold> | <bold>0.710</bold> | 0.832 | 0.745 | 0.736 | <bold>0.763</bold> | 0.740 | <bold>0.743</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7ce0aea2-aa42-4cc8-99eb-6bdfe53cf6da",
    "input": "## Claim\nHere is a claim: We hypothesize that the gating mechanism can better capture longdistance dependencies between nodes far apart in the graph. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "9d096e1f-029b-48e6-85af-d946dfa8a375",
    "input": "## Claim\nHere is a claim: the model converged and yielded high performance, verifying the efficacy of the implicit answer vector representation for matching word meanings Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VIII: Precision scores for the Semantic Analogy Test\nQuestions Subset | # of Questions Seen | GloVe | Word2Vec | Proposed\nAll | 8783 | 78.94 | 81.03 | 79.96\nAt least one | 1635 | 67.58 | 70.89 | 67.89\nconcept word | 1635 | 67.58 | 70.89 | 67.89\nAll concept words | 110 | 77.27 | 89.09 | 83.64\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "4f80f854-aa9a-4f6d-b67f-0edd0cb7c126",
    "input": "## Claim\nHere is a claim: [CONTINUE] Pretraining the HAN models, although intuitively promising, yields only comparable results with those without. Does the following context support or refute the claim?\n\n## Table\nPaper title: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks\nTable caption: Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\n[BOLD] System | [BOLD] ROUGE-1  [BOLD] R (%) | [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%) | [BOLD] ROUGE-2  [BOLD] R (%) | [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%) | [BOLD] Sentence-Level  [BOLD] R (%) | [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)\n[BOLD] ILP | 24.5 | 41.1 | 29.3\u00b10.5 | 7.9 | 15.0 | 9.9\u00b10.5 | 13.6 | 22.6 | 15.6\u00b10.4\n[BOLD] Sum-Basic | 28.4 | 44.4 | 33.1\u00b10.5 | 8.5 | 15.6 | 10.4\u00b10.4 | 14.7 | 22.9 | 16.7\u00b10.5\n[BOLD] KL-Sum | 39.5 | 34.6 | 35.5\u00b10.5 | 13.0 | 12.7 | 12.3\u00b10.5 | 15.2 | 21.1 | 16.3\u00b10.5\n[BOLD] LexRank | 42.1 | 39.5 | 38.7\u00b10.5 | 14.7 | 15.3 | 14.2\u00b10.5 | 14.3 | 21.5 | 16.0\u00b10.5\n[BOLD] MEAD | 45.5 | 36.5 | 38.5\u00b1 0.5 | 17.9 | 14.9 | 15.4\u00b10.5 | 27.8 | 29.2 | 26.8\u00b10.5\n[BOLD] SVM | 19.0 | 48.8 | 24.7\u00b10.8 | 7.5 | 21.1 | 10.0\u00b10.5 | 32.7 | 34.3 | 31.4\u00b10.4\n[BOLD] LogReg | 26.9 | 34.5 | 28.7\u00b10.6 | 6.4 | 9.9 | 7.3\u00b10.4 | 12.2 | 14.9 | 12.7\u00b10.5\n[BOLD] LogReg [ITALIC] r | 28.0 | 34.8 | 29.4\u00b10.6 | 6.9 | 10.4 | 7.8\u00b10.4 | 12.1 | 14.5 | 12.5\u00b10.5\n[BOLD] HAN | 31.0 | 42.8 | 33.7\u00b10.7 | 11.2 | 17.8 | 12.7\u00b10.5 | 26.9 | 34.1 | 32.4\u00b10.5\n[BOLD] HAN+pretrainT | 32.2 | 42.4 | 34.4\u00b10.7 | 11.5 | 17.5 | 12.9\u00b10.5 | 29.6 | 35.8 | 32.2\u00b10.5\n[BOLD] HAN+pretrainU | 32.1 | 42.1 | 33.8\u00b10.7 | 11.6 | 17.6 | 12.9\u00b10.5 | 30.1 | 35.6 | 32.3\u00b10.5\n[BOLD] HAN [ITALIC] r | 38.1 | 40.5 | [BOLD] 37.8\u00b10.5 | 14.0 | 17.1 | [BOLD] 14.7\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainT [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.5 | 16.8 | [BOLD] 14.4\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainU [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.6 | 16.9 | [BOLD] 14.4\u00b10.5 | 33.9 | 33.8 | [BOLD] 33.8\u00b10.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "da1ceb61-ee17-4e2e-970e-222fe51acd08",
    "input": "## Claim\nHere is a claim: While Glorot achieves slightly better results on BShift and TopConst, CMOW's ability to memorize word content is not improved by our initialization strategy. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 7: Scores for initialization strategies on probing tasks.\nInitialization | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\nN(0,0.1) | 29.7 | 71.5 | 82.0 | 78.5 | 60.1 | 80.5 | 76.3 | 74.7 | [BOLD] 51.3 | 52.5\nGlorot | 31.3 | [BOLD] 72.3 | 81.8 | 78.7 | 59.4 | 81.3 | 76.6 | [BOLD] 74.6 | 50.4 | 57.0\nOur paper | [BOLD] 35.1 | 70.8 | [BOLD] 82.0 | [BOLD] 80.2 | [BOLD] 61.8 | [BOLD] 82.8 | [BOLD] 79.7 | 74.2 | 50.7 | [BOLD] 72.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "f447aac8-3df2-4446-82f9-89b20ad46901",
    "input": "## Claim\nHere is a claim: In Table 5, it can be seen that generative pretraining via language modeling does not account for a considerable amount of performance, constituting 44.32% of the overall performance (a boost of 42.67% in accuracy) in the multitasking setup, and constituting 43.93% of the overall performance (a boost of 39.97%) in the standard finetuning setup. Does the following context support or refute the claim?\n\n## Table\nPaper title: Localization of Fake News Detection via Multitask Transfer Learning\nTable caption: Table 5: An ablation study on the effects of pretraining for multitasking-based and standard GPT-2 finetuning. Results show that pretraining greatly accounts for almost half of performance on both finetuning techniques. \u201cAcc. Inc.\u201d refers to the boost in performance contributed by the pretraining step. \u201c% of Perf.\u201d refers to the percentage of the total performance that the pretraining step contributes.\nFinetuning | Pretrained? | Accuracy | Val. Loss | Acc. Inc. | % of Perf.\nMultitasking | No | 53.61% | 0.7217 | - | -\n[EMPTY] | Yes | 96.28% | 0.2197 | +42.67% | 44.32%\nStandard | No | 51.02% | 0.7024 | - | -\n[EMPTY] | Yes | 90.99% | 0.1826 | +39.97% | 43.93%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "286a8de8-bba3-4a30-8e62-b75d6d91ed7d",
    "input": "## Claim\nHere is a claim: Note that using discriminative training, even with no additional monolingual data, leads to better performance than that of the best language model: the CS-ONLY-DISCRIMINATIVE model achieves an accuracy of 70.5%, 5.1 points more than the accuracy of the FINE-TUNED-LM model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training\nTable caption: Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.\n[EMPTY] | dev perp \u2193 | dev acc \u2191 | dev wer \u2193 | test perp \u2193 | test acc \u2191 | test wer \u2193\nSpanish-only-LM | 329.68 | 26.6 | 30.47 | 322.26 | 25.1 | 29.62\nEnglish-only-LM | 320.92 | 29.3 | 32.02 | 314.04 | 30.3 | 32.51\nAll:CS-last-LM | 76.64 | 47.8 | 14.56 | 76.97 | 49.2 | 14.13\nAll:Shuffled-LM | 68.00 | 51.8 | 13.64 | 68.72 | 51.4 | 13.89\nCS-only-LM | 43.20 | 60.7 | 12.60 | 43.42 | 57.9 | 12.18\nCS-only+vocab-LM | 45.61 | 61.0 | 12.56 | 45.79 | 58.8 | 12.49\nFine-Tuned-LM | 39.76 | 66.9 | 10.71 | 40.11 | 65.4 | 10.17\nCS-only-disc | \u2013 | 72.0 | 6.35 | \u2013 | 70.5 | 6.70\nFine-Tuned-disc | \u2013 | [BOLD] 74.2 | [BOLD] 5.85 | \u2013 | [BOLD] 75.5 | [BOLD] 5.59\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b2165172-65ea-4be7-808b-e6bd633803e9",
    "input": "## Claim\nHere is a claim: [CONTINUE] The effectiveness of our hierarchical attention design is proved by an accuracy drop of 1.95% after removing residual connections and the hierarchical stack of our attention modules. Does the following context support or refute the claim?\n\n## Table\nPaper title: Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation\nTable caption: Table 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set. For \u201c- Hierachical-Attn\u201d, we remove the residual connections between the attention modules in the CMR decoders and all the attention memory access are based on the output from the LSTM. For \u201c- MLP\u201d, we further replace the MLP with a single linear layer with the non-linear activation.\n[BOLD] Model | [BOLD] Joint Acc.\nCOMER | 88.64%\n- Hierachical-Attn | 86.69%\n- MLP | 83.24%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a0a02ddf-285a-4710-8e41-1b089783b27b",
    "input": "## Claim\nHere is a claim: This observation concurs with the performance boost for this model across the two datasets and shows that using a more advanced architecture with more parameters results in larger improvements using the coverage mechanism. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ea6f8a48-40b4-4af6-9c29-8eb66af68e74",
    "input": "## Claim\nHere is a claim: for example, for those rewards, models learn the ROUGE reward much better than the full extent of system-level ROUGE correlation as shown in Table\u00a01, which will also increase system-level ROUGE. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "4169c808-fdbd-4bc2-a58c-0ad6872535b1",
    "input": "## Claim\nHere is a claim: Mentions of time are not specific of complaints (been, still, on, days, Temporal References cluster). Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\n[BOLD] Complaints  [BOLD] Feature | [BOLD] Complaints  [ITALIC] r | [BOLD] Not Complaints  [BOLD] Feature | [BOLD] Not Complaints  [ITALIC] r\n[BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams\nnot | .154 | [URL] | .150\nmy | .131 | ! | .082\nworking | .124 | he | .069\nstill | .123 | thank | .067\non | .119 | , | .064\ncan\u2019t | .113 | love | .064\nservice | .112 | lol | .061\ncustomer | .109 | you | .060\nwhy | .108 | great | .058\nwebsite | .107 | win | .058\nno | .104 | \u2019 | .058\n? | .098 | she | .054\nfix | .093 | : | .053\nwon\u2019t | .092 | that | .053\nbeen | .090 | more | .052\nissue | .089 | it | .052\ndays | .088 | would | .051\nerror | .087 | him | .047\nis | .084 | life | .046\ncharged | .083 | good | .046\n[BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)\nVBN | .141 | UH | .104\n$ | .118 | NNP | .098\nVBZ | .114 | PRP | .076\nNN_VBZ | .114 | HT | .076\nPRP$ | .107 | PRP_. | .076\nPRP$_NN | .105 | PRP_RB | .067\nVBG | .093 | NNP_NNP | .062\nCD | .092 | VBP_PRP | .054\nWRB_VBZ | .084 | JJ | .053\nVBZ_VBN | .084 | DT_JJ | .051\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2ac86f7a-9e75-41a0-a2c9-f36542cb12cf",
    "input": "## Claim\nHere is a claim: This is especially true in the case of DAN where we see a large increase as the decoder repeatedly predicts words having high sentiment value. Does the following context support or refute the claim?\n\n## Table\nPaper title: What do Deep Networks Like to Read?\nTable caption: Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.\n[EMPTY] | <bold>RNN</bold> | <bold>CNN</bold> | <bold>DAN</bold>\nPositive | +9.7 | +4.3 | +<bold>23.6</bold>\nNegative | +6.9 | +5.5 | +<bold>16.1</bold>\nFlipped to Positive | +20.2 | +24.9 | +27.4\nFlipped to Negative | +31.5 | +28.6 | +19.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f494e100-00bf-4835-9c27-5bb78145ce1a",
    "input": "## Claim\nHere is a claim: , then randomly selected 5 examples were manually examined to understand the sources of errors, which was helpful in identifying issues in cue detection. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 4: Cue classification on the test set.\n[EMPTY] | [BOLD] F-Score  [BOLD] Baseline | [BOLD] F-Score  [BOLD] Proposed | [BOLD] Support\nFalse cues | 0.61 | 0.68 | 47\nActual cues | 0.97 | 0.98 | 557\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "565a7149-affb-4f76-b5fe-0cae0e82eacd",
    "input": "## Claim\nHere is a claim: the utterance in the first premise \u201cThe woman went down into the cellar\u201d leads BERT-large to produce \u201cThe woman entered the cellar\u201d and to choose the distractor rather than the correct premise. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 7: Sensitivity of BERT-large to superficial cues identified in \u00a72 (unit: 10\u22122). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.\nCue | [ITALIC] SCOPA | [ITALIC] SB_COPA | Diff. | Prod.\nwoman | 7.98 | 4.84 | -3.14 | 0.25\nmother | 5.16 | 3.95 | -1.21 | 0.75\nwent | 6.00 | 5.15 | -0.85 | 0.73\ndown | 5.52 | 4.93 | -0.58 | 0.71\ninto | 4.07 | 3.51 | -0.56 | 0.40\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0ee9a5d8-8b90-424c-9e68-f02437594591",
    "input": "## Claim\nHere is a claim: Our models DCGCN(single) and DCGCN(ensemble)consist of full GCN layers, removing the burden of employing a recurrent encoder to extract non-local contextual information in the bottom layers. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 4: Main results on English-German and English-Czech datasets.\n[BOLD] Model | [BOLD] Type | [BOLD] English-German #P | [BOLD] English-German B | [BOLD] English-German C | [BOLD] English-Czech #P | [BOLD] English-Czech B | [BOLD] English-Czech C\nBoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | -\nCNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | -\nBiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | -\nPB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4\nSeq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8\nGGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3\nDCGCN (ours) | Single | [BOLD]  29.7M | [BOLD] 19.0 | [BOLD] 44.1 | [BOLD]  28.3M | [BOLD] 12.1 | [BOLD] 37.1\nSeq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4\nGGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9\nDCGCN (ours) | Ensemble | [BOLD]  149M | [BOLD] 20.5 | [BOLD] 45.8 | [BOLD]  142M | [BOLD] 13.1 | [BOLD] 37.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "14c7a0f9-247b-4a40-bd54-5ad510a0a091",
    "input": "## Claim\nHere is a claim: AME performs better than FME model on both symmetric and asymmetric modes, which shows the advantage of finetuning word embeddings during training. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task\nTable caption: Table 1: Image-caption ranking results for English (Multi30k)\n[EMPTY] | Image to Text R@1 | Image to Text R@5 | Image to Text R@10 | Image to Text Mr | Text to Image R@1 | Text to Image R@5 | Text to Image R@10 | Text to Image Mr | Alignment\n[BOLD] symmetric | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nParallel\u00a0gella:17 | 31.7 | 62.4 | 74.1 | 3 | 24.7 | 53.9 | 65.7 | 5 | -\nUVS\u00a0kiros:15 | 23.0 | 50.7 | 62.9 | 5 | 16.8 | 42.0 | 56.5 | 8 | -\nEmbeddingNet\u00a0wang:18 | 40.7 | 69.7 | 79.2 | - | 29.2 | 59.6 | 71.7 | - | -\nsm-LSTM\u00a0huang:17 | 42.5 | 71.9 | 81.5 | 2 | 30.2 | 60.4 | 72.3 | 3 | -\nVSE++\u00a0faghri:18 | [BOLD] 43.7 | 71.9 | 82.1 | 2 | 32.3 | 60.9 | 72.1 | 3 | -\nMono | 41.4 | 74.2 | 84.2 | 2 | 32.1 | 63.0 | 73.9 | 3 | -\nFME | 39.2 | 71.1 | 82.1 | 2 | 29.7 | 62.5 | 74.1 | 3 | 76.81%\nAME | 43.5 | [BOLD] 77.2 | [BOLD] 85.3 | [BOLD] 2 | [BOLD] 34.0 | [BOLD] 64.2 | [BOLD] 75.4 | [BOLD] 3 | 66.91%\n[BOLD] asymmetric | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nPivot\u00a0gella:17 | 33.8 | 62.8 | 75.2 | 3 | 26.2 | 56.4 | 68.4 | 4 | -\nParallel\u00a0gella:17 | 31.5 | 61.4 | 74.7 | 3 | 27.1 | 56.2 | 66.9 | 4 | -\nMono | 47.7 | 77.1 | 86.9 | 2 | 35.8 | 66.6 | 76.8 | 3 | -\nFME | 44.9 | 76.9 | 86.4 | 2 | 34.2 | 66.1 | 77.1 | 3 | 76.81%\nAME | [BOLD] 50.5 | [BOLD] 79.7 | [BOLD] 88.4 | [BOLD] 1 | [BOLD] 38.0 | [BOLD] 68.5 | [BOLD] 78.4 | [BOLD] 2 | 73.10%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "4a50e39b-a5db-4dce-b474-46fda3d1159b",
    "input": "## Claim\nHere is a claim: On the contrary, we found the quality of 3-step NLDs is relatively higher than the others. Does the following context support or refute the claim?\n\n## Table\nPaper title: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension\nTable caption: Table 2: Ratings of annotated NLDs by human judges.\n# steps | Reachability | Derivability Step 1 | Derivability Step 2 | Derivability Step 3\n1 | 3.0 | 3.8 | - | -\n2 | 2.8 | 3.8 | 3.7 | -\n3 | 2.3 | 3.9 | 3.8 | 3.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4cef41a6-d5a3-4308-9bc0-95a8154255d8",
    "input": "## Claim\nHere is a claim: Our joint model does not outperform all the base [CONTINUE] The results do not reconfirm that the lemma baseline, when combined with effective topic clustering, is a strong baseline for CD event coreference resolution on the ECB+ corpus (Upadhyay et al., 2016). Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\n<bold>Baselines</bold> | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nCluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5\nCV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73\nKCP Kenyon-Dean et\u00a0al. (<ref id='bib-bib14'>2018</ref>) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69\nCluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6\n<bold>Model Variants</bold> | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nDisjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5\nJoint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | <bold>79.5</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "965efd28-0126-4d8a-92dc-216b431dafaf",
    "input": "## Claim\nHere is a claim: BI+IS with EWC-adapted models gives a 0.9 / 3.4 BLEU loss over the strong uniform EWC ensemble, and a 2.4 / 10.2 overall BLEU loss over the approach described in Freitag and Al-Onaizan (2016). Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.\n[BOLD] Language pair | [BOLD] Model type | [BOLD] Oracle model | [BOLD] Decoder configuration  [BOLD] Uniform | [BOLD] Decoder configuration  [BOLD] BI + IS\nes-en | Unadapted | 36.4 | 34.7 | 36.6\nes-en | No-reg | 36.6 | 34.8 | -\nes-en | EWC | 37.0 | 36.3 | [BOLD] 37.2\nen-de | Unadapted | 36.4 | 26.8 | 38.8\nen-de | No-reg | 41.7 | 31.8 | -\nen-de | EWC | 42.1 | 38.6 | [BOLD] 42.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e6163646-e624-431a-a99d-c4f2450a0183",
    "input": "## Claim\nHere is a claim: More importantly, their G-Pre and G-Rec scores are all above .50, which means that more than half of the good summaries identified by the metrics are actually good, and more than 50%. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nMetric | [ITALIC] \u03c1 | [ITALIC] r | G-Pre | G-Rec\nROUGE-1 | .290 | .304 | .392 | .428\nROUGE-2 | .259 | .278 | .408 | .444\nROUGE-L | .274 | .297 | .390 | .426\nROUGE-SU4 | .282 | .279 | .404 | .440\nBLEU-1 | .256 | .281 | .409 | .448\nBLEU-2 | .301 | .312 | .411 | .446\nBLEU-3 | .317 | .312 | .409 | .444\nBLEU-4 | .311 | .307 | .409 | .446\nBLEU-5 | .308 | .303 | .420 | .459\nMETEOR | .305 | .285 | .409 | .444\nInferSent-Cosine | [BOLD] .329 | [BOLD] .339 | .417 | .460\nBERT-Cosine | .312 | .335 | [BOLD] .440 | [BOLD] .484\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "047c6f79-94c1-4ebe-bf3d-a85e8dd82e60",
    "input": "## Claim\nHere is a claim: One interpretation for this difference is that under the simulated conversations with random reward function, GP-MBCM does not align well with the different human users. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.\nGP-MBCM | ACER | PPO | ALDM | GDPL\n1.666 | 0.775 | 0.639 | 1.069 | [BOLD] 0.238\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "7e741dca-daea-49be-9ccc-ca533cf8b802",
    "input": "## Claim\nHere is a claim: Again, when ROUGE is used as rewards, the generated summaries have higher ROUGE scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. \u201cPref%\u201d: in how many percentage of documents a system receives the higher human rating.\nReward | R-1 | R-2 | R-L | Human | Pref%\nR-L (original) | 40.9 | 17.8 | 38.5 | 1.75 | 15\nLearned (ours) | 39.2 | 17.4 | 37.5 | [BOLD] 2.20 | [BOLD] 75\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "73207db1-84fa-40bf-b763-0c0b9f859942",
    "input": "## Claim\nHere is a claim: This suggests that graph encoders based on gating mechanisms are very effective in text generation models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "961db06c-7cce-438a-ad9b-89e45a05da2a",
    "input": "## Claim\nHere is a claim: the models more often fail to realise part of the MR, rather than hallucinating additional information. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Cleaned | TGen\u2212 | 36.85 | 5.3782 | 35.14 | 55.01 | 1.6016 | 00.34 | 09.81 | 00.15 | 10.31\nOriginal | [BOLD] Cleaned | TGen | 39.23 | 6.0217 | 36.97 | 55.52 | 1.7623 | 00.40 | 03.59 | 00.07 | 04.05\nOriginal | [BOLD] Cleaned | TGen+ | 40.25 | 6.1448 | 37.50 | 56.19 | 1.8181 | 00.21 | 01.99 | 00.05 | 02.24\nOriginal | [BOLD] Cleaned | SC-LSTM | 23.88 | 3.9310 | 32.11 | 39.90 | 0.5036 | 07.73 | 17.76 | 09.52 | 35.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen\u2212 | 40.19 | 6.0543 | 37.38 | 55.88 | 1.8104 | 00.17 | 01.31 | 00.25 | 01.72\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen | 40.73 | 6.1711 | 37.76 | 56.09 | 1.8518 | 00.07 | 00.72 | 00.08 | 00.87\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen+ | 40.51 | 6.1226 | 37.61 | 55.98 | 1.8286 | 00.02 | 00.63 | 00.06 | 00.70\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | SC-LSTM | 23.66 | 3.9511 | 32.93 | 39.29 | 0.3855 | 07.89 | 15.60 | 08.44 | 31.94\nCleaned missing | [BOLD] Cleaned | TGen\u2212 | 40.48 | 6.0269 | 37.26 | 56.19 | 1.7999 | 00.43 | 02.84 | 00.26 | 03.52\nCleaned missing | [BOLD] Cleaned | TGen | 41.57 | 6.2830 | 37.99 | 56.36 | 1.8849 | 00.37 | 01.40 | 00.09 | 01.86\nCleaned missing | [BOLD] Cleaned | TGen+ | 41.56 | 6.2700 | 37.94 | 56.38 | 1.8827 | 00.21 | 01.04 | 00.07 | 01.31\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen\u2212 | 35.99 | 5.0734 | 34.74 | 54.79 | 1.5259 | 00.02 | 11.58 | 00.02 | 11.62\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen | 40.07 | 6.1243 | 37.45 | 55.81 | 1.8026 | 00.05 | 03.23 | 00.01 | 03.29\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+ | 40.80 | 6.2197 | 37.86 | 56.13 | 1.8422 | 00.01 | 01.87 | 00.01 | 01.88\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b8186df4-979b-4b04-bb09-a484f0e6dfd6",
    "input": "## Claim\nHere is a claim: These results show significant performance improvement by using Predicate Schemas knowledge on hard coreference problems. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nDataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb\n[ITALIC] Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 | [BOLD] 76.41\n[ITALIC] WinoCoref | AntePre | 68.37 | 74.32 | \u2014\u2013 | 88.48 | 88.95 | [BOLD] 89.32\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3ab43867-7cf7-491e-ba17-95ea2d79dfce",
    "input": "## Claim\nHere is a claim: This is expected, since the questions in the SQuAD and QA-SRL datasets tend to be very different (more declarative in the former, more interrogative in the latter). Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0150f1a0-fe1d-4497-8489-a649003ab619",
    "input": "## Claim\nHere is a claim: The AAS method with weights wAC=1 and wAD=105 shows the lowest WER and DCE. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition\nTable caption: TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set\nMethod | WER (%) | DCE\nNo enhancement | 17.3 | 0.828\nWiener filter | 19.5 | 0.722\nMinimizing DCE | 15.8 | [BOLD] 0.269\nFSEGAN | 14.9 | 0.291\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=0) | 15.6 | 0.330\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 14.4 | 0.303\nClean speech | 5.7 | 0.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4667459d-519c-4af0-9060-21a22cd745f1",
    "input": "## Claim\nHere is a claim: The most representative models are ELMO, GPT, BERT and its variants, and XLNET. Does the following context support or refute the claim?\n\n## Table\nPaper title: Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches\nTable caption: Table 2: Comparison of exact-match accuracy achieved on selected benchmarks by a random or majority-choice baseline, various neural contextual embedding models, and humans. ELMo refers to the highest-performing listed approach using ELMo embeddings. Best system performance on each benchmark in bold. Information extracted from leaderboards (linked to in the first column) at time of writing (October 2019), and original papers for benchmarks introduced in Section\u00a02.\n[BOLD] Benchmark | [BOLD]  Simple Baseline  | [BOLD] ELMo | [BOLD] GPT | [BOLD] BERT | [BOLD] MT-DNN | [BOLD] XLNet | [BOLD] RoBERTa | [BOLD] ALBERT | [BOLD] Human\n[BOLD] CLOTH | 25.0 | 70.7 | \u2013 | [BOLD] 86.0 | \u2013 | \u2013 | \u2013 | \u2013 | 85.9\n[BOLD] Cosmos QA | \u2013 | \u2013 | 54.5 | 67.1 | \u2013 | \u2013 | \u2013 | \u2013 | 94.0\n[BOLD] DREAM | 33.4 | 59.5 | 55.5 | 66.8 | \u2013 | [BOLD] 72.0 | \u2013 | \u2013 | 95.5\n[BOLD] GLUE | \u2013 | 70.0 | \u2013 | 80.5 | 87.6 | 88.4 | 88.5 | [BOLD] 89.4 | 87.1\n[BOLD] HellaSWAG | 25.0 | 33.3 | 41.7 | 47.3 | \u2013 | \u2013 | [BOLD] 85.2 | [EMPTY] | 95.6\n[BOLD] MC-TACO | 17.4 | 26.4 | \u2013 | 42.7 | \u2013 | \u2013 | [BOLD] 43.6 | \u2013 | 75.8\n[BOLD] RACE | 24.9 | \u2013 | 59.0 | 72.0 | \u2013 | 81.8 | 83.2 | [BOLD] 89.4 | 94.5\n[BOLD] SciTail | 60.3 | \u2013 | 88.3 | \u2013 | 94.1 | \u2013 | \u2013 | \u2013 | \u2013\n[BOLD] SQuAD 1.1 | 1.3 | 81.0 | \u2013 | 87.4 | \u2013 | [BOLD] 89.9 | \u2013 | \u2013 | 82.3\n[BOLD] SQuAD 2.0 | 48.9 | 63.4 | \u2013 | 80.8 | \u2013 | 86.3 | 86.8 | [BOLD] 89.7 | 86.9\n[BOLD] SuperGLUE | 47.1 | \u2013 | \u2013 | 69.0 | \u2013 | \u2013 | [BOLD] 84.6 | \u2013 | 89.8\n[BOLD] SWAG | 25.0 | 59.1 | 78.0 | 86.3 | 87.1 | \u2013 | [BOLD] 89.9 | \u2013 | 88.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "81c0bde7-fe67-476c-bf82-5fe7def3b1f3",
    "input": "## Claim\nHere is a claim: In general, increasing the number of GCN layers from 2 to 9 boosts the model performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.\n[BOLD] GCN +RC (2) | B 16.8 | C 48.1 | [BOLD] GCN +RC+LA (2) | B 18.3 | C 47.9\n+RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1\n+RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8\n+RC (9) | [BOLD] 21.1 | 50.5 | +RC+LA (9) | [BOLD] 22.0 | 52.6\n+RC (10) | 20.7 | [BOLD] 50.7 | +RC+LA (10) | 21.2 | [BOLD] 52.9\nDCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7\nDCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "6d133fca-e9b6-4507-ba72-5a65564bf8da",
    "input": "## Claim\nHere is a claim: The results of CLUSTER+KCP indicate that pre-clustering of documents to topics is not beneficial, performing substantially worse than our joint model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\n<bold>Baselines</bold> | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nCluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5\nCV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73\nKCP Kenyon-Dean et\u00a0al. (<ref id='bib-bib14'>2018</ref>) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69\nCluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6\n<bold>Model Variants</bold> | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nDisjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5\nJoint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | <bold>79.5</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "0fbadeff-af49-4236-b0b4-749c3e102f94",
    "input": "## Claim\nHere is a claim: [CONTINUE] We found that innovations are helpful in both early and late fusion frameworks, while late fusion performs better on average. Does the following context support or refute the claim?\n\n## Table\nPaper title: Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection\nTable caption: Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. \u201cRaw\u201d indicates the usage of original prosodic features (Section 3.2), while \u201cinnovations\u201d indicate the usage of innovation features (Section 3.3).\n[EMPTY] | [BOLD] Model | [BOLD] dev mean | [BOLD] dev best | [BOLD] test mean | [BOLD] test best | [ITALIC] \u03b1\nsingle | text | 86.54 | 86.80 | 86.47 | 86.96 | \u2013\nsingle | raw | 35.00 | 37.33 | 35.78 | 37.70 | \u2013\nsingle | innovations | 80.86 | 81.51 | 80.28 | 82.15 | \u2013\nearly | text + raw | 86.46 | 86.65 | 86.24 | 86.53 | \u2013\nearly | text + innovations | 86.53 | 86.77 | 86.54 | 87.00 | \u2013\nearly | text + raw + innovations | 86.35 | 86.69 | 86.55 | 86.44 | \u2013\nlate | text + raw | 86.71 | 87.05 | 86.35 | 86.71 | 0.2\nlate | text + innovations | [BOLD] 86.98 | [BOLD] 87.48 | [BOLD] 86.68 | [BOLD] 87.02 | 0.5\nlate | text + raw + innovations | 86.95 | 87.30 | 86.60 | 86.87 | 0.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c45cc229-e5f8-4a18-b769-42397cd1f57d",
    "input": "## Claim\nHere is a claim: [CONTINUE] As these models use object detectors pretrained on Pascal-VOC , they have somewhat higher performance on classes that are common to both Flickr30k and Pascal-VOC (\"animals\", \"people\" and \"vehicles\"). Does the following context support or refute the claim?\n\n## Table\nPaper title: Zero-Shot Grounding of Objects from Natural Language Queries\nTable caption: Table 3: Category-wise performance with the default split of Flickr30k Entities.\nMethod | Overall | people | clothing | bodyparts | animals | vehicles | instruments | scene | other\nQRC - VGG(det) | 60.21 | 75.08 | 55.9 | 20.27 | 73.36 | 68.95 | 45.68 | 65.27 | 38.8\nCITE - VGG(det) | 61.89 | [BOLD] 75.95 | 58.50 | 30.78 | [BOLD] 77.03 | [BOLD] 79.25 | 48.15 | 58.78 | 43.24\nZSGNet - VGG (cls) | 60.12 | 72.52 | 60.57 | 38.51 | 63.61 | 64.47 | 49.59 | 64.66 | 41.09\nZSGNet - Res50 (cls) | [BOLD] 63.39 | 73.87 | [BOLD] 66.18 | [BOLD] 45.27 | 73.79 | 71.38 | [BOLD] 58.54 | [BOLD] 66.49 | [BOLD] 45.53\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "57a46ccf-9c16-4b99-a81c-e0ca346de3af",
    "input": "## Claim\nHere is a claim: Coverage helps the model improve its EM by 1.5 and its F1 by 0.5. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "9d83016c-5c81-49d2-9e74-dde587642c9c",
    "input": "## Claim\nHere is a claim: models with NSP performance drop a lot when trained with COPA. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large | B-COPA | 70.5 (\u00b1 2.5) | 72.6 (\u00b1 2.3) | [BOLD] 69.1 (\u00b1 2.7)\nBERT-large | B-COPA (50%) | 69.9 (\u00b1 1.9) | 71.2 (\u00b1 1.3) | 69.0 (\u00b1 3.5)\nBERT-large | COPA | [BOLD] 71.7 (\u00b1 0.5) | [BOLD] 80.5 (\u00b1 0.4) | 66.3 (\u00b1 0.8)\nRoBERTa-large | B-COPA | [BOLD] 76.7 (\u00b1 0.8) | 73.3 (\u00b1 1.5) | [BOLD] 78.8 (\u00b1 2.0)\nRoBERTa-large | B-COPA (50%) | 72.4 (\u00b1 2.0) | 72.1 (\u00b1 1.7) | 72.6 (\u00b1 2.1)\nRoBERTa-large | COPA | 76.4 (\u00b1 0.7) | [BOLD] 79.6 (\u00b1 1.0) | 74.4 (\u00b1 1.1)\nBERT-base-NSP | None | [BOLD] 66.4 | 66.2 | [BOLD] 66.7\nBERT-large-NSP | None | 65.0 | [BOLD] 66.9 | 62.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8d3edac1-1144-4e49-ab95-1d0898c2acaf",
    "input": "## Claim\nHere is a claim: The hybrid model is able to repair this deficit, reducing the difference to 8%. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 3: Scores on unsupervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nMethod | STS12 | STS13 | STS14 | STS15 | STS16\nCBOW | 43.5 | [BOLD] 50.0 | [BOLD] 57.7 | [BOLD] 63.2 | 61.0\nCMOW | 39.2 | 31.9 | 38.7 | 49.7 | 52.2\nHybrid | [BOLD] 49.6 | 46.0 | 55.1 | 62.4 | [BOLD] 62.1\ncmp. CBOW | +14.6% | -8% | -4.5% | -1.5% | +1.8%\ncmp. CMOW | +26.5% | +44.2% | +42.4 | +25.6% | +19.0%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7d863838-7856-49cb-b150-588aa3ed26c2",
    "input": "## Claim\nHere is a claim: our framework captures more information about the intended semantic feature. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6ff909e7-efc8-4f69-a140-ebb18d859825",
    "input": "## Claim\nHere is a claim: We can see that the dual attention model does not work at all and the scores slightly drop. Does the following context support or refute the claim?\n\n## Table\nPaper title: The MeMAD Submission to the WMT18 Multimodal Translation Task\nTable caption: Table 4: Adding automatic image captions (only the best one or all 5). The table shows BLEU scores in %. All results with Marian Amun.\nen-fr | flickr16 | flickr17 | mscoco17\nmulti30k | 61.4 | 54.0 | 43.1\n+autocap (dual attn.) | 60.9 | 52.9 | 43.3\n+autocap 1 (concat) | 61.7 | 53.7 | 43.9\n+autocap 1-5 (concat) | [BOLD] 62.2 | [BOLD] 54.4 | [BOLD] 44.1\nen-de | flickr16 | flickr17 | mscoco17\nmulti30k | 38.9 | 32.0 | 27.7\n+autocap (dual attn.) | 37.8 | 30.2 | 27.0\n+autocap 1 (concat) | 39.7 | [BOLD] 32.2 | [BOLD] 28.8\n+autocap 1-5 (concat) | [BOLD] 39.9 | 32.0 | 28.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "04896b77-6ed9-48a4-bff4-6200b7bd4ec6",
    "input": "## Claim\nHere is a claim: in terms of correctness, the averaged Ok rate on all 15 decisions is 44.3% Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.\nModel | Diversity | App | Good% | OK% | Invalid%\nDAMD | 3.12 | 2.50 | 56.5% | [BOLD] 37.4% | 6.1%\nDAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1% | 9.9%\nHDSA (+) | 2.14 | 2.47 | 57.5% | 32.5% | [BOLD] 10.0%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ccc54cf1-8e27-4c66-868b-a174db35f0bb",
    "input": "## Claim\nHere is a claim: The results in Table 7 show that the proposed method is not as effective as the state of the art BiLSTM model from (Fancellu et al., 2016) on gold negation cues for scope prediction. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "8024d1af-5c05-4fe3-8254-ce70860006b0",
    "input": "## Claim\nHere is a claim: SciBERT does not significantly boost performance for scientific datasets including SciERC and GENIA. Does the following context support or refute the claim?\n\n## Table\nPaper title: Entity, Relation, and Event Extraction with Contextualized Span Representations\nTable caption: Table 7: In-domain pre-training: SciBERT vs. BERT\n[EMPTY] | SciERC Entity | SciERC Relation | GENIA Entity\nBest BERT | 69.8 | 41.9 | 78.4\nBest SciBERT | [BOLD] 72.0 | [BOLD] 45.3 | [BOLD] 79.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e0ae18f4-9bba-4ce9-83da-01228c8b4f30",
    "input": "## Claim\nHere is a claim: Our single model is not comparable to the ensemble results of Seq2SeqB and GGNN2Seq, while the number of parameters of our models is only about 1/6 of theirs. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 4: Main results on English-German and English-Czech datasets.\n[BOLD] Model | [BOLD] Type | [BOLD] English-German #P | [BOLD] English-German B | [BOLD] English-German C | [BOLD] English-Czech #P | [BOLD] English-Czech B | [BOLD] English-Czech C\nBoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | -\nCNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | -\nBiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | -\nPB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4\nSeq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8\nGGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3\nDCGCN (ours) | Single | [BOLD]  29.7M | [BOLD] 19.0 | [BOLD] 44.1 | [BOLD]  28.3M | [BOLD] 12.1 | [BOLD] 37.1\nSeq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4\nGGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9\nDCGCN (ours) | Ensemble | [BOLD]  149M | [BOLD] 20.5 | [BOLD] 45.8 | [BOLD]  142M | [BOLD] 13.1 | [BOLD] 37.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "31933281-32c2-4d77-9884-37546c8599f8",
    "input": "## Claim\nHere is a claim: The results for testing on cleaned data (Table 3, top half) confirm the positive impact of cleaned training data and also show that the cleaned test data is more challenging (cf. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Cleaned | TGen\u2212 | 36.85 | 5.3782 | 35.14 | 55.01 | 1.6016 | 00.34 | 09.81 | 00.15 | 10.31\nOriginal | [BOLD] Cleaned | TGen | 39.23 | 6.0217 | 36.97 | 55.52 | 1.7623 | 00.40 | 03.59 | 00.07 | 04.05\nOriginal | [BOLD] Cleaned | TGen+ | 40.25 | 6.1448 | 37.50 | 56.19 | 1.8181 | 00.21 | 01.99 | 00.05 | 02.24\nOriginal | [BOLD] Cleaned | SC-LSTM | 23.88 | 3.9310 | 32.11 | 39.90 | 0.5036 | 07.73 | 17.76 | 09.52 | 35.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen\u2212 | 40.19 | 6.0543 | 37.38 | 55.88 | 1.8104 | 00.17 | 01.31 | 00.25 | 01.72\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen | 40.73 | 6.1711 | 37.76 | 56.09 | 1.8518 | 00.07 | 00.72 | 00.08 | 00.87\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen+ | 40.51 | 6.1226 | 37.61 | 55.98 | 1.8286 | 00.02 | 00.63 | 00.06 | 00.70\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | SC-LSTM | 23.66 | 3.9511 | 32.93 | 39.29 | 0.3855 | 07.89 | 15.60 | 08.44 | 31.94\nCleaned missing | [BOLD] Cleaned | TGen\u2212 | 40.48 | 6.0269 | 37.26 | 56.19 | 1.7999 | 00.43 | 02.84 | 00.26 | 03.52\nCleaned missing | [BOLD] Cleaned | TGen | 41.57 | 6.2830 | 37.99 | 56.36 | 1.8849 | 00.37 | 01.40 | 00.09 | 01.86\nCleaned missing | [BOLD] Cleaned | TGen+ | 41.56 | 6.2700 | 37.94 | 56.38 | 1.8827 | 00.21 | 01.04 | 00.07 | 01.31\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen\u2212 | 35.99 | 5.0734 | 34.74 | 54.79 | 1.5259 | 00.02 | 11.58 | 00.02 | 11.62\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen | 40.07 | 6.1243 | 37.45 | 55.81 | 1.8026 | 00.05 | 03.23 | 00.01 | 03.29\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+ | 40.80 | 6.2197 | 37.86 | 56.13 | 1.8422 | 00.01 | 01.87 | 00.01 | 01.88\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3f5ef015-47e3-4ccd-abc8-9db195e5a363",
    "input": "## Claim\nHere is a claim: If we check the relative ranks of the good summaries according to the metrics (row 1), for example for ROUGE-SU4, we see that 98.4% of them belong to the top 25% summaries in the metric. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nMetric | [ITALIC] \u03c1 | [ITALIC] r | G-Pre | G-Rec\nROUGE-1 | .290 | .304 | .392 | .428\nROUGE-2 | .259 | .278 | .408 | .444\nROUGE-L | .274 | .297 | .390 | .426\nROUGE-SU4 | .282 | .279 | .404 | .440\nBLEU-1 | .256 | .281 | .409 | .448\nBLEU-2 | .301 | .312 | .411 | .446\nBLEU-3 | .317 | .312 | .409 | .444\nBLEU-4 | .311 | .307 | .409 | .446\nBLEU-5 | .308 | .303 | .420 | .459\nMETEOR | .305 | .285 | .409 | .444\nInferSent-Cosine | [BOLD] .329 | [BOLD] .339 | .417 | .460\nBERT-Cosine | .312 | .335 | [BOLD] .440 | [BOLD] .484\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "769e0a86-2bc0-4010-8be4-e9b42e868bed",
    "input": "## Claim\nHere is a claim: we observe that MQAN (RAE-based) suffers most without coverage: in all out-of-domain settings it underperforms the original. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\n[EMPTY] | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK\nMQAN | 72.30 | 60.91 | 41.82 | 53.95\n+ coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold>\nESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37\n+ coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "30256121-6d2d-467e-b640-e752b29ffb03",
    "input": "## Claim\nHere is a claim: ( 2019). Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See \u00a72 for model details. * indicates our replication experiments.\nModel | Accuracy\nBigramPMI\u00a0Goodwin et al. ( 2012 ) | 63.4\nPMI\u00a0Gordon et al. ( 2011 ) | 65.4\nPMI+Connectives\u00a0Luo et al. ( 2016 ) | 70.2\nPMI+Con.+Phrase\u00a0Sasaki et al. ( 2017 ) | 71.4\nBERT-large\u00a0Wang et al. ( 2019 ) | 70.5\nBERT-large\u00a0Sap et al. ( 2019 ) | 75.0\nBERT-large\u00a0Li et al. ( 2019 ) | 75.4\nRoBERTa-large (finetuned) | 90.6\nBERT-large (finetuned)* | 76.5 \u00b1 2.7\nRoBERTa-large (finetuned)* | 87.7 \u00b1 0.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e91d427d-29e5-46b3-a123-ba6111eb0525",
    "input": "## Claim\nHere is a claim: We notice no significant improvements relative to the baseline showing that self-attention alone does not improve the VQA task. Does the following context support or refute the claim?\n\n## Table\nPaper title: Modulated Self-attention Convolutional Network for VQA\nTable caption: Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\n[BOLD] ResNet-34 | [BOLD] Eval set % | [BOLD] #param\nBaseline (No SA)Anderson et al. ( 2018 ) | 55.00 | 0M\nSA (S: 1,2,3 - B: 1) | 55.11 | } 0.107M\nSA (S: 1,2,3 - B: 2) | 55.17 | } 0.107M\n[BOLD] SA (S: 1,2,3 - B: 3) | [BOLD] 55.27 | } 0.107M\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d0e62762-04dc-4281-b31e-d45d3580665e",
    "input": "## Claim\nHere is a claim: Our DKRN agent outperforms all other agents with a large margin. Does the following context support or refute the claim?\n\n## Table\nPaper title: Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation\nTable caption: Table 5: Results of the Human Rating on CWC.\nSystem | Succ. (%) | Smoothness\nRetrieval-Stgy\u00a0 | 54.0 | 2.48\nPMI\u00a0 | 46.0 | 2.56\nNeural\u00a0 | 36.0 | 2.50\nKernel\u00a0 | 58.0 | 2.48\nDKRN (ours) | [BOLD] 88.0 | [BOLD] 3.22\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "4c635753-fc61-423d-a4b9-cbb47e724697",
    "input": "## Claim\nHere is a claim: When we add multi-factor attention to the baseline BiLSTM-CNN model without the dependency distance-based weight factor in the attention mechanism, we get 0.8% F1 score improvement (A2\u2212A1). Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.\n[EMPTY] | Prec. | Rec. | F1\n(A1) BiLSTM-CNN | 0.473 | 0.606 | 0.531\n(A2) Standard attention | 0.466 | 0.638 | 0.539\n(A3) Window size ( [ITALIC] ws)=5 | 0.507 | 0.652 | [BOLD] 0.571\n(A4) Window size ( [ITALIC] ws)=10 | 0.510 | 0.640 | 0.568\n(A5) Softmax | 0.490 | 0.658 | 0.562\n(A6) Max-pool | 0.492 | 0.600 | 0.541\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "621a2ffa-a852-4a18-87d4-c5312befefd5",
    "input": "## Claim\nHere is a claim: These results show no significant performance improvement by using Predicate Schemas knowledge on hard coreference problems. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nDataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb\n[ITALIC] Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 | [BOLD] 76.41\n[ITALIC] WinoCoref | AntePre | 68.37 | 74.32 | \u2014\u2013 | 88.48 | 88.95 | [BOLD] 89.32\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "65f4f4b5-a857-4cff-a33a-12fbad54d0fd",
    "input": "## Claim\nHere is a claim: Also, the performance drop between Cat1/Cat2 and full data indicates that there is a need to design more complicated knowledge schemas and to refine the knowledge acquisition for further performance improvement. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.\nSchema | AntePre(Test) | AntePre(Train)\nType 1 | 76.67 | 86.79\nType 2 | 79.55 | 88.86\nType 1 (Cat1) | 90.26 | 93.64\nType 2 (Cat2) | 83.38 | 92.49\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b27fa344-5820-4aed-a2a9-228427b26999",
    "input": "## Claim\nHere is a claim: In contrast, models in the lower portion (7-12) involve dialogue states, which is estimated using Belief Tracker, or in other words, by models in the upper portion. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "9f36b848-b4e8-449c-aa79-3a4ed3e10d71",
    "input": "## Claim\nHere is a claim: Audio2vec works better than chance and mean MFCC on paraphrase retrieval, but does not correlate with the visual space. Does the following context support or refute the claim?\n\n## Table\nPaper title: On the difficulty of a distributional semantics of spoken language\nTable caption: Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.\n[EMPTY] | Recall@10 (%) | Median rank | RSAimage\nVGS | 27 | 6 | 0.4\nSegMatch | [BOLD] 10 | [BOLD] 37 | [BOLD] 0.5\nAudio2vec-U | 5 | 105 | 0.0\nAudio2vec-C | 2 | 647 | 0.0\nMean MFCC | 1 | 1,414 | 0.0\nChance | 0 | 3,955 | 0.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3ccac9a1-709e-4f2a-a485-b6ecf90417cc",
    "input": "## Claim\nHere is a claim: Also, we notice a drop in performance between PG-original, and PG-MMR (which takes the pre-trained PG-original and applies MMR on top of the model). Does the following context support or refute the claim?\n\n## Table\nPaper title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\nTable caption: Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.\n[BOLD] Method | [BOLD] R-1 | [BOLD] R-2 | [BOLD] R-SU\nFirst-1 | 26.83 | 7.25 | 6.46\nFirst-2 | 35.99 | 10.17 | 12.06\nFirst-3 | 39.41 | 11.77 | 14.51\nLexRank Erkan and Radev ( 2004 ) | 38.27 | 12.70 | 13.20\nTextRank Mihalcea and Tarau ( 2004 ) | 38.44 | 13.10 | 13.50\nMMR Carbonell and Goldstein ( 1998 ) | 38.77 | 11.98 | 12.91\nPG-Original Lebanoff et\u00a0al. ( 2018 ) | 41.85 | 12.91 | 16.46\nPG-MMR Lebanoff et\u00a0al. ( 2018 ) | 40.55 | 12.36 | 15.87\nPG-BRNN Gehrmann et\u00a0al. ( 2018 ) | 42.80 | 14.19 | 16.75\nCopyTransformer Gehrmann et\u00a0al. ( 2018 ) | [BOLD] 43.57 | 14.03 | 17.37\nHi-MAP (Our Model) | 43.47 | [BOLD] 14.89 | [BOLD] 17.41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f47e2f72-62c7-485b-b3f5-72e99c6fd3b8",
    "input": "## Claim\nHere is a claim: their informative and match scores are higher than ours since they prioritize the dialog turn to show referents, while we take into account various factors in dialog quality. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "566a37c6-ec13-48a6-a1b3-116b99739810",
    "input": "## Claim\nHere is a claim: As a result, the recursive approach performs better than the folding technique for the training task. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2\nTable caption: Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold\u2019s folding technique, and TensorFlow\u2019s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.\nBatch size | Throughput (instances/s) Inference | Throughput (instances/s) Inference | Throughput (instances/s) Inference | Throughput (instances/s) Training | Throughput (instances/s) Training | Throughput (instances/s) Training\nBatch size | Iter | Recur | Fold | Iter | Recur | Fold\n1 | 19.2 | 81.4 | 16.5 | 2.5 | 4.8 | 9.0\n10 | 49.3 | 217.9 | 52.2 | 4.0 | 4.2 | 37.5\n25 | 72.1 | 269.9 | 61.6 | 5.5 | 3.6 | 54.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ceeea11f-c920-442d-8462-46a12e693a9c",
    "input": "## Claim\nHere is a claim: The evaluation results shown in Table 2 indicate that the annotated NLDs are of high quality (Reachability), and each NLD is properly derived from supporting documents (Derivability). Does the following context support or refute the claim?\n\n## Table\nPaper title: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension\nTable caption: Table 2: Ratings of annotated NLDs by human judges.\n# steps | Reachability | Derivability Step 1 | Derivability Step 2 | Derivability Step 3\n1 | 3.0 | 3.8 | - | -\n2 | 2.8 | 3.8 | 3.7 | -\n3 | 2.3 | 3.9 | 3.8 | 3.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f32c052e-7348-4b5c-86e9-8376b541c61d",
    "input": "## Claim\nHere is a claim: TF and DF achieved almost the same values of precision, recall and f-measure using the English corpora, achieving the same value of precision (P=0.0150) and f-measure (F=0.0293) when using the Europarl corpus in English. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150 | 0.0150 | 0.0445 | 0.0326\nP | EN | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.0060 | 0.0092 | 0.0090 | 0.0356 | 0.0162\nP | PT | Europarl | 0.5710 | 0.1948 | 0.3855 | 0.5474 | 0.4485 | [BOLD] 0.8052 | 0.4058\n[EMPTY] | PT | Ted Talks | [BOLD] 0.6304 | 0.1870 | 0.3250 | 0.5312 | 0.4576 | 0.6064 | 0.3698\nR | EN | Europarl | 0.0037 | 0.3278 | 0.5941 | 0.6486 | [BOLD] 0.6490 | 0.0017 | 0.0003\nR | EN | Ted Talks | 0.0002 | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332 | 0.0967 | 0.0003\nR | PT | Europarl | 0.0002 | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932 | 0.0032 | 0.0001\n[EMPTY] | PT | Ted Talks | 2.10-5 | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887 | 0.1390 | 0.0002\nF | EN | Europarl | 0.0073 | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033 | 0.0006\nF | EN | Ted Talks | 0.0004 | 0.0132 | 0.0118 | 0.0181 | 0.0179 | [BOLD] 0.0520 | 0.0005\nF | PT | Europarl | 0.0005 | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109 | 0.0064 | 0.0002\n[EMPTY] | PT | Ted Talks | 4.10-5 | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149 | 0.2261 | 0.0004\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "fd4cae40-9362-44da-a07e-14659a6dcbcf",
    "input": "## Claim\nHere is a claim: Table 5 summarizes the above experimental results on the affected domain in terms of the number of dialog turns, and the numbers of inform, match, and success actions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "9dcfd95a-e2f4-426a-8ccc-f49889d05521",
    "input": "## Claim\nHere is a claim: Our model obtains the best performance on three out of the four datasets. Does the following context support or refute the claim?\n\n## Table\nPaper title: Keyphrase Generation for Scientific Articles using GANs\nTable caption: Table 2: \u03b1-nDCG@5 metrics\nModel | Inspec | Krapivin | NUS | KP20k\nCatseq | 0.87803 | 0.781 | 0.82118 | 0.804\nCatseq-RL | 0.8602 | [BOLD] 0.786 | 0.83 | 0.809\nGAN | [BOLD] 0.891 | 0.771 | [BOLD] 0.853 | [BOLD] 0.85\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7e3de05e-1093-426a-9ad5-e6929654530d",
    "input": "## Claim\nHere is a claim: The first block in Table 6 shows the performance of our two baseline models: multi-layer GCNs with residual connections (GCN+RC) and multi-layer GCNs with both residual connections and layer aggregations (GCN+RC+LA). Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.\n[BOLD] GCN +RC (2) | B 16.8 | C 48.1 | [BOLD] GCN +RC+LA (2) | B 18.3 | C 47.9\n+RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1\n+RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8\n+RC (9) | [BOLD] 21.1 | 50.5 | +RC+LA (9) | [BOLD] 22.0 | 52.6\n+RC (10) | 20.7 | [BOLD] 50.7 | +RC+LA (10) | 21.2 | [BOLD] 52.9\nDCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7\nDCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "9ca997f9-4c36-49e9-b0ed-d0b1bcbdcee6",
    "input": "## Claim\nHere is a claim: The results in Table 4 confirm the findings of the automatic [CONTINUE] metrics: systems trained on the fully cleaned set or the set with cleaned missing slots have nearperfect performance, with the fully-cleaned one showing a few more slight disfluencies than the other. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).\n[BOLD] Training data | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] Disfl\nOriginal | 0 | 22 | 0 | 14\nCleaned added | 0 | 23 | 0 | 14\nCleaned missing | 0 | 1 | 0 | 2\nCleaned | 0 | 0 | 0 | 5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d611540c-f325-4180-b442-77996127c1a8",
    "input": "## Claim\nHere is a claim: We see clear benefits of the coverage mechanism in the out-of-domain setting, especially in the low-resource case of QA-SRL. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "5f50a2cd-c027-4ecb-b169-e26fcc59005f",
    "input": "## Claim\nHere is a claim: It does not improve by over 20% over a state-of-art general coreference system on Winograd and also does not outperform Rahman and Ng (2012) by a margin of 3.3%. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nDataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb\n[ITALIC] Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 | [BOLD] 76.41\n[ITALIC] WinoCoref | AntePre | 68.37 | 74.32 | \u2014\u2013 | 88.48 | 88.95 | [BOLD] 89.32\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "67b478a8-a83d-4700-8567-f8550bcce109",
    "input": "## Claim\nHere is a claim: among opinions: We see that OD significantly outperforms the baseline methods and the OD-parse variant [CONTINUE] OD achieves high ARI and Sil scores, [CONTINUE] From the above table, we observe that the text-similarity based baselines, such as TF-IDF, WMD and Doc2vec achieving ARI and Silhouette coefficient scores of close to zero on the \"Video Games\" and \"Pornography\" datasets (barely providing a performance improvement over random clustering, i.e., a zero ARI score). Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 3: ARI and Silhouette coefficient scores.\nMethods | Seanad Abolition ARI | Seanad Abolition  [ITALIC] Sil | Video Games ARI | Video Games  [ITALIC] Sil | Pornography ARI | Pornography  [ITALIC] Sil\nTF-IDF | 0.23 | 0.02 | -0.01 | 0.01 | -0.02 | 0.01\nWMD | 0.09 | 0.01 | 0.01 | 0.01 | -0.02 | 0.01\nSent2vec | -0.01 | -0.01 | 0.11 | 0.06 | 0.01 | 0.02\nDoc2vec | -0.01 | -0.03 | -0.01 | 0.01 | 0.02 | -0.01\nBERT | 0.03 | -0.04 | 0.08 | 0.05 | -0.01 | 0.03\nOD-parse | 0.01 | -0.04 | -0.01 | 0.02 | 0.07 | 0.05\nOD | [BOLD] 0.54 | [BOLD] 0.31 | [BOLD] 0.56 | [BOLD] 0.42 | [BOLD] 0.41 | [BOLD] 0.41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "acab10a0-f7f1-409b-89d5-9e8b256c3c2d",
    "input": "## Claim\nHere is a claim: In particular, our single DCGCN model does not consistently outperform Seq2Seq models when trained without external resources. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.\n[BOLD] Model | [BOLD] T | #P | B | C\nSeq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1\nGGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4\nSeq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5\nGGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5\nDCGCN (ours) | S | [BOLD] 19.1M | 27.9 | 57.3\nDCGCN (ours) | E | 92.5M | [BOLD] 30.4 | [BOLD] 59.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ce13f0c9-f310-4589-ab1a-1ab472a1d338",
    "input": "## Claim\nHere is a claim: Our joint model improves upon the strong lemma baseline by 3.8 points in CoNLL F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | <bold>71.2</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "e5169b8b-aa8f-41af-aaaa-634c71406de9",
    "input": "## Claim\nHere is a claim: Furthermore, we do not see over-fitting in either of the models, even if they are trained on all the data in B-COPA. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large-FT | B-COPA | 74.5 (\u00b1 0.7) | 74.7 (\u00b1 0.4) | [BOLD] 74.4 (\u00b1 0.9)\nBERT-large-FT | B-COPA (50%) | 74.3 (\u00b1 2.2) | 76.8 (\u00b1 1.9) | 72.8 (\u00b1 3.1)\nBERT-large-FT | COPA | [BOLD] 76.5 (\u00b1 2.7) | [BOLD] 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5)\nRoBERTa-large-FT | B-COPA | [BOLD] 89.0 (\u00b1 0.3) | 88.9 (\u00b1 2.1) | [BOLD] 89.0 (\u00b1 0.8)\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (\u00b1 2.2) | 87.4 (\u00b1 1.1) | 85.4 (\u00b1 2.9)\nRoBERTa-large-FT | COPA | 87.7 (\u00b1 0.9) | [BOLD] 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8aac774b-9ded-41b0-8070-26614c5200f2",
    "input": "## Claim\nHere is a claim: The difference between accuracy on Easy and Hard is less pronounced for RoBERTa, but still suggests some reliance on superficial cues. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "4cf92df9-f1dd-4f9c-b370-cae9598acf82",
    "input": "## Claim\nHere is a claim: However, in the all questions set which includes a large percentage of questions without concept words (containing antonym words), the proposed model underperforms GloVe Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VIII: Precision scores for the Semantic Analogy Test\nQuestions Subset | # of Questions Seen | GloVe | Word2Vec | Proposed\nAll | 8783 | 78.94 | 81.03 | 79.96\nAt least one | 1635 | 67.58 | 70.89 | 67.89\nconcept word | 1635 | 67.58 | 70.89 | 67.89\nAll concept words | 110 | 77.27 | 89.09 | 83.64\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "4e46bf68-7ddc-4376-b0b7-b5143661ea93",
    "input": "## Claim\nHere is a claim: The results in Table 2 (top half) for the original setup confirm that the ranking mechanism for TGen is not effective for both WOMs and SER, whereas the SC-LSTM seems to have difficulty scaling to the E2E dataset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Original | TGen\u2212 | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94\nOriginal | [BOLD] Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27\nOriginal | [BOLD] Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80\nOriginal | [BOLD] Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen\u2212 | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37\nCleaned missing | [BOLD] Original | TGen\u2212 | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61\nCleaned missing | [BOLD] Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53\nCleaned missing | [BOLD] Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen\u2212 | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1984f5e0-9737-4eac-8b4e-f3ed389145ee",
    "input": "## Claim\nHere is a claim: over the different entity types, our joint model performs best in within-document coreference. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | <bold>71.2</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "b70cedda-3704-43f2-9ec0-cbf50e7bab85",
    "input": "## Claim\nHere is a claim: In general terms, the results displayed in table 1 show that the rejection method cannot reduce the error of the output predictions when applying a pre-trained black-box classification system to a new domain. Does the following context support or refute the claim?\n\n## Table\nPaper title: Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability\nTable caption: Table 1: Accuracy obtained by training an standalone classifier, applying the API and the proposed wrapper for each domain\n[EMPTY] | [BOLD] BB source acc. | [BOLD] BB target acc. | [BOLD] Non-reject. acc. (10/20/30%) | [BOLD] Class. quality (10/20/30%) | [BOLD] Reject. quality (10/20/30%)\n[BOLD] Apply Yelp BB to SST-2 | 89.18\u00b10.08% | 77.13\u00b10.52% | 82.43\u00b10.22% 88.19\u00b10.50% 93.60\u00b10.16% | 80.40\u00b10.39% 83.11\u00b10.80% 83.05\u00b10.23% | 6.03\u00b10.45 6.04\u00b10.51 4.97\u00b10.07\n[BOLD] Apply SST-2 BB to Yelp | 83.306\u00b10.18% | 82.106\u00b10.88% | 87,98\u00b10.18% 92.13\u00b10.38% 94.19\u00b10.33% | 85.49\u00b10.88% 84.53\u00b10.38% 78.99\u00b10.46% | 8.30\u00b11.63 5.72\u00b10.27 3.73\u00b10.10\n[BOLD] Apply Electronics BB to Music | 86.39\u00b10.22% | 90.38\u00b10.13% | 95.04\u00b10.43% 96.45\u00b10.35% 97.26\u00b10.31% | 90.67\u00b10.88% 83.93\u00b10.67% 75.77\u00b10.54% | 10.7\u00b11.65 4.82\u00b10.35 3.25\u00b10.14\n[BOLD] Apply Music BB to Electronics | 93.10\u00b10.02% | 79.85\u00b10.0% | 83.26\u00b10.41% 87.06\u00b10.55% 90.50\u00b10.29% | 79.97\u00b10.74% 79.93\u00b10.87% 76.81\u00b10.41% | 4.1\u00b10.55 3.80\u00b10.35 3.32\u00b10.09\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "0d4a8de9-4bda-499e-a51c-870a1fed2a55",
    "input": "## Claim\nHere is a claim: Under oracle setup, all models are notably improved due to the higher quality of reranked passages, but our model does not achieve statistically significantly better BLEU scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: Argument Generation with Retrieval, Planning, and Realization\nTable caption: Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.\n[EMPTY] | [ITALIC] w/ System Retrieval  [BOLD] B-2 | [ITALIC] w/ System Retrieval  [BOLD] B-4 | [ITALIC] w/ System Retrieval  [BOLD] R-2 | [ITALIC] w/ System Retrieval  [BOLD] MTR | [ITALIC] w/ System Retrieval  [BOLD] #Word | [ITALIC] w/ System Retrieval  [BOLD] #Sent | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4 | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent\nHuman | - | - | - | - | 66 | 22 | - | - | - | - | 66 | 22\nRetrieval | 7.55 | 1.11 | 8.64 | 14.38 | 123 | 23 | 10.97 | 3.05 | 23.49 | 20.08 | 140 | 21\n[BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [EMPTY] | [EMPTY]\nSeq2seq | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15 | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15\nSeq2seqAug | 8.26 | 2.24 | 13.79 | 15.75 | 78 | 14 | 10.98 | 4.41 | 22.97 | 19.62 | 71 | 14\n[ITALIC] w/o psg | 7.94 | 2.28 | 10.13 | 15.71 | 75 | 12 | 9.89 | 3.34 | 14.20 | 18.40 | 66 | 12\nH&W\u00a0Hua and Wang ( 2018 ) | 3.64 | 0.92 | 8.83 | 11.78 | 51 | 12 | 8.51 | 2.86 | 18.89 | 17.18 | 58 | 12\n[BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [EMPTY] | [EMPTY]\nCANDELA | 12.02\u2217 | [BOLD] 2.99\u2217 | [BOLD] 14.93\u2217 | [BOLD] 16.92\u2217 | 119 | 22 | 15.80\u2217 | [BOLD] 5.00\u2217 | [BOLD] 23.75 | [BOLD] 20.18 | 116 | 22\n[ITALIC] w/o psg | [BOLD] 12.33\u2217 | 2.86\u2217 | 14.53\u2217 | 16.60\u2217 | 123 | 23 | [BOLD] 16.33\u2217 | 4.98\u2217 | 23.65 | 19.94 | 123 | 23\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d661490a-948e-4b22-ad8e-4d11b28c00cb",
    "input": "## Claim\nHere is a claim: Our KnowComb system does not achieve the same level of performance as the state-of-art general coreference system we base it on. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.\nSystem | MUC | BCUB | CEAFe | AVG\nACE | ACE | ACE | ACE | ACE\nIlliCons | [BOLD] 78.17 | 81.64 | [BOLD] 78.45 | [BOLD] 79.42\nKnowComb | 77.51 | [BOLD] 81.97 | 77.44 | 78.97\nOntoNotes | OntoNotes | OntoNotes | OntoNotes | OntoNotes\nIlliCons | 84.10 | [BOLD] 78.30 | [BOLD] 68.74 | [BOLD] 77.05\nKnowComb | [BOLD] 84.33 | 78.02 | 67.95 | 76.76\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d18651b7-1296-4f52-9b4b-fef0290b7507",
    "input": "## Claim\nHere is a claim: the substantial drop in accuracy can be attributed to the different train-test split. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See \u00a72 for model details. * indicates our replication experiments.\nModel | Accuracy\nBigramPMI\u00a0Goodwin et al. ( 2012 ) | 63.4\nPMI\u00a0Gordon et al. ( 2011 ) | 65.4\nPMI+Connectives\u00a0Luo et al. ( 2016 ) | 70.2\nPMI+Con.+Phrase\u00a0Sasaki et al. ( 2017 ) | 71.4\nBERT-large\u00a0Wang et al. ( 2019 ) | 70.5\nBERT-large\u00a0Sap et al. ( 2019 ) | 75.0\nBERT-large\u00a0Li et al. ( 2019 ) | 75.4\nRoBERTa-large (finetuned) | 90.6\nBERT-large (finetuned)* | 76.5 \u00b1 2.7\nRoBERTa-large (finetuned)* | 87.7 \u00b1 0.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c3ab8958-374d-4ec8-9c98-275829bee11f",
    "input": "## Claim\nHere is a claim: the performance of Our Model is better than Rank+ExATT at most recall ratios, which indicates the importance of our match function with fine-grained entity identification. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 1: Precisions on the NYT dataset.\nRecall | 0.1 | 0.2 | 0.3 | 0.4 | AUC\nPCNN+ATT | 0.698 | 0.606 | 0.518 | 0.446 | 0.323\nRank+ExATT | 0.789 | 0.726 | 0.620 | 0.514 | 0.395\nOur Model | 0.788 | [BOLD] 0.743 | [BOLD] 0.654 | [BOLD] 0.546 | [BOLD] 0.397\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "45d2802f-6f82-4ee9-96f9-462ec333cbc2",
    "input": "## Claim\nHere is a claim: [CONTINUE] For LOC, it turns out that candidate selection is a bottleneck: when candidate selection was flawless, the models made only about 12% errors, down from about 57%. Does the following context support or refute the claim?\n\n## Table\nPaper title: Distant Learning for Entity Linking with Automatic Noise Detection\nTable caption: Table 3: % errors on the development set for different named entity types under two settings. (Smaller is better.)\nSystem | All LOC | All ORG | All PER | All MISC | In  [ITALIC] E+ LOC | In  [ITALIC] E+ ORG | In  [ITALIC] E+ PER | In  [ITALIC] E+ MISC\nName matching | 96.26 | 89.48 | 57.38 | 96.60 | 92.32 | 76.87 | 47.40 | 76.29\nMIL | 57.09 | [BOLD] 76.30 | 41.35 | 93.35 | 11.90 | [BOLD] 47.90 | 27.60 | 53.61\nMIL-ND | 57.15 | 77.15 | 35.95 | 92.47 | 12.02 | 49.77 | 20.94 | 47.42\n[ITALIC] \u03c4MIL-ND | [BOLD] 55.15 | 76.56 | [BOLD] 34.03 | [BOLD] 92.15 | [BOLD] 11.14 | 51.18 | [BOLD] 20.59 | [BOLD] 40.00\nSupervised learning | 55.58 | 61.32 | 24.98 | 89.96 | 8.80 | 14.95 | 7.40 | 29.90\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "8c6099dc-368a-44c2-8051-2af00a3c8bdd",
    "input": "## Claim\nHere is a claim: [CONTINUE] Sentiment polarity shifters have a high impact on clustering performance of opinion distance: We find that not utilizing the sentiment polarity shifters, especially in case of datasets \"Video games\" and \"Pornography\" hurts the Opinion Representation phase, and thereby leads to incorrect computation of opinion distance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.\n[EMPTY] | Difference Function | Seanad Abolition | Video Games | Pornography\nOD-parse | Absolute | 0.01 | -0.01 | 0.07\nOD-parse | JS div. | 0.01 | -0.01 | -0.01\nOD-parse | EMD | 0.07 | 0.01 | -0.01\nOD | Absolute | [BOLD] 0.54 | [BOLD] 0.56 | [BOLD] 0.41\nOD | JS div. | 0.07 | -0.01 | -0.02\nOD | EMD | 0.26 | -0.01 | 0.01\nOD (no polarity shifters) | Absolute | 0.23 | 0.08 | 0.04\nOD (no polarity shifters) | JS div. | 0.09 | -0.01 | -0.02\nOD (no polarity shifters) | EMD | 0.10 | 0.01 | -0.01\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "aa9eb3ce-acdd-4019-b442-f8b929d15b84",
    "input": "## Claim\nHere is a claim: [CONTINUE] OD does not significantly outperform OD-parse: We observe that compared to OD-parse, OD is not significantly more accurate. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.\n[EMPTY] | Difference Function | Seanad Abolition | Video Games | Pornography\nOD-parse | Absolute | 0.01 | -0.01 | 0.07\nOD-parse | JS div. | 0.01 | -0.01 | -0.01\nOD-parse | EMD | 0.07 | 0.01 | -0.01\nOD | Absolute | [BOLD] 0.54 | [BOLD] 0.56 | [BOLD] 0.41\nOD | JS div. | 0.07 | -0.01 | -0.02\nOD | EMD | 0.26 | -0.01 | 0.01\nOD (no polarity shifters) | Absolute | 0.23 | 0.08 | 0.04\nOD (no polarity shifters) | JS div. | 0.09 | -0.01 | -0.02\nOD (no polarity shifters) | EMD | 0.10 | 0.01 | -0.01\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "474a4ee7-88be-4e91-abc9-7f5d22b64f62",
    "input": "## Claim\nHere is a claim: To validate Acc, human annotators were asked to judge the style of 150 transferred sentences. We then compute the percentage of machine and human judgments that match. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.\nMetric | Method of validation | Yelp | Lit.\nAcc | % of machine and human judgments that match | 94 | 84\nSim | Spearman\u2019s  [ITALIC] \u03c1 b/w Sim and human ratings of semantic preservation | 0.79 | 0.75\nPP | Spearman\u2019s  [ITALIC] \u03c1 b/w negative PP and human ratings of fluency | 0.81 | 0.67\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "238007ba-b7a6-4b65-9173-b00fdafd9bf2",
    "input": "## Claim\nHere is a claim: [CONTINUE] Moreover, for TVMAX, automatic metrics results are slightly worse than sparsemax and significantly worse than softmax on MSCOCO and similar on Flickr30k. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.\n[EMPTY] | MSCOCO spice | MSCOCO cider | MSCOCO rouge [ITALIC] L | MSCOCO bleu4 | MSCOCO meteor | MSCOCO rep\u2193 | Flickr30k spice | Flickr30k cider | Flickr30k rouge [ITALIC] L | Flickr30k bleu4 | Flickr30k meteor | Flickr30k rep\u2193\nsoftmax | 18.4 | 0.967 | 52.9 | 29.9 | 24.9 | 3.76 | 13.5 | 0.443 | 44.2 | 19.9 | 19.1 | 6.09\nsparsemax | [BOLD] 18.9 | [BOLD] 0.990 | [BOLD] 53.5 | [BOLD] 31.5 | [BOLD] 25.3 | 3.69 | [BOLD] 13.7 | [BOLD] 0.444 | [BOLD] 44.3 | [BOLD] 20.7 | [BOLD] 19.3 | 5.84\nTVmax | 18.5 | 0.974 | 53.1 | 29.9 | 25.1 | [BOLD] 3.17 | 13.3 | 0.438 | 44.2 | 20.5 | 19.0 | [BOLD] 3.97\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e0931d7d-8c19-4823-8c57-1be6544bb616",
    "input": "## Claim\nHere is a claim: On the other hand, our BiLSTM model using contextualized word representation and PCS only obtained 0.72 F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "1efcb145-e05f-4506-a7d7-2f2adbf98715",
    "input": "## Claim\nHere is a claim: word analogies are especially useful for creating and evaluating continuous vector representations, since the solution of many analogy questions requires vector addition. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "97d1d7a0-87be-4a17-92c0-da0ee295a374",
    "input": "## Claim\nHere is a claim: Surprisingly, we observe a decrease of BLEU-2, BLEU-4, ROUGE-2, and METEOR when removing passages from our model input. Does the following context support or refute the claim?\n\n## Table\nPaper title: Argument Generation with Retrieval, Planning, and Realization\nTable caption: Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.\n[EMPTY] | [ITALIC] w/ System Retrieval  [BOLD] B-2 | [ITALIC] w/ System Retrieval  [BOLD] B-4 | [ITALIC] w/ System Retrieval  [BOLD] R-2 | [ITALIC] w/ System Retrieval  [BOLD] MTR | [ITALIC] w/ System Retrieval  [BOLD] #Word | [ITALIC] w/ System Retrieval  [BOLD] #Sent | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4 | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent\nHuman | - | - | - | - | 66 | 22 | - | - | - | - | 66 | 22\nRetrieval | 7.55 | 1.11 | 8.64 | 14.38 | 123 | 23 | 10.97 | 3.05 | 23.49 | 20.08 | 140 | 21\n[BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [EMPTY] | [EMPTY]\nSeq2seq | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15 | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15\nSeq2seqAug | 8.26 | 2.24 | 13.79 | 15.75 | 78 | 14 | 10.98 | 4.41 | 22.97 | 19.62 | 71 | 14\n[ITALIC] w/o psg | 7.94 | 2.28 | 10.13 | 15.71 | 75 | 12 | 9.89 | 3.34 | 14.20 | 18.40 | 66 | 12\nH&W\u00a0Hua and Wang ( 2018 ) | 3.64 | 0.92 | 8.83 | 11.78 | 51 | 12 | 8.51 | 2.86 | 18.89 | 17.18 | 58 | 12\n[BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [EMPTY] | [EMPTY]\nCANDELA | 12.02\u2217 | [BOLD] 2.99\u2217 | [BOLD] 14.93\u2217 | [BOLD] 16.92\u2217 | 119 | 22 | 15.80\u2217 | [BOLD] 5.00\u2217 | [BOLD] 23.75 | [BOLD] 20.18 | 116 | 22\n[ITALIC] w/o psg | [BOLD] 12.33\u2217 | 2.86\u2217 | 14.53\u2217 | 16.60\u2217 | 123 | 23 | [BOLD] 16.33\u2217 | 4.98\u2217 | 23.65 | 19.94 | 123 | 23\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2f5f8672-0cea-486d-bf9b-88ee35183cc5",
    "input": "## Claim\nHere is a claim: The results also show that it is better to compile knowledge into constraints when the knowledge quality is high than add them as features. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nDataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb\n[ITALIC] Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 | [BOLD] 76.41\n[ITALIC] WinoCoref | AntePre | 68.37 | 74.32 | \u2014\u2013 | 88.48 | 88.95 | [BOLD] 89.32\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ba7dd703-d08b-42fa-b1e2-99c6183443eb",
    "input": "## Claim\nHere is a claim: B-COPA is sufficient for training performance models (e.g., BERT-large), as non-fine-tuned models achieve 66.4% on B-COPA, showing that even structural information captured by BERT is not required for reasoning about causality. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large | B-COPA | 70.5 (\u00b1 2.5) | 72.6 (\u00b1 2.3) | [BOLD] 69.1 (\u00b1 2.7)\nBERT-large | B-COPA (50%) | 69.9 (\u00b1 1.9) | 71.2 (\u00b1 1.3) | 69.0 (\u00b1 3.5)\nBERT-large | COPA | [BOLD] 71.7 (\u00b1 0.5) | [BOLD] 80.5 (\u00b1 0.4) | 66.3 (\u00b1 0.8)\nRoBERTa-large | B-COPA | [BOLD] 76.7 (\u00b1 0.8) | 73.3 (\u00b1 1.5) | [BOLD] 78.8 (\u00b1 2.0)\nRoBERTa-large | B-COPA (50%) | 72.4 (\u00b1 2.0) | 72.1 (\u00b1 1.7) | 72.6 (\u00b1 2.1)\nRoBERTa-large | COPA | 76.4 (\u00b1 0.7) | [BOLD] 79.6 (\u00b1 1.0) | 74.4 (\u00b1 1.1)\nBERT-base-NSP | None | [BOLD] 66.4 | 66.2 | [BOLD] 66.7\nBERT-large-NSP | None | 65.0 | [BOLD] 66.9 | 62.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "715f5a99-6ca2-4117-b4df-a55d7bb5833e",
    "input": "## Claim\nHere is a claim: Consequently, with an 8% improvement on average, the hybrid model [CONTINUE] Word Content are increased. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ee9ec2e9-4f60-4b21-a8e8-1e1ffd3c4d73",
    "input": "## Claim\nHere is a claim: In [14], they compare the word vectors generated by word2vec to GloVe and word2sense. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "bded69dc-786d-4fc5-83d4-d2e766361785",
    "input": "## Claim\nHere is a claim: Model wiki.el, trained only on Wikipedia, was the best in the category semantic with no oov words and the overall category with oov words. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluation of Greek Word Embeddings\nTable caption: Table 3: Summary for 3CosAdd and top-1 nearest vectors.\nCategory Semantic | Category no oov words | gr_def 58.42% | gr_neg10 59.33% | cc.el.300  [BOLD] 68.80% | wiki.el 27.20% | gr_cbow_def 31.76% | gr_d300_nosub 60.79% | gr_w2v_sg_n5 52.70%\n[EMPTY] | with oov words | 52.97% | 55.33% | [BOLD] 64.34% | 25.73% | 28.80% | 55.11% | 47.82%\nSyntactic | no oov words | 65.73% | 61.02% | [BOLD] 69.35% | 40.90% | 64.02% | 53.69% | 52.60%\n[EMPTY] | with oov words | [BOLD] 53.95% | 48.69% | 49.43% | 28.42% | 52.54% | 44.06% | 43.13%\nOverall | no oov words | 63.02% | 59.96% | [BOLD] 68.97% | 36.45% | 52.04% | 56.30% | 52.66%\n[EMPTY] | with oov words | 53.60% | 51.00% | [BOLD] 54.60% | 27.50% | 44.30% | 47.90% | 44.80%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3e5908f5-1d9d-47a6-8e48-a0822530dbdb",
    "input": "## Claim\nHere is a claim: [CONTINUE] Results with BERT show that contextual information is valuable for performance improvement. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nModel | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time\nRockt\u00e4schel et\u00a0al. ( 2016 ) | Rockt\u00e4schel et\u00a0al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | -\nThis | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 | [BOLD] 90.49 | 0.696\nThis | GRU | 6.41M | [BOLD] 85.71 | 0.245 | [BOLD] 86.05 | 0.419 | [BOLD] 90.29 | 0.529 | 90.10 | 0.695\nThis | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580\nWork | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555\n[EMPTY] | LRN | 4.25M | 84.88 | [BOLD] 0.209 | 85.06 | [BOLD] 0.223 | 89.98 | [BOLD] 0.488 | 89.93 | [BOLD] 0.506\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d39ee230-80e9-4d79-ae14-922b3fc922e4",
    "input": "## Claim\nHere is a claim: We see different results for Waseem and Hovy (2016) and Waseem (2016). Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 4: Experiment 2, t= \u201cb*tch\u201d\nDataset | Class | \u02c6 [ITALIC] piblack | \u02c6 [ITALIC] piwhite | [ITALIC] t | [ITALIC] p | \u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite\n[ITALIC] Waseem and Hovy | Racism | 0.010 | 0.010 | -0.632 | [EMPTY] | 0.978\n[EMPTY] | Sexism | 0.963 | 0.944 | 20.064 | *** | 1.020\n[ITALIC] Waseem | Racism | 0.011 | 0.011 | -1.254 | [EMPTY] | 0.955\n[EMPTY] | Sexism | 0.349 | 0.290 | 28.803 | *** | 1.203\n[EMPTY] | Racism and sexism | 0.012 | 0.012 | -0.162 | [EMPTY] | 0.995\n[ITALIC] Davidson et al. | Hate | 0.017 | 0.015 | 4.698 | *** | 1.152\n[EMPTY] | Offensive | 0.988 | 0.991 | -6.289 | *** | 0.997\n[ITALIC] Golbeck et al. | Harassment | 0.099 | 0.091 | 6.273 | *** | 1.091\n[ITALIC] Founta et al. | Hate | 0.074 | 0.027 | 46.054 | *** | 2.728\n[EMPTY] | Abusive | 0.925 | 0.968 | -41.396 | *** | 0.956\n[EMPTY] | Spam | 0.010 | 0.010 | 0.000 | [EMPTY] | 1.000\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1563ec9d-c56c-4d98-b401-792e93c5a56d",
    "input": "## Claim\nHere is a claim: It improves by over 20% over a state-of-art general coreference system on Winograd and also outperforms Rahman and Ng (2012) by a margin of 3.3%. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nDataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb\n[ITALIC] Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 | [BOLD] 76.41\n[ITALIC] WinoCoref | AntePre | 68.37 | 74.32 | \u2014\u2013 | 88.48 | 88.95 | [BOLD] 89.32\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "584be382-d99a-4b4e-92c5-bcdb3ef882e9",
    "input": "## Claim\nHere is a claim: [CONTINUE] The effectiveness of our hierarchical attention design is disproved by an accuracy drop of only 1.95% after removing residual connections and the hierarchical stack of our attention modules. Does the following context support or refute the claim?\n\n## Table\nPaper title: Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation\nTable caption: Table 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set. For \u201c- Hierachical-Attn\u201d, we remove the residual connections between the attention modules in the CMR decoders and all the attention memory access are based on the output from the LSTM. For \u201c- MLP\u201d, we further replace the MLP with a single linear layer with the non-linear activation.\n[BOLD] Model | [BOLD] Joint Acc.\nCOMER | 88.64%\n- Hierachical-Attn | 86.69%\n- MLP | 83.24%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e3125f04-5ee3-4b83-957b-861f893cd399",
    "input": "## Claim\nHere is a claim: Comparing POS and SEM tagging (Table 5), we note that higher layer representations do not necessarily improve SEM tagging, while POS tagging does not peak at layer 1. We noticed no improvements in both translation (+0.9 BLEU) and POS and SEM tagging (up to +0.6% accuracy) when using features extracted from an NMT model trained with residual connections (Table 5). Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks\nTable caption: Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.\nUni | POS | 0 87.9 | 1 92.0 | 2 91.7 | 3 91.8 | 4 91.9\nUni | SEM | 81.8 | 87.8 | 87.4 | 87.6 | 88.2\nBi | POS | 87.9 | 93.3 | 92.9 | 93.2 | 92.8\nBi | SEM | 81.9 | 91.3 | 90.8 | 91.9 | 91.9\nRes | POS | 87.9 | 92.5 | 91.9 | 92.0 | 92.4\nRes | SEM | 81.9 | 88.2 | 87.5 | 87.6 | 88.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9be65c57-384f-4389-a499-15fd4ac3ff16",
    "input": "## Claim\nHere is a claim: Dual2seq-LinAMR shows much worse performance than our Dual2seq model and significantly outperforms the Seq2seq baseline. Does the following context support or refute the claim?\n\n## Table\nPaper title: Semantic Neural Machine Translation using AMR\nTable caption: Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.\nSystem | NC-v11 BLEU | NC-v11 TER\u2193 | NC-v11 Meteor | Full BLEU | Full TER\u2193 | Full Meteor\nOpenNMT-tf | 15.1 | 0.6902 | 0.3040 | 24.3 | 0.5567 | 0.4225\nTransformer-tf | 17.1 | 0.6647 | 0.3578 | 25.1 | 0.5537 | 0.4344\nSeq2seq | 16.0 | 0.6695 | 0.3379 | 23.7 | 0.5590 | 0.4258\nDual2seq-LinAMR | 17.3 | 0.6530 | 0.3612 | 24.0 | 0.5643 | 0.4246\nDuel2seq-SRL | 17.2 | 0.6591 | 0.3644 | 23.8 | 0.5626 | 0.4223\nDual2seq-Dep | 17.8 | 0.6516 | 0.3673 | 25.0 | 0.5538 | 0.4328\nDual2seq | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840 | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d732415c-f84a-431c-af8e-7c1fc901c561",
    "input": "## Claim\nHere is a claim: the final scores (lines 3 and 6 of the table) are the actual numbers reported in the paper (Table 2, right-most column). Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\n[EMPTY] | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK\nMQAN | 72.30 | 60.91 | 41.82 | 53.95\n+ coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold>\nESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37\n+ coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "2a2b14fa-d841-40bf-a7fc-5143ce77f391",
    "input": "## Claim\nHere is a claim: the results of these experiments were statistically significant (t-test, p < .001). Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "5db7cece-882f-45e8-96c3-82a786c846c2",
    "input": "## Claim\nHere is a claim: [CONTINUE] Analyzing Table 3, we can observe that all values of precision using the Portuguese corpora have higher scores when compared with the English corpora. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b1e36aae-7b30-4c59-b940-00c04ce3ea16",
    "input": "## Claim\nHere is a claim: shows that humans who participate in the experiment cannot differentiate between the two options in a third of Balanced COPA questions, and hence Balance COPA questions significantly favor one answer choice. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.\nDataset | Accuracy | Fleiss\u2019 kappa  [ITALIC] k\nOriginal COPA | 100.0 | 0.973\nBalanced COPA | 97.0 | 0.798\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "2f553672-527e-49c7-85e8-c13ecb888e56",
    "input": "## Claim\nHere is a claim: For window-based w2 contexts POS disambiguation yields significantly better F scores on lemmatized targets for VN (p \u2264 .005) with no significant difference for WN-N and WN-V (p \u2248 .05). Does the following context support or refute the claim?\n\n## Table\nPaper title: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources\nTable caption: Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.\n[EMPTY] | WN-N P | WN-N R | WN-N F | WN-V P | WN-V R | WN-V F | VN P | VN R | VN F\nContext: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2\ntype | .700 | .654 | .676 | .535 | .474 | .503 | .327 | .309 | .318\nx+POS | .699 | .651 | .674 | .544 | .472 | .505 | .339 | .312 | .325\nlemma | .706 | .660 | .682 | .576 | .520 | .547 | .384 | .360 | .371\nx+POS | <bold>.710</bold> | <bold>.662</bold> | <bold>.685</bold> | <bold>.589</bold> | <bold>.529</bold> | <bold>.557</bold> | <bold>.410</bold> | <bold>.389</bold> | <bold>.399</bold>\nContext: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep\ntype | .712 | .661 | .686 | .545 | .457 | .497 | .324 | .296 | .310\nx+POS | .715 | .659 | .686 | .560 | .464 | .508 | .349 | .320 | .334\nlemma | <bold>.725</bold> | <bold>.668</bold> | <bold>.696</bold> | .591 | .512 | .548 | .408 | .371 | .388\nx+POS | .722 | .666 | .693 | <bold>.609</bold> | <bold>.527</bold> | <bold>.565</bold> | <bold>.412</bold> | <bold>.381</bold> | <bold>.396</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5f0079a8-4eb7-4fbd-8bcc-a2e289bc4562",
    "input": "## Claim\nHere is a claim: In most setups our average case is better than the former best case. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Vector-spaces with Noisy Supervised Lexicons\nTable caption: Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En\u2192De, En\u2192Fi and En\u2192Es improvements are significant at p<0.05 according to ANOVA on the different runs.\nMethod | En\u2192It best | En\u2192It avg | En\u2192It iters | En\u2192De best | En\u2192De avg | En\u2192De iters | En\u2192Fi best | En\u2192Fi avg | En\u2192Fi iters | En\u2192Es best | En\u2192Es avg | En\u2192Es iters\nArtetxe et\u00a0al., 2018b | [BOLD] 48.53 | 48.13 | 573 | 48.47 | 48.19 | 773 | 33.50 | 32.63 | 988 | 37.60 | 37.33 | 808\nNoise-aware Alignment | [BOLD] 48.53 | [BOLD] 48.20 | 471 | [BOLD] 49.67 | [BOLD] 48.89 | 568 | [BOLD] 33.98 | [BOLD] 33.68 | 502 | [BOLD] 38.40 | [BOLD] 37.79 | 551\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "72882c1b-b616-44af-8650-561e948db115",
    "input": "## Claim\nHere is a claim: Furthermore, the PPO agent performs badly as it fails to ask enough questions to establish proper constraints. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6d64d32d-781d-473e-b478-f646adbed3f4",
    "input": "## Claim\nHere is a claim: [CONTINUE] After applying our data augmentation, both the action and slot diversity are improved consistently, [CONTINUE] HDSA has the worse performance and benefits less from data augmentation comparing to our proposed domain-aware multi-decoder network, Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d0b294b5-acf6-4f1a-8068-bd6adadf1140",
    "input": "## Claim\nHere is a claim: however, the sdp information has a clear positive impact on all the relation types. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ba4e9b1e-818d-4a1e-97d9-9d7b71b2e18d",
    "input": "## Claim\nHere is a claim: Supervising path attentions (the PRKGC+NS model) is not effective for improving the human interpretability of generated NLDs. Does the following context support or refute the claim?\n\n## Table\nPaper title: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension\nTable caption: Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).\nModel | Answerability Macro P/R/F | # Answerable | Answer Prec. | Derivation Prec. RG-L (P/R/F) | Derivation Prec. BL-4\nShortest Path | 54.8/55.5/53.2 | 976 | 3.6 | 56.7/38.5/41.5 | 31.3\nPRKGC | 52.6/51.5/50.7 | 1,021 | 45.2 | 40.7/60.7/44.7 | 30.9\nPRKGC+NS | 53.6/54.1/52.1 | 980 | 45.4 | 42.2/61.6/46.1 | 33.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ba0126ce-21f6-46bd-8eff-2d93dfcaa85c",
    "input": "## Claim\nHere is a claim: As expected, the average ranking of samegender pairs is significantly higher than that of different-gender pairs, both for German and Italian, while the difference between the sets in English is much smaller. Does the following context support or refute the claim?\n\n## Table\nPaper title: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?\nTable caption: Table 2: Averages of rankings of the words in same-gender pairs vs. different-gender pairs for Italian and German, along with their differences. Og stands for the original embeddings, Db for the debiased embeddings, and En for English. Each row presents the averages of pairs with the respective scores in SimLex-999 (0\u20134, 4\u20137, 7\u201310).\n[EMPTY] | Italian Same-gender | Italian Diff-Gender | Italian difference | German Same-gender | German Diff-Gender | German difference\n7\u201310 | Og: 4884 | Og: 12947 | Og: 8063 | Og: 5925 | Og: 33604 | Og: 27679\n7\u201310 | Db: 5523 | Db: 7312 | Db: 1789 | Db: 7653 | Db: 26071 | Db: 18418\n7\u201310 | En: 6978 | En: 2467 | En: -4511 | En: 4517 | En: 8666 | En: 4149\n4\u20137 | Og: 10954 | Og: 15838 | Og: 4884 | Og: 19271 | Og: 27256 | Og: 7985\n4\u20137 | Db: 12037 | Db: 12564 | Db: 527 | Db: 24845 | Db: 22970 | Db: -1875\n4\u20137 | En: 15891 | En: 17782 | En: 1891 | En: 13282 | En: 17649 | En: 4367\n0\u20134 | Og: 23314 | Og: 35783 | Og: 12469 | Og: 50983 | Og: 85263 | Og: 34280\n0\u20134 | Db: 26386 | Db: 28067 | Db: 1681 | Db: 60603 | Db: 79081 | Db: 18478\n0\u20134 | En: 57278 | En: 53053 | En: -4225 | En: 41509 | En: 62929 | En: 21420\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "fecfd170-1f8f-4f70-8b18-e211486982f2",
    "input": "## Claim\nHere is a claim: On the other hand, choosing the best hypernym worked very well for DocSub which obtained the best precision for the Portuguese corpora. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1038 | 0.0170 | 0.0490 | 0.0641 | 0.0641 | 0.0613 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1282 | 0.0291 | 0.0410 | 0.0270 | 0.0270 | 0.1154 | 0.0661\nP | PT | Europarl | 0.6185 | 0.3744 | 0.4144 | 0.4394 | 0.4394 | [BOLD] 0.7553 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.6308 | 0.4124 | 0.4404 | 0.4515 | 0.4945 | [BOLD] 0.8609 | 0.5295\nR | EN | Europarl | [BOLD] 0.0021 | 0.0004 | 0.0011 | 0.0014 | 0.0014 | 0.0013 | 0.0017\nR | EN | Ted Talks | 0.0011 | 0.0008 | 0.0011 | 0.0008 | 0.0008 | [BOLD] 0.0030 | 0.0018\nR | PT | Europarl | 0.0012 | 0.0008 | 0.0009 | 0.0010 | 0.0010 | [BOLD] 0.0016 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0003 | 0.0009 | 0.0009 | 0.0010 | 0.0010 | [BOLD] 0.0017 | 0.0011\nF | EN | Europarl | [BOLD] 0.0041 | 0.0007 | 0.0021 | 0.0027 | 0.0027 | 0.0026 | 0.0033\nF | EN | Ted Talks | 0.0022 | 0.0016 | 0.0022 | 0.0015 | 0.0015 | [BOLD] 0.0058 | 0.0036\nF | PT | Europarl | 0.0024 | 0.0016 | 0.0018 | 0.0019 | 0.0019 | [BOLD] 0.0031 | 0.0023\n[EMPTY] | PT | Ted Talks | 0.0005 | 0.0018 | 0.0018 | 0.0020 | 0.0021 | [BOLD] 0.0034 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "de3034a6-6815-43f6-ab74-408ae82f3718",
    "input": "## Claim\nHere is a claim: The improvement is not significant enough to warrant further research into visual modulation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Modulated Self-attention Convolutional Network for VQA\nTable caption: Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\n[BOLD] ResNet-34 | [BOLD] Eval set % | [BOLD] #param\nSA (S: 3 - M: 1) | 55.25 | } 0.082M\n[BOLD] SA (S: 3 - B: 3) | [BOLD] 55.42 | } 0.082M\nSA (S: 3 - B: 4) | 55.33 | } 0.082M\nSA (S: 3 - B: 6) | 55.31 | } 0.082M\nSA (S: 3 - B: 1,3,5) | 55.45 | } 0.245M\n[BOLD] SA (S: 3 - B: 2,4,6) | [BOLD] 55.56 | } 0.245M\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "52f9985b-e7c4-4516-a758-9eebf47cb971",
    "input": "## Claim\nHere is a claim: [CONTINUE] We showed that it is possible to improve the feature extraction procedure for the VQA task by adding self-attention modules in the different ResNet blocks. Does the following context support or refute the claim?\n\n## Table\nPaper title: Modulated Self-attention Convolutional Network for VQA\nTable caption: Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\n[BOLD] ResNet-34 | [BOLD] Eval set % | [BOLD] #param\nBaseline (No SA)Anderson et al. ( 2018 ) | 55.00 | 0M\nSA (S: 1,2,3 - B: 1) | 55.11 | } 0.107M\nSA (S: 1,2,3 - B: 2) | 55.17 | } 0.107M\n[BOLD] SA (S: 1,2,3 - B: 3) | [BOLD] 55.27 | } 0.107M\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "51c11868-1bbf-466a-b95d-621a87c43768",
    "input": "## Claim\nHere is a claim: In fact, DocSub had worse results in precision only when using Europarl corpus in English, where DF reached best values of precision and f-measure. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2de15a72-9055-4a7e-897f-cf0c3c3aeb36",
    "input": "## Claim\nHere is a claim: [CONTINUE] Though ALDM obtains a lower inform F1 and match rate than PPO, it gets a slight improvement [CONTINUE] on task success [CONTINUE] Ablation test is investigated in Table 3. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "47e0e193-299d-4225-aa6f-6dcd0fe79f5c",
    "input": "## Claim\nHere is a claim: Moreover, all agents tend to perform better on booking flights, but worse on booking hotels. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0f61a515-9f46-4502-a2db-a1ef9a6c2148",
    "input": "## Claim\nHere is a claim: we present BLEU and TER for the REV systems in Table 5, [CONTINUE] While RNN models are the best ones according to the evaluation metrics, Does the following context support or refute the claim?\n\n## Table\nPaper title: Lost in Translation: Loss and Decay of Linguistic Richness in Machine Translation\nTable caption: Table 5: Automatic evaluation scores (BLEU and TER) for the REV systems.\nSystem reference | BLEU\u2191 | TER\u2193\nen-fr-rnn-rev | 33.3 | 50.2\nen-fr-smt-rev | 36.5 | 47.1\nen-fr-trans-rev | [BOLD] 36.8 | [BOLD] 46.8\nen-es-rnn-rev | 37.8 | 45.0\nen-es-smt-rev | 39.2 | 44.0\nen-es-trans-rev | [BOLD] 40.4 | [BOLD] 42.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9b0998ee-9b1d-472a-a9dc-19f4dc5c57a8",
    "input": "## Claim\nHere is a claim: [CONTINUE] Wikipedia-PubMed-PMC embeddings (Moen and Ananiadou, 2013) outperforms GloVe (Mikolov et al., 2013a) in the extraction of most relation types (Table 1) [CONTINUE] the combination feature of BoC and sentence embeddings outperforms sentence embeddings alone, but do not exceed the upper boundary of BoC feature, in which again demonstrating the competitiveness of BoC feature. Does the following context support or refute the claim?\n\n## Table\nPaper title: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data\nTable caption: Table 1: Performance of supervised learning models with different features.\nFeature | LR P | LR R | LR F1 | SVM P | SVM R | SVM F1 | ANN P | ANN R | ANN F1\n+BoW | 0.93 | 0.91 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+BoC (Wiki-PubMed-PMC) | 0.94 | 0.92 | [BOLD] 0.93 | 0.94 | 0.92 | [BOLD] 0.93 | 0.91 | 0.91 | [BOLD] 0.91\n+BoC (GloVe) | 0.93 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+ASM | 0.90 | 0.85 | 0.88 | 0.90 | 0.86 | 0.88 | 0.89 | 0.89 | 0.89\n+Sentence Embeddings(SEs) | 0.89 | 0.89 | 0.89 | 0.90 | 0.86 | 0.88 | 0.88 | 0.88 | 0.88\n+BoC(Wiki-PubMed-PMC)+SEs | 0.92 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "e9042900-dce1-494a-a106-5bf07dc8e36d",
    "input": "## Claim\nHere is a claim: This indicates that the number of top sessions and the diversity of human responses may suffer from the hand-crafted reward. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "775b4ac6-9478-4306-b388-b0e8203c4ac0",
    "input": "## Claim\nHere is a claim: [CONTINUE] Finally, image resizing gives another 4% increase. Does the following context support or refute the claim?\n\n## Table\nPaper title: Zero-Shot Grounding of Objects from Natural Language Queries\nTable caption: Table 6: Ablation study: BM=Base Model, softmax means we classify only one candidate box as foreground, BCE = Binary Cross Entropy means we classify each candidate box as the foreground or background, FL = Focal Loss, Img-Resize: use images of dimension 600\u00d7600\nModel | Accuracy on RefClef\nBM + Softmax | 48.54\nBM + BCE | 55.20\nBM + FL | 57.13\nBM + FL + Img-Resize | [BOLD] 61.75\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f3e9ad19-2aaa-4fc1-9416-3d1f84765f21",
    "input": "## Claim\nHere is a claim: an evaluation of the best joint model on the test dataset with the new evaluation scripts (Teresi et al., 2019) gives 71.2 F1, which is slightly higher than the value reported by the organizers of the competition (Teresi et al., 2017), namely 71.1 F1. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | <bold>71.2</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "3197bce9-5af1-44bb-ae78-af57c4346c14",
    "input": "## Claim\nHere is a claim: Most denying instances get misclassified as querying (see Table 5), Does the following context support or refute the claim?\n\n## Table\nPaper title: Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM\nTable caption: Table 5: Confusion matrix for testing set predictions\n[BOLD] LabelPrediction | [BOLD] C | [BOLD] D | [BOLD] Q | [BOLD] S\n[BOLD] Commenting | 760 | 0 | 12 | 6\n[BOLD] Denying | 68 | 0 | 1 | 2\n[BOLD] Querying | 69 | 0 | 36 | 1\n[BOLD] Supporting | 67 | 0 | 1 | 26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1965045b-8626-4983-9653-9a38b7e6b14d",
    "input": "## Claim\nHere is a claim: the classifier succeeded in effectively reducing the number of false cues, in spite of their unpredictable nature. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 4: Cue classification on the test set.\n[EMPTY] | [BOLD] F-Score  [BOLD] Baseline | [BOLD] F-Score  [BOLD] Proposed | [BOLD] Support\nFalse cues | 0.61 | 0.68 | 47\nActual cues | 0.97 | 0.98 | 557\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "919169e5-b0e4-4818-96d5-27895efad28b",
    "input": "## Claim\nHere is a claim: [CONTINUE] When removing sweat smile and confused accuracy decreased. Does the following context support or refute the claim?\n\n## Table\nPaper title: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations\nTable caption: Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.\n[BOLD] Emoji alias | [BOLD] N | [BOLD] emoji # | [BOLD] emoji % | [BOLD] no-emoji # | [BOLD] no-emoji % | [BOLD] \u0394%\nmask | 163 | 154 | 94.48 | 134 | 82.21 | - 12.27\ntwo_hearts | 87 | 81 | 93.10 | 77 | 88.51 | - 4.59\nheart_eyes | 122 | 109 | 89.34 | 103 | 84.43 | - 4.91\nheart | 267 | 237 | 88.76 | 235 | 88.01 | - 0.75\nrage | 92 | 78 | 84.78 | 66 | 71.74 | - 13.04\ncry | 116 | 97 | 83.62 | 83 | 71.55 | - 12.07\nsob | 490 | 363 | 74.08 | 345 | 70.41 | - 3.67\nunamused | 167 | 121 | 72.46 | 116 | 69.46 | - 3.00\nweary | 204 | 140 | 68.63 | 139 | 68.14 | - 0.49\njoy | 978 | 649 | 66.36 | 629 | 64.31 | - 2.05\nsweat_smile | 111 | 73 | 65.77 | 75 | 67.57 | 1.80\nconfused | 77 | 46 | 59.74 | 48 | 62.34 | 2.60\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "48715abe-b59e-4f35-a43a-b1539a2dbfc4",
    "input": "## Claim\nHere is a claim: We can see that the two policy gradient approaches outperform RL using the discriminative model and the value based RL on the majority of the metrics. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "65d85bed-5ff2-4954-92c9-f7e7cfd25951",
    "input": "## Claim\nHere is a claim: The resulting cross-dataset improvements on the SNLI and Glockner datasets are larger than those on the SICK dataset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\n[EMPTY] | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK\nMQAN | 72.30 | 60.91 | 41.82 | 53.95\n+ coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold>\nESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37\n+ coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "6201e247-3ab6-4be7-b8cf-739a3acab517",
    "input": "## Claim\nHere is a claim: This explains why our proposed method achieves the best average reward, and confirms the fact that our proposed policy learns to control the number of turns better than other baselines Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.\nGP-MBCM | ACER | PPO | ALDM | GDPL\n1.666 | 0.775 | 0.639 | 1.069 | [BOLD] 0.238\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ef13c2cf-6271-4c5c-a1ee-17e71aea7566",
    "input": "## Claim\nHere is a claim: [CONTINUE] Relation propagation (RelProp) improves relation extraction performance over both pretrained and fine-tuned BERT. Does the following context support or refute the claim?\n\n## Table\nPaper title: Entity, Relation, and Event Extraction with Contextualized Span Representations\nTable caption: Table 3: F1 scores on Relation.\n[EMPTY] | ACE05 | SciERC | WLPC\nBERT + LSTM | 60.6 | 40.3 | 65.1\n+RelProp | 61.9 | 41.1 | 65.3\n+CorefProp | 59.7 | 42.6 | -\nBERT FineTune | [BOLD] 62.1 | 44.3 | 65.4\n+RelProp | 62.0 | 43.0 | [BOLD] 65.5\n+CorefProp | 60.0 | [BOLD] 45.3 | -\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "b6f67c1c-41b9-4975-ba63-26448b6c1b08",
    "input": "## Claim\nHere is a claim: In addition, our metric also has the highest Pearson correlation with humans. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nMetric | [ITALIC] \u03c1 | [ITALIC] r | G-Pre | G-Rec\nROUGE-1 | .290 | .304 | .392 | .428\nROUGE-2 | .259 | .278 | .408 | .444\nROUGE-L | .274 | .297 | .390 | .426\nROUGE-SU4 | .282 | .279 | .404 | .440\nBLEU-1 | .256 | .281 | .409 | .448\nBLEU-2 | .301 | .312 | .411 | .446\nBLEU-3 | .317 | .312 | .409 | .444\nBLEU-4 | .311 | .307 | .409 | .446\nBLEU-5 | .308 | .303 | .420 | .459\nMETEOR | .305 | .285 | .409 | .444\nInferSent-Cosine | [BOLD] .329 | [BOLD] .339 | .417 | .460\nBERT-Cosine | .312 | .335 | [BOLD] .440 | [BOLD] .484\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "fe71de2d-b005-4e58-81c6-22d84526ca66",
    "input": "## Claim\nHere is a claim: Our summaries are notably longer than in other works, about 260 words on average. Does the following context support or refute the claim?\n\n## Table\nPaper title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\nTable caption: Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.\n[BOLD] Dataset | [BOLD] # pairs | [BOLD] # words (doc) | [BOLD] # sents (docs) | [BOLD] # words (summary) | [BOLD] # sents (summary) | [BOLD] vocab size\nMulti-News | 44,972/5,622/5,622 | 2,103.49 | 82.73 | 263.66 | 9.97 | 666,515\nDUC03+04 | 320 | 4,636.24 | 173.15 | 109.58 | 2.88 | 19,734\nTAC 2011 | 176 | 4,695.70 | 188.43 | 99.70 | 1.00 | 24,672\nCNNDM | 287,227/13,368/11,490 | 810.57 | 39.78 | 56.20 | 3.68 | 717,951\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "70361fad-829f-4e8c-af9b-e23d8acd1375",
    "input": "## Claim\nHere is a claim: The difference is most prevalent in KP20k, the largest of the four datasets, where our GAN model (at 0.85) is nearly 5% better than both the other baseline models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Keyphrase Generation for Scientific Articles using GANs\nTable caption: Table 2: \u03b1-nDCG@5 metrics\nModel | Inspec | Krapivin | NUS | KP20k\nCatseq | 0.87803 | 0.781 | 0.82118 | 0.804\nCatseq-RL | 0.8602 | [BOLD] 0.786 | 0.83 | 0.809\nGAN | [BOLD] 0.891 | 0.771 | [BOLD] 0.853 | [BOLD] 0.85\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "6fceb272-19a2-4482-9526-d93a34ee8ba4",
    "input": "## Claim\nHere is a claim: The ND classifier had a significant positive effect on F1 for the 'In E+' setting. Does the following context support or refute the claim?\n\n## Table\nPaper title: Distant Learning for Entity Linking with Automatic Noise Detection\nTable caption: Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.\nSystem | All P | All R | All F1 | In  [ITALIC] E+ P | In  [ITALIC] E+ R | In  [ITALIC] E+ F1\nName matching | 15.03 | 15.03 | 15.03 | 29.13 | 29.13 | 29.13\nMIL (model 1) | 35.87 | 35.87 | 35.87 \u00b10.72 | 69.38 | 69.38 | 69.38 \u00b11.29\nMIL-ND (model 2) | 37.42 | [BOLD] 37.42 | 37.42 \u00b10.35 | 72.50 | [BOLD] 72.50 | [BOLD] 72.50 \u00b10.68\n[ITALIC] \u03c4MIL-ND (model 2) | [BOLD] 38.91 | 36.73 | [BOLD] 37.78 \u00b10.26 | [BOLD] 73.19 | 71.15 | 72.16 \u00b10.48\nSupervised learning | 42.90 | 42.90 | 42.90 \u00b10.59 | 83.12 | 83.12 | 83.12 \u00b11.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e9ed6a2c-59be-47fd-815e-3a08898c26c5",
    "input": "## Claim\nHere is a claim: We performed an ablation study on a single model having obtained 69.23% accuracy on the validation set. Does the following context support or refute the claim?\n\n## Table\nPaper title: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations\nTable caption: Table 2: Ablation study results.\n[BOLD] Variation | [BOLD] Accuracy (%) | [BOLD] \u0394%\nSubmitted | [BOLD] 69.23 | -\nNo emoji | 68.36 | - 0.87\nNo ELMo | 65.52 | - 3.71\nConcat Pooling | 68.47 | - 0.76\nLSTM hidden=4096 | 69.10 | - 0.13\nLSTM hidden=1024 | 68.93 | - 0.30\nLSTM hidden=512 | 68.43 | - 0.80\nPOS emb dim=100 | 68.99 | - 0.24\nPOS emb dim=75 | 68.61 | - 0.62\nPOS emb dim=50 | 69.33 | + 0.10\nPOS emb dim=25 | 69.21 | - 0.02\nSGD optim lr=1 | 64.33 | - 4.90\nSGD optim lr=0.1 | 66.11 | - 3.12\nSGD optim lr=0.01 | 60.72 | - 8.51\nSGD optim lr=0.001 | 30.49 | - 38.74\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2df5a453-0b55-4883-8f4a-a1486ebbb214",
    "input": "## Claim\nHere is a claim: Overall, none of the implementations can improve the performances of base models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Two Causal Principles for Improving Visual Dialog\nTable caption: Table 1: Performance (NDCG%) comparison for the experiments of applying our principles on the validation set of VisDial v1.0. LF is the enhanced version as we mentioned. QT, S and D denote question type, answer score sampling, and hidden dictionary learning, respectively. R0, R1, R2, R3 denote regressive loss, weighted softmax loss, binary sigmoid loss ,and generalized ranking loss, respectively.\nModel | baseline | QT | S  [ITALIC] R0 | S  [ITALIC] R1 | S  [ITALIC] R2 | S  [ITALIC] R3 | D\nLF\u00a0 | 57.21 | 58.97 | 67.82 | 71.27 | 72.04 | 72.36 | 72.65\nLF +P1 | 61.88 | 62.87 | 69.47 | 72.16 | 72.85 | 73.42 | [BOLD] 73.63\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d697a74d-c867-4d09-950e-3d3f84fef4c5",
    "input": "## Claim\nHere is a claim: As shown in Table 6, the performance of LRN matches that of ATR and SRU, though LSTM and GRU operate better (+1.05 and +0.79). Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 6: F1 score on CoNLL-2003 English NER task. \u201c#Params\u201d: the parameter number in NER task. LSTM* denotes the reported result\u00a0Lample et\u00a0al. (2016).\nModel | #Params | NER\nLSTM* | - | 90.94\nLSTM | 245K | [BOLD] 89.61\nGRU | 192K | 89.35\nATR | 87K | 88.46\nSRU | 161K | 88.89\nLRN | 129K | 88.56\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "dffdd441-9432-4656-830a-adf397ec3283",
    "input": "## Claim\nHere is a claim: Our models DCGCN(single) and DCGCN(ensemble) do not remove the burden of employing a recurrent encoder to extract non-local contextual information in the bottom layers, as evidenced by the results of BoW+GCN, CNN+GCN, and BiRNN+GCN. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 4: Main results on English-German and English-Czech datasets.\n[BOLD] Model | [BOLD] Type | [BOLD] English-German #P | [BOLD] English-German B | [BOLD] English-German C | [BOLD] English-Czech #P | [BOLD] English-Czech B | [BOLD] English-Czech C\nBoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | -\nCNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | -\nBiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | -\nPB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4\nSeq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8\nGGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3\nDCGCN (ours) | Single | [BOLD]  29.7M | [BOLD] 19.0 | [BOLD] 44.1 | [BOLD]  28.3M | [BOLD] 12.1 | [BOLD] 37.1\nSeq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4\nGGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9\nDCGCN (ours) | Ensemble | [BOLD]  149M | [BOLD] 20.5 | [BOLD] 45.8 | [BOLD]  142M | [BOLD] 13.1 | [BOLD] 37.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a1f1fc3a-e648-4af9-84ce-3b59c2a584d5",
    "input": "## Claim\nHere is a claim: Consequently, with an 8% decrease on average, the hybrid model [CONTINUE] Word Content are decreased. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e6ccc836-e7ca-4096-b1ed-6261d09f7a11",
    "input": "## Claim\nHere is a claim: they report one big advantage of our method, which is increasing performance when the correct answer is missing from the training corpus. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VIII: Precision scores for the Semantic Analogy Test\nQuestions Subset | # of Questions Seen | GloVe | Word2Vec | Proposed\nAll | 8783 | 78.94 | 81.03 | 79.96\nAt least one | 1635 | 67.58 | 70.89 | 67.89\nconcept word | 1635 | 67.58 | 70.89 | 67.89\nAll concept words | 110 | 77.27 | 89.09 | 83.64\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "cd352a89-328c-4845-bf4c-d60c006603a7",
    "input": "## Claim\nHere is a claim: On 7 out of 11 supervised tasks, the joint model even improves upon the better model, and on SST2, SST5, and MRPC the difference is more than 1 point. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nMethod | SUBJ | CR | MR | MPQA | MRPC | TREC | SICK-E | SST2 | SST5 | STS-B | SICK-R\nCBOW/784 | 90.0 | [BOLD] 79.2 | [BOLD] 74.0 | 87.1 | 71.6 | 85.6 | 78.9 | 78.5 | 42.1 | 61.0 | [BOLD] 78.1\nCMOW/784 | 87.5 | 73.4 | 70.6 | [BOLD] 87.3 | 69.6 | [BOLD] 88.0 | 77.2 | 74.7 | 37.9 | 56.5 | 76.2\nHybrid | [BOLD] 90.2 | 78.7 | 73.7 | [BOLD] 87.3 | [BOLD] 72.7 | 87.6 | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8\ncmp. CBOW | +0.2% | -0.6% | -0.4% | +0.2% | +1.5% | +2.3% | +0.6% | +1.4% | +2.9% | +3.9% | -0.4%\ncmp. CMOW | +3.1% | +7.2% | +4.4% | +0% | +4.5% | -0.5% | +2.9% | +6.7% | +14.3 | +12.2% | +2.1%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "6d69f58a-3261-43e4-97f0-80e4cbf6ed87",
    "input": "## Claim\nHere is a claim: our model outperforms all the variants significantly under any recall and AUC. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\n-Word-ATT | 0.648 | 0.515 | 0.395 | 0.389\n-Capsule | 0.635 | 0.507 | 0.413 | 0.386\nOur Model | 0.650 | 0.519 | 0.422 | 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "362601b1-3bf5-47a7-a8ab-192050bbeea7",
    "input": "## Claim\nHere is a claim: This suggests that enriching input graphs with the global node and excluding the linear combination can facilitate GCNs to learn better information aggregations, producing more expressive graph representations. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "0522e114-a86c-49ae-a0c1-2291cf0b6e30",
    "input": "## Claim\nHere is a claim: As the table 4 depicts, the training time increases with the growth of d. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 4: Precisions on the Wikidata dataset with different choice of d.\nRecall | 0.1 | 0.2 | 0.3 | AUC | Time\n[ITALIC] d=1 | 0.602 | 0.487 | 0.403 | 0.367 | 4h\n[ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.370 | -\n[ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h\n[ITALIC] d=8 | 0.650 | 0.519 | 0.422 | 0.405 | 8h\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "cb276238-d6d0-427f-9a25-e923b519df9b",
    "input": "## Claim\nHere is a claim: For all batch sizes, the training throughput on the balanced dataset is the highest, while the throughput on the linear dataset is the lowest. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2\nTable caption: Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.\nBatch size | Throughput (instances/s) Balanced | Throughput (instances/s) Moderate | Throughput (instances/s) Linear\n1 | 46.7 | 27.3 | 7.6\n10 | 125.2 | 78.2 | 22.7\n25 | 129.7 | 83.1 | 45.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "87d5b95f-61f8-4387-9fd5-f6678fe07708",
    "input": "## Claim\nHere is a claim: It might be that model generalization is improved when the model is initialized with weights that have been fine-tuned to a challenging dataset, even if this dataset comes from a different domain. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large-FT | B-COPA | 74.5 (\u00b1 0.7) | 74.7 (\u00b1 0.4) | [BOLD] 74.4 (\u00b1 0.9)\nBERT-large-FT | B-COPA (50%) | 74.3 (\u00b1 2.2) | 76.8 (\u00b1 1.9) | 72.8 (\u00b1 3.1)\nBERT-large-FT | COPA | [BOLD] 76.5 (\u00b1 2.7) | [BOLD] 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5)\nRoBERTa-large-FT | B-COPA | [BOLD] 89.0 (\u00b1 0.3) | 88.9 (\u00b1 2.1) | [BOLD] 89.0 (\u00b1 0.8)\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (\u00b1 2.2) | 87.4 (\u00b1 1.1) | 85.4 (\u00b1 2.9)\nRoBERTa-large-FT | COPA | 87.7 (\u00b1 0.9) | [BOLD] 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6cd67271-8961-4a05-9ba2-2f3f773359c6",
    "input": "## Claim\nHere is a claim: Consequently, with an 8% decrease, CMOW is substantially less linguistically informed than CBOW. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "112b9843-9e26-40be-a18d-dfc98e44037f",
    "input": "## Claim\nHere is a claim: Therefore, we have strong evidence that our learned reward can be evaluated and optimized over. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "d0317274-0a8c-4613-b192-9b9d0da2ca72",
    "input": "## Claim\nHere is a claim: [CONTINUE] Dual2seq is signifi [CONTINUE] cantly better than Seq2seq in both settings, [CONTINUE] In particular, the improvement is much larger under the small-scale setting (+3.2 BLEU) than that under the large-scale setting (+1.7 BLEU). Does the following context support or refute the claim?\n\n## Table\nPaper title: Semantic Neural Machine Translation using AMR\nTable caption: Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.\nSystem | NC-v11 BLEU | NC-v11 TER\u2193 | NC-v11 Meteor | Full BLEU | Full TER\u2193 | Full Meteor\nOpenNMT-tf | 15.1 | 0.6902 | 0.3040 | 24.3 | 0.5567 | 0.4225\nTransformer-tf | 17.1 | 0.6647 | 0.3578 | 25.1 | 0.5537 | 0.4344\nSeq2seq | 16.0 | 0.6695 | 0.3379 | 23.7 | 0.5590 | 0.4258\nDual2seq-LinAMR | 17.3 | 0.6530 | 0.3612 | 24.0 | 0.5643 | 0.4246\nDuel2seq-SRL | 17.2 | 0.6591 | 0.3644 | 23.8 | 0.5626 | 0.4223\nDual2seq-Dep | 17.8 | 0.6516 | 0.3673 | 25.0 | 0.5538 | 0.4328\nDual2seq | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840 | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "74bfcde1-c624-4382-8be4-2c3de804b05a",
    "input": "## Claim\nHere is a claim: Results also show the linear combination is more effective than the global node. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5df569a6-e075-4743-8b37-c7182d701cd8",
    "input": "## Claim\nHere is a claim: 2018). Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "a5635632-7ec5-4631-bf69-893fe583a701",
    "input": "## Claim\nHere is a claim: [CONTINUE] Analyzing Table 5 we observe that Patt achieves again the best precision values for the English corpora. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1038 | 0.0170 | 0.0490 | 0.0641 | 0.0641 | 0.0613 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1282 | 0.0291 | 0.0410 | 0.0270 | 0.0270 | 0.1154 | 0.0661\nP | PT | Europarl | 0.6185 | 0.3744 | 0.4144 | 0.4394 | 0.4394 | [BOLD] 0.7553 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.6308 | 0.4124 | 0.4404 | 0.4515 | 0.4945 | [BOLD] 0.8609 | 0.5295\nR | EN | Europarl | [BOLD] 0.0021 | 0.0004 | 0.0011 | 0.0014 | 0.0014 | 0.0013 | 0.0017\nR | EN | Ted Talks | 0.0011 | 0.0008 | 0.0011 | 0.0008 | 0.0008 | [BOLD] 0.0030 | 0.0018\nR | PT | Europarl | 0.0012 | 0.0008 | 0.0009 | 0.0010 | 0.0010 | [BOLD] 0.0016 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0003 | 0.0009 | 0.0009 | 0.0010 | 0.0010 | [BOLD] 0.0017 | 0.0011\nF | EN | Europarl | [BOLD] 0.0041 | 0.0007 | 0.0021 | 0.0027 | 0.0027 | 0.0026 | 0.0033\nF | EN | Ted Talks | 0.0022 | 0.0016 | 0.0022 | 0.0015 | 0.0015 | [BOLD] 0.0058 | 0.0036\nF | PT | Europarl | 0.0024 | 0.0016 | 0.0018 | 0.0019 | 0.0019 | [BOLD] 0.0031 | 0.0023\n[EMPTY] | PT | Ted Talks | 0.0005 | 0.0018 | 0.0018 | 0.0020 | 0.0021 | [BOLD] 0.0034 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d8e62ced-4be2-4ae7-b832-ac7070831958",
    "input": "## Claim\nHere is a claim: the models more often hallucinate additional information, rather than failing to realise part of the MR. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Cleaned | TGen\u2212 | 36.85 | 5.3782 | 35.14 | 55.01 | 1.6016 | 00.34 | 09.81 | 00.15 | 10.31\nOriginal | [BOLD] Cleaned | TGen | 39.23 | 6.0217 | 36.97 | 55.52 | 1.7623 | 00.40 | 03.59 | 00.07 | 04.05\nOriginal | [BOLD] Cleaned | TGen+ | 40.25 | 6.1448 | 37.50 | 56.19 | 1.8181 | 00.21 | 01.99 | 00.05 | 02.24\nOriginal | [BOLD] Cleaned | SC-LSTM | 23.88 | 3.9310 | 32.11 | 39.90 | 0.5036 | 07.73 | 17.76 | 09.52 | 35.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen\u2212 | 40.19 | 6.0543 | 37.38 | 55.88 | 1.8104 | 00.17 | 01.31 | 00.25 | 01.72\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen | 40.73 | 6.1711 | 37.76 | 56.09 | 1.8518 | 00.07 | 00.72 | 00.08 | 00.87\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen+ | 40.51 | 6.1226 | 37.61 | 55.98 | 1.8286 | 00.02 | 00.63 | 00.06 | 00.70\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | SC-LSTM | 23.66 | 3.9511 | 32.93 | 39.29 | 0.3855 | 07.89 | 15.60 | 08.44 | 31.94\nCleaned missing | [BOLD] Cleaned | TGen\u2212 | 40.48 | 6.0269 | 37.26 | 56.19 | 1.7999 | 00.43 | 02.84 | 00.26 | 03.52\nCleaned missing | [BOLD] Cleaned | TGen | 41.57 | 6.2830 | 37.99 | 56.36 | 1.8849 | 00.37 | 01.40 | 00.09 | 01.86\nCleaned missing | [BOLD] Cleaned | TGen+ | 41.56 | 6.2700 | 37.94 | 56.38 | 1.8827 | 00.21 | 01.04 | 00.07 | 01.31\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen\u2212 | 35.99 | 5.0734 | 34.74 | 54.79 | 1.5259 | 00.02 | 11.58 | 00.02 | 11.62\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen | 40.07 | 6.1243 | 37.45 | 55.81 | 1.8026 | 00.05 | 03.23 | 00.01 | 03.29\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+ | 40.80 | 6.2197 | 37.86 | 56.13 | 1.8422 | 00.01 | 01.87 | 00.01 | 01.88\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3bb2c35b-c1d6-48c3-b4a9-3b99298604e0",
    "input": "## Claim\nHere is a claim: This improvement is mainly due to the fact that this model becomes better at predicting entity span boundaries. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | <bold>71.2</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8e524e36-4e8a-4a05-8698-51fa8969dd6e",
    "input": "## Claim\nHere is a claim: The performance increase between Cat1/Cat2 and full data indicates that the existing knowledge schemas and knowledge acquisition are sufficient for further performance improvement. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.\nSchema | AntePre(Test) | AntePre(Train)\nType 1 | 76.67 | 86.79\nType 2 | 79.55 | 88.86\nType 1 (Cat1) | 90.26 | 93.64\nType 2 (Cat2) | 83.38 | 92.49\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "abe5fac5-e3f6-453a-8fbf-4e0cf1ed10d0",
    "input": "## Claim\nHere is a claim: [CONTINUE] We see similar results for Waseem and Hovy (2016) and Waseem (2016). Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 4: Experiment 2, t= \u201cb*tch\u201d\nDataset | Class | \u02c6 [ITALIC] piblack | \u02c6 [ITALIC] piwhite | [ITALIC] t | [ITALIC] p | \u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite\n[ITALIC] Waseem and Hovy | Racism | 0.010 | 0.010 | -0.632 | [EMPTY] | 0.978\n[EMPTY] | Sexism | 0.963 | 0.944 | 20.064 | *** | 1.020\n[ITALIC] Waseem | Racism | 0.011 | 0.011 | -1.254 | [EMPTY] | 0.955\n[EMPTY] | Sexism | 0.349 | 0.290 | 28.803 | *** | 1.203\n[EMPTY] | Racism and sexism | 0.012 | 0.012 | -0.162 | [EMPTY] | 0.995\n[ITALIC] Davidson et al. | Hate | 0.017 | 0.015 | 4.698 | *** | 1.152\n[EMPTY] | Offensive | 0.988 | 0.991 | -6.289 | *** | 0.997\n[ITALIC] Golbeck et al. | Harassment | 0.099 | 0.091 | 6.273 | *** | 1.091\n[ITALIC] Founta et al. | Hate | 0.074 | 0.027 | 46.054 | *** | 2.728\n[EMPTY] | Abusive | 0.925 | 0.968 | -41.396 | *** | 0.956\n[EMPTY] | Spam | 0.010 | 0.010 | 0.000 | [EMPTY] | 1.000\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "33bbe459-fafc-4c3f-93af-70abe946a9f5",
    "input": "## Claim\nHere is a claim: our model imparted 62% more relevant information about the words of the English language than GloVe embeddings. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "169b9c65-30e6-4fa5-8874-293dc2112cce",
    "input": "## Claim\nHere is a claim: The largest loss is by 4% on the CoordInv task. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "45169d93-9391-46ff-8995-4bc3a255b777",
    "input": "## Claim\nHere is a claim: we achieve an increased accuracy of our cue detection classifier in a transductive setting Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 4: Cue classification on the test set.\n[EMPTY] | [BOLD] F-Score  [BOLD] Baseline | [BOLD] F-Score  [BOLD] Proposed | [BOLD] Support\nFalse cues | 0.61 | 0.68 | 47\nActual cues | 0.97 | 0.98 | 557\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e4a34f48-84bd-4a32-a55b-11982d2ab811",
    "input": "## Claim\nHere is a claim: The output of SPINE is not very reliable in the sense that there is no upper bound for distances between vectors and vectors can take any values in R+ for each dimension. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ba1c0ef0-0efa-45dc-9490-9446bd078bec",
    "input": "## Claim\nHere is a claim: using different dimensions may affect the accuracy of predictions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 4: Precisions on the Wikidata dataset with different choice of d.\nRecall | 0.1 | 0.2 | 0.3 | AUC | Time\n[ITALIC] d=1 | 0.602 | 0.487 | 0.403 | 0.367 | 4h\n[ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.370 | -\n[ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h\n[ITALIC] d=8 | 0.650 | 0.519 | 0.422 | 0.405 | 8h\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ed4eb2c0-b3d0-4980-bf4b-9425b6ef4eb9",
    "input": "## Claim\nHere is a claim: The smaller performance gap between Easy and Hard subsets indicates that training on BCOPA encourages BERT and RoBERTa to rely less on superficial cues. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large-FT | B-COPA | 74.5 (\u00b1 0.7) | 74.7 (\u00b1 0.4) | [BOLD] 74.4 (\u00b1 0.9)\nBERT-large-FT | B-COPA (50%) | 74.3 (\u00b1 2.2) | 76.8 (\u00b1 1.9) | 72.8 (\u00b1 3.1)\nBERT-large-FT | COPA | [BOLD] 76.5 (\u00b1 2.7) | [BOLD] 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5)\nRoBERTa-large-FT | B-COPA | [BOLD] 89.0 (\u00b1 0.3) | 88.9 (\u00b1 2.1) | [BOLD] 89.0 (\u00b1 0.8)\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (\u00b1 2.2) | 87.4 (\u00b1 1.1) | 85.4 (\u00b1 2.9)\nRoBERTa-large-FT | COPA | 87.7 (\u00b1 0.9) | [BOLD] 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "498f6ec3-e6dc-4f56-bf77-c79ee97e46e2",
    "input": "## Claim\nHere is a claim: We see that SPINE performs much better on the polarized set than the mixed set, but our model with projected vectors performs better overall, even on the polarized set Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f0e4ce13-671a-44e9-bce5-246e5e76dd4f",
    "input": "## Claim\nHere is a claim: results demonstrate the efficacy of the proposed two-phase learning scheme. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "bf1a3820-efa1-4b5b-bbe1-7810752e51d9",
    "input": "## Claim\nHere is a claim: When comparing DF model which takes into account only the number of documents that the word occurs, with DocSub which considers the number of shared documents between two words, DF achieved better values of precision, but lower values of recall. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1c5d7274-3f2c-4083-b06d-3b41a4df7b59",
    "input": "## Claim\nHere is a claim: This is another evidence of the effectiveness of the multiple-hop distillation with jointly learning agent. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c8665c99-3cac-4c5e-aec7-745269049acf",
    "input": "## Claim\nHere is a claim: Our word embedding model performs similar to existing word embedding based algorithms, although there are many hyperparameters, such as N, h, where the number of features selected in the feature set selection step, in need of extensive hyperparameter tuning. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6fcdbe4f-bff1-4f7e-a83a-58760b3ce74e",
    "input": "## Claim\nHere is a claim: when we reach 100 episodes or more, our greedy agent matches the performance of the extractive-RL model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e68e3a06-166a-404d-a05f-127f61bcc59e",
    "input": "## Claim\nHere is a claim: according to the Figure 3, we can see that the policy layer of GPDL is updating faster than other layers. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "3ff4b1c7-6d1b-4795-a87f-8f2095841c18",
    "input": "## Claim\nHere is a claim: from the empirical results, the number of turns taken by the RL policy is very close to that of the human conversations. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.\nGP-MBCM | ACER | PPO | ALDM | GDPL\n1.666 | 0.775 | 0.639 | 1.069 | [BOLD] 0.238\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "d065dac8-71db-42b4-9bd5-fbe88dab9be2",
    "input": "## Claim\nHere is a claim: Replacing the cue words in a sentence by the alternatives where they belong to leads to contradictory judgment in 37.5% of all sentences. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nCue | App. | Prod. | Cov.\nin | 47 | 55.3 | 9.40\nwas | 55 | 61.8 | 11.0\nto | 82 | 40.2 | 16.4\nthe | 85 | 38.8 | 17.0\na | 106 | 57.5 | 21.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "43e1a4df-038c-4b24-9c68-e1455a5a77a7",
    "input": "## Claim\nHere is a claim: However, the main improvement of SER comes from training on cleaned data with up to 97% error reduction with the ranker and 94% without.11 just cleaning the training data has a much more dramatic effect than just using a semantic control mechanism, such as the reranker (0.97% vs. 4.27% SER). Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Original | TGen\u2212 | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94\nOriginal | [BOLD] Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27\nOriginal | [BOLD] Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80\nOriginal | [BOLD] Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen\u2212 | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37\nCleaned missing | [BOLD] Original | TGen\u2212 | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61\nCleaned missing | [BOLD] Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53\nCleaned missing | [BOLD] Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen\u2212 | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "9602fe65-f85f-4388-81ee-1fd46873e99b",
    "input": "## Claim\nHere is a claim: FME performs better than AME model on both symmetric and asymmetric modes, which shows the advantage of finetuning word embeddings during training. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task\nTable caption: Table 1: Image-caption ranking results for English (Multi30k)\n[EMPTY] | Image to Text R@1 | Image to Text R@5 | Image to Text R@10 | Image to Text Mr | Text to Image R@1 | Text to Image R@5 | Text to Image R@10 | Text to Image Mr | Alignment\n[BOLD] symmetric | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nParallel\u00a0gella:17 | 31.7 | 62.4 | 74.1 | 3 | 24.7 | 53.9 | 65.7 | 5 | -\nUVS\u00a0kiros:15 | 23.0 | 50.7 | 62.9 | 5 | 16.8 | 42.0 | 56.5 | 8 | -\nEmbeddingNet\u00a0wang:18 | 40.7 | 69.7 | 79.2 | - | 29.2 | 59.6 | 71.7 | - | -\nsm-LSTM\u00a0huang:17 | 42.5 | 71.9 | 81.5 | 2 | 30.2 | 60.4 | 72.3 | 3 | -\nVSE++\u00a0faghri:18 | [BOLD] 43.7 | 71.9 | 82.1 | 2 | 32.3 | 60.9 | 72.1 | 3 | -\nMono | 41.4 | 74.2 | 84.2 | 2 | 32.1 | 63.0 | 73.9 | 3 | -\nFME | 39.2 | 71.1 | 82.1 | 2 | 29.7 | 62.5 | 74.1 | 3 | 76.81%\nAME | 43.5 | [BOLD] 77.2 | [BOLD] 85.3 | [BOLD] 2 | [BOLD] 34.0 | [BOLD] 64.2 | [BOLD] 75.4 | [BOLD] 3 | 66.91%\n[BOLD] asymmetric | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nPivot\u00a0gella:17 | 33.8 | 62.8 | 75.2 | 3 | 26.2 | 56.4 | 68.4 | 4 | -\nParallel\u00a0gella:17 | 31.5 | 61.4 | 74.7 | 3 | 27.1 | 56.2 | 66.9 | 4 | -\nMono | 47.7 | 77.1 | 86.9 | 2 | 35.8 | 66.6 | 76.8 | 3 | -\nFME | 44.9 | 76.9 | 86.4 | 2 | 34.2 | 66.1 | 77.1 | 3 | 76.81%\nAME | [BOLD] 50.5 | [BOLD] 79.7 | [BOLD] 88.4 | [BOLD] 1 | [BOLD] 38.0 | [BOLD] 68.5 | [BOLD] 78.4 | [BOLD] 2 | 73.10%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2a9bf464-c3de-47c7-9016-4452eccf3a6b",
    "input": "## Claim\nHere is a claim: , as compared to the original dataset, the balanced dataset requires around two times as many questions to be answered, but has lower inter-annotator agreement and is thus slightly more difficult. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.\nDataset | Accuracy | Fleiss\u2019 kappa  [ITALIC] k\nOriginal COPA | 100.0 | 0.973\nBalanced COPA | 97.0 | 0.798\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "cc03ab7e-6e0d-45fd-a054-8cf38d14a2a1",
    "input": "## Claim\nHere is a claim: After integrating Elmo for contextual modeling, the performance of LRN does not reach the best (76.1 EM and 83.83 F1), with GRU and LSTM outperforming it (+0.33EM, +0.71F1). Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 4: Exact match/F1-score on SQuad dataset. \u201c#Params\u201d: the parameter number of Base. rnet*: results published by\u00a0Wang et\u00a0al. (2017).\nModel | #Params | Base | +Elmo\nrnet* | - | 71.1/79.5 | -/-\nLSTM | 2.67M | [BOLD] 70.46/78.98 | 75.17/82.79\nGRU | 2.31M | 70.41/ [BOLD] 79.15 | 75.81/83.12\nATR | 1.59M | 69.73/78.70 | 75.06/82.76\nSRU | 2.44M | 69.27/78.41 | 74.56/82.50\nLRN | 2.14M | 70.11/78.83 | [BOLD] 76.14/ [BOLD] 83.83\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "91b08a45-84ea-4123-9ff5-e2588de34aa2",
    "input": "## Claim\nHere is a claim: RSI  = 89.20 doesn\u2019t meet the requirement, but we measure the distance as 22.00 in the intrusion test, while we have 8 numbers between 119.99 and 120.00 Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "737a3ba3-f0ff-476c-b995-bb8fc27b877e",
    "input": "## Claim\nHere is a claim: Its productivity of 57.5% expresses that it appears in in correct alternatives 7.5% more often than expected by random chance. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nCue | App. | Prod. | Cov.\nin | 47 | 55.3 | 9.40\nwas | 55 | 61.8 | 11.0\nto | 82 | 40.2 | 16.4\nthe | 85 | 38.8 | 17.0\na | 106 | 57.5 | 21.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "36419694-fbe7-448e-ab77-5a057e25f499",
    "input": "## Claim\nHere is a claim: While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C yields 5 points lower scores on WordContent [CONTINUE] and BigramShift. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 4: Scores for different training objectives on the linguistic probing tasks.\nMethod | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\nCMOW-C | [BOLD] 36.2 | 66.0 | 81.1 | 78.7 | 61.7 | [BOLD] 83.9 | 79.1 | 73.6 | 50.4 | 66.8\nCMOW-R | 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | [BOLD] 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | [BOLD] 74.2 | [BOLD] 50.7 | [BOLD] 72.9\nCBOW-C | [BOLD] 34.3 | [BOLD] 50.5 | [BOLD] 79.8 | [BOLD] 79.9 | 53.0 | [BOLD] 75.9 | [BOLD] 79.8 | [BOLD] 72.9 | 48.6 | 89.0\nCBOW-R | 33.0 | 49.6 | 79.3 | 78.4 | [BOLD] 53.6 | 74.5 | 78.6 | 72.0 | [BOLD] 49.6 | [BOLD] 89.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a859b35d-7e4b-4d4d-b124-9e84252c100b",
    "input": "## Claim\nHere is a claim: The results illustrate the viability of urgency detection in low-supervision settings (with our approach yielding 69.44% F-Measure on Nepal, at 99% significance compared to the Local baseline), with different feature sets contributing differently to the four metrics. Does the following context support or refute the claim?\n\n## Table\nPaper title: Low-supervision urgency detection and transfer in short crisis messages\nTable caption: TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal\nSystem | Accuracy | Precision | Recall | F-Measure\nLocal | 63.97% | 64.27% | 64.50% | 63.93%\nManual | 64.25% | [BOLD] 70.84%\u2217\u2217 | 48.50% | 57.11%\nWiki | 67.25% | 66.51% | 69.50% | 67.76%\nLocal-Manual | 65.75% | 67.96% | 59.50% | 62.96%\nWiki-Local | 67.40% | 65.54% | 68.50% | 66.80%\nWiki-Manual | 67.75% | 70.38% | 63.00% | 65.79%\n[ITALIC] Our Approach | [BOLD] 69.25%\u2217\u2217\u2217 | 68.76% | [BOLD] 70.50%\u2217\u2217 | [BOLD] 69.44%\u2217\u2217\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "26d9a34b-a30a-474c-b97d-4bf387ec919a",
    "input": "## Claim\nHere is a claim: RoBERTa-large-FT was fine-tuned with a much higher learning rate (1e-5) to prevent an under-optimized model. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large-FT | B-COPA | 74.5 (\u00b1 0.7) | 74.7 (\u00b1 0.4) | [BOLD] 74.4 (\u00b1 0.9)\nBERT-large-FT | B-COPA (50%) | 74.3 (\u00b1 2.2) | 76.8 (\u00b1 1.9) | 72.8 (\u00b1 3.1)\nBERT-large-FT | COPA | [BOLD] 76.5 (\u00b1 2.7) | [BOLD] 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5)\nRoBERTa-large-FT | B-COPA | [BOLD] 89.0 (\u00b1 0.3) | 88.9 (\u00b1 2.1) | [BOLD] 89.0 (\u00b1 0.8)\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (\u00b1 2.2) | 87.4 (\u00b1 1.1) | 85.4 (\u00b1 2.9)\nRoBERTa-large-FT | COPA | 87.7 (\u00b1 0.9) | [BOLD] 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "4f42503e-f21e-45b0-83e1-2d986870b0a9",
    "input": "## Claim\nHere is a claim: Table 1 shows that our proposed token level embedding scheme OntoLSTM-PP outperforms the better variant of our baseline LSTM-PP (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%. Does the following context support or refute the claim?\n\n## Table\nPaper title: Ontology-Aware Token Embeddings for Prepositional Phrase Attachment\nTable caption: Table 1: Results on belinkov2014exploring\u2019s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et\u00a0al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Sch\u00fctze (2015) on GloVe.\n[BOLD] System | [BOLD] Initialization | [BOLD] Embedding | [BOLD] Resources | [BOLD] Test Acc.\nHPCD (full) | Syntactic-SG | Type | WordNet, VerbNet | 88.7\nLSTM-PP | GloVe | Type | - | 84.3\nLSTM-PP | GloVe-retro | Type | WordNet | 84.8\nOntoLSTM-PP | GloVe-extended | Token | WordNet | [BOLD] 89.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d3085af2-d938-41fe-8453-0c632cca7716",
    "input": "## Claim\nHere is a claim: BoW+GCN, CNN+GCN and BiRNN+GCN refer to employing the following encoders with a GCN layer on top respectively: 1) a bag-of-words encoder, 2) a one-layer CNN, 3) a bidirectional RNN. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 4: Main results on English-German and English-Czech datasets.\n[BOLD] Model | [BOLD] Type | [BOLD] English-German #P | [BOLD] English-German B | [BOLD] English-German C | [BOLD] English-Czech #P | [BOLD] English-Czech B | [BOLD] English-Czech C\nBoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | -\nCNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | -\nBiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | -\nPB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4\nSeq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8\nGGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3\nDCGCN (ours) | Single | [BOLD]  29.7M | [BOLD] 19.0 | [BOLD] 44.1 | [BOLD]  28.3M | [BOLD] 12.1 | [BOLD] 37.1\nSeq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4\nGGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9\nDCGCN (ours) | Ensemble | [BOLD]  149M | [BOLD] 20.5 | [BOLD] 45.8 | [BOLD]  142M | [BOLD] 13.1 | [BOLD] 37.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2a32cde4-28fb-4f0d-9e67-1a3ef3871e5b",
    "input": "## Claim\nHere is a claim: As occurred in the experiment using the top 1,000 words, this experiment also kept TF with the highest values of f-measure for most methods. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150 | 0.0150 | 0.0445 | 0.0326\nP | EN | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.0060 | 0.0092 | 0.0090 | 0.0356 | 0.0162\nP | PT | Europarl | 0.5710 | 0.1948 | 0.3855 | 0.5474 | 0.4485 | [BOLD] 0.8052 | 0.4058\n[EMPTY] | PT | Ted Talks | [BOLD] 0.6304 | 0.1870 | 0.3250 | 0.5312 | 0.4576 | 0.6064 | 0.3698\nR | EN | Europarl | 0.0037 | 0.3278 | 0.5941 | 0.6486 | [BOLD] 0.6490 | 0.0017 | 0.0003\nR | EN | Ted Talks | 0.0002 | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332 | 0.0967 | 0.0003\nR | PT | Europarl | 0.0002 | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932 | 0.0032 | 0.0001\n[EMPTY] | PT | Ted Talks | 2.10-5 | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887 | 0.1390 | 0.0002\nF | EN | Europarl | 0.0073 | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033 | 0.0006\nF | EN | Ted Talks | 0.0004 | 0.0132 | 0.0118 | 0.0181 | 0.0179 | [BOLD] 0.0520 | 0.0005\nF | PT | Europarl | 0.0005 | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109 | 0.0064 | 0.0002\n[EMPTY] | PT | Ted Talks | 4.10-5 | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149 | 0.2261 | 0.0004\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b9b60316-88c2-497b-a548-5474b6280198",
    "input": "## Claim\nHere is a claim: Pretrained Word2Sense embeddings outperform our method, however it has the advantage of training on a larger corpus. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b2d6a815-61a5-4777-8eaa-3d472f4dfe3f",
    "input": "## Claim\nHere is a claim: Crucially, this performance difference holds even on the hard instances, which have been described as better tests of commonsense (Landauer et al., 1998). Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "808f4732-df2d-4930-8f56-448c21b910eb",
    "input": "## Claim\nHere is a claim: Model wiki.el, trained only on Wikipedia, was the worst almost in every category (and sub-category). Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluation of Greek Word Embeddings\nTable caption: Table 3: Summary for 3CosAdd and top-1 nearest vectors.\nCategory Semantic | Category no oov words | gr_def 58.42% | gr_neg10 59.33% | cc.el.300  [BOLD] 68.80% | wiki.el 27.20% | gr_cbow_def 31.76% | gr_d300_nosub 60.79% | gr_w2v_sg_n5 52.70%\n[EMPTY] | with oov words | 52.97% | 55.33% | [BOLD] 64.34% | 25.73% | 28.80% | 55.11% | 47.82%\nSyntactic | no oov words | 65.73% | 61.02% | [BOLD] 69.35% | 40.90% | 64.02% | 53.69% | 52.60%\n[EMPTY] | with oov words | [BOLD] 53.95% | 48.69% | 49.43% | 28.42% | 52.54% | 44.06% | 43.13%\nOverall | no oov words | 63.02% | 59.96% | [BOLD] 68.97% | 36.45% | 52.04% | 56.30% | 52.66%\n[EMPTY] | with oov words | 53.60% | 51.00% | [BOLD] 54.60% | 27.50% | 44.30% | 47.90% | 44.80%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "95e75058-7410-4ec2-a530-d83517280977",
    "input": "## Claim\nHere is a claim: Table 8 shows the results for the experimental configuration using all available heuristics. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VIII: Precision scores for the Semantic Analogy Test\nQuestions Subset | # of Questions Seen | GloVe | Word2Vec | Proposed\nAll | 8783 | 78.94 | 81.03 | 79.96\nAt least one | 1635 | 67.58 | 70.89 | 67.89\nconcept word | 1635 | 67.58 | 70.89 | 67.89\nAll concept words | 110 | 77.27 | 89.09 | 83.64\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "73f747e6-bd1f-459a-be5d-557593d0128c",
    "input": "## Claim\nHere is a claim: MLP with BERT as en(2018) coder has the best overall performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a2ada531-d61d-4735-836c-cbb20e8c7d46",
    "input": "## Claim\nHere is a claim: When we increase the DCGCN blocks from 1 to 4, the model performance continues increasing on AMR15 development set. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.\n[BOLD] GCN +RC (2) | B 16.8 | C 48.1 | [BOLD] GCN +RC+LA (2) | B 18.3 | C 47.9\n+RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1\n+RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8\n+RC (9) | [BOLD] 21.1 | 50.5 | +RC+LA (9) | [BOLD] 22.0 | 52.6\n+RC (10) | 20.7 | [BOLD] 50.7 | +RC+LA (10) | 21.2 | [BOLD] 52.9\nDCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7\nDCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ced2b584-1a28-45df-92fc-3613d7dbcf34",
    "input": "## Claim\nHere is a claim: Similarly, when DCGCN3 and DCGCN4 contain 18.6M and 18.4M parameters. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 7: Comparisons of different DCGCN models under almost the same parameter budget.\n[BOLD] Model | D | #P | B | C\nDCGCN(1) | 300 | 10.9M | 20.9 | 52.0\nDCGCN(2) | 180 | 10.9M | [BOLD] 22.2 | [BOLD] 52.3\nDCGCN(2) | 240 | 11.3M | 22.8 | 52.8\nDCGCN(4) | 180 | 11.4M | [BOLD] 23.4 | [BOLD] 53.4\nDCGCN(1) | 420 | 12.6M | 22.2 | 52.4\nDCGCN(2) | 300 | 12.5M | 23.8 | 53.8\nDCGCN(3) | 240 | 12.3M | [BOLD] 23.9 | [BOLD] 54.1\nDCGCN(2) | 360 | 14.0M | 24.2 | [BOLD] 54.4\nDCGCN(3) | 300 | 14.0M | [BOLD] 24.4 | 54.2\nDCGCN(2) | 420 | 15.6M | 24.1 | 53.7\nDCGCN(4) | 300 | 15.6M | [BOLD] 24.6 | [BOLD] 54.8\nDCGCN(3) | 420 | 18.6M | 24.5 | 54.6\nDCGCN(4) | 360 | 18.4M | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2496440e-30cf-465a-8de6-814a13e9b1f5",
    "input": "## Claim\nHere is a claim: OntoLSTM-PP also outperforms HPCD (full), the previous best result on this dataset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Ontology-Aware Token Embeddings for Prepositional Phrase Attachment\nTable caption: Table 1: Results on belinkov2014exploring\u2019s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et\u00a0al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Sch\u00fctze (2015) on GloVe.\n[BOLD] System | [BOLD] Initialization | [BOLD] Embedding | [BOLD] Resources | [BOLD] Test Acc.\nHPCD (full) | Syntactic-SG | Type | WordNet, VerbNet | 88.7\nLSTM-PP | GloVe | Type | - | 84.3\nLSTM-PP | GloVe-retro | Type | WordNet | 84.8\nOntoLSTM-PP | GloVe-extended | Token | WordNet | [BOLD] 89.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7643b874-54d1-4b3e-a2f6-d022395c9e6d",
    "input": "## Claim\nHere is a claim: For example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9). Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.\n[BOLD] GCN +RC (2) | B 16.8 | C 48.1 | [BOLD] GCN +RC+LA (2) | B 18.3 | C 47.9\n+RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1\n+RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8\n+RC (9) | [BOLD] 21.1 | 50.5 | +RC+LA (9) | [BOLD] 22.0 | 52.6\n+RC (10) | 20.7 | [BOLD] 50.7 | +RC+LA (10) | 21.2 | [BOLD] 52.9\nDCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7\nDCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "88a0b5f9-de39-4d20-be1b-987819a85d13",
    "input": "## Claim\nHere is a claim: The domain prediction module (DPM) used in our GDPL and GDPL-discr is also trained and tested using their public codes in the end-to-end ALDM. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8a7394db-ec6e-4ef6-845a-0eaeac590412",
    "input": "## Claim\nHere is a claim: However, the overall results in the English language show that, compared to the current state-of-the-art word embeddings models, a subspace was yet to be found that we could improve upon without jeopardizing the system for these tasks. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VI: Correlations for Word Similarity Tests\nDataset (EN-) | GloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\nWS-353-ALL | 0.612 | 0.7156 | 0.634 | 0.622 | 0.173 | 0.690 | 0.657\nSIMLEX-999 | 0.359 | 0.3939 | 0.295 | 0.355 | 0.090 | 0.380 | 0.381\nVERB-143 | 0.326 | 0.4430 | 0.255 | 0.271 | 0.293 | 0.271 | 0.348\nSimVerb-3500 | 0.193 | 0.2856 | 0.184 | 0.197 | 0.035 | 0.234 | 0.245\nWS-353-REL | 0.578 | 0.6457 | 0.595 | 0.578 | 0.134 | 0.695 | 0.619\nRW-STANF. | 0.378 | 0.4858 | 0.316 | 0.373 | 0.122 | 0.390 | 0.382\nYP-130 | 0.524 | 0.5211 | 0.353 | 0.482 | 0.169 | 0.420 | 0.589\nMEN-TR-3k | 0.710 | 0.7528 | 0.684 | 0.696 | 0.298 | 0.769 | 0.725\nRG-65 | 0.768 | 0.8051 | 0.736 | 0.732 | 0.338 | 0.761 | 0.774\nMTurk-771 | 0.650 | 0.6712 | 0.593 | 0.623 | 0.199 | 0.665 | 0.671\nWS-353-SIM | 0.682 | 0.7883 | 0.713 | 0.702 | 0.220 | 0.720 | 0.720\nMC-30 | 0.749 | 0.8112 | 0.799 | 0.726 | 0.330 | 0.735 | 0.776\nMTurk-287 | 0.649 | 0.6645 | 0.591 | 0.631 | 0.295 | 0.674 | 0.634\nAverage | 0.552 | 0.6141 | 0.519 | 0.538 | 0.207 | 0.570 | 0.579\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0e60f569-bde5-4ada-8fbc-6176240824bf",
    "input": "## Claim\nHere is a claim: [CONTINUE] After applying our data augmentation, both the action and slot diversity are improved consistently, [CONTINUE] HDSA has the better performance and benefits more from data augmentation comparing to our proposed domain-aware multi-decoder network. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a2e66d42-21d8-4914-9262-6b5dac5f738d",
    "input": "## Claim\nHere is a claim: Table II shows that Nepal is roughly balanced, while Kerala is imbalanced. Does the following context support or refute the claim?\n\n## Table\nPaper title: Low-supervision urgency detection and transfer in short crisis messages\nTable caption: TABLE II: Details on datasets used for experiments.\nDataset | Unlabeled / Labeled Messages | Urgent / Non-urgent Messages | Unique Tokens | Avg. Tokens / Message | Time Range\nNepal | 6,063/400 | 201/199 | 1,641 | 14 | 04/05/2015-05/06/2015\nMacedonia | 0/205 | 92/113 | 129 | 18 | 09/18/2018-09/21/2018\nKerala | 92,046/400 | 125/275 | 19,393 | 15 | 08/17/2018-08/22/2018\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2fc011a7-d4b3-4564-aad7-966f8e4fa993",
    "input": "## Claim\nHere is a claim: We report the two best performance for slot filling, for which we trained one system without ontology and another without ontology and coarse-grained slot types (Acc.) Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "2ff59490-1779-47f3-828f-e2c1600f71e9",
    "input": "## Claim\nHere is a claim: On the WinoCoref dataset, KnowComb does not improve by 15%. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nDataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb\n[ITALIC] Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 | [BOLD] 76.41\n[ITALIC] WinoCoref | AntePre | 68.37 | 74.32 | \u2014\u2013 | 88.48 | 88.95 | [BOLD] 89.32\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "35404383-b2cd-4672-9a7f-88226e5ef712",
    "input": "## Claim\nHere is a claim: On the other hand, compared to the BiLSTM baseline, PCS introduces significantly more in-scope (1,039) than out-of-scope (298) relations Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f658a810-80dc-48f8-b9fc-5445844a411c",
    "input": "## Claim\nHere is a claim: However, EWC does not outperform no-reg and L2 on News, as it only gives a 0.5 BLEU improvement over the baseline News model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 4: Test BLEU for en-de adaptive training, with sequential adaptation to a third task. EWC-tuned models give the best performance on each domain.\n[EMPTY] | [BOLD] Training scheme | [BOLD] News | [BOLD] TED | [BOLD] IT\n1 | News | 37.8 | 25.3 | 35.3\n2 | TED | 23.7 | 24.1 | 14.4\n3 | IT | 1.6 | 1.8 | 39.6\n4 | News and TED | 38.2 | 25.5 | 35.4\n5 | 1 then TED, No-reg | 30.6 | [BOLD] 27.0 | 22.1\n6 | 1 then TED, L2 | 37.9 | 26.7 | 31.8\n7 | 1 then TED, EWC | [BOLD] 38.3 | [BOLD] 27.0 | 33.1\n8 | 5 then IT, No-reg | 8.0 | 6.9 | 56.3\n9 | 6 then IT, L2 | 32.3 | 22.6 | 56.9\n10 | 7 then IT, EWC | 35.8 | 24.6 | [BOLD] 57.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "7fe42729-fa71-4447-b95a-5048ec662bce",
    "input": "## Claim\nHere is a claim: [CONTINUE] As the results of applying the co-occurrence baseline (\u03c1 = 0) shows (Table 2), the semantic relations in this data are not strongly concentrated within a sentence boundary, as evidenced by the relatively low F1 scores for the relation of TestTiming (0.90) and TestFinding (0.76). Does the following context support or refute the claim?\n\n## Table\nPaper title: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data\nTable caption: Table 2: F1 score results per relation type of the best performing models.\nRelation type | Count | Intra-sentential co-occ.  [ITALIC] \u03c1=0 | Intra-sentential co-occ.  [ITALIC] \u03c1=5 | Intra-sentential co-occ.  [ITALIC] \u03c1=10 | BoC(Wiki-PubMed-PMC) LR | BoC(Wiki-PubMed-PMC) SVM | BoC(Wiki-PubMed-PMC) ANN\nTherapyTiming(TP,TD) | 428 | [BOLD] 0.84 | 0.59 | 0.47 | 0.78 | 0.81 | 0.78\nNextReview(Followup,TP) | 164 | [BOLD] 0.90 | 0.83 | 0.63 | 0.86 | 0.88 | 0.84\nToxicity(TP,CF/TR) | 163 | [BOLD] 0.91 | 0.77 | 0.55 | 0.85 | 0.86 | 0.86\nTestTiming(TN,TD/TP) | 184 | 0.90 | 0.81 | 0.42 | 0.96 | [BOLD] 0.97 | 0.95\nTestFinding(TN,TR) | 136 | 0.76 | 0.60 | 0.44 | [BOLD] 0.82 | 0.79 | 0.78\nThreat(O,CF/TR) | 32 | 0.85 | 0.69 | 0.54 | [BOLD] 0.95 | [BOLD] 0.95 | 0.92\nIntervention(TP,YR) | 5 | [BOLD] 0.88 | 0.65 | 0.47 | - | - | -\nEffectOf(Com,CF) | 3 | [BOLD] 0.92 | 0.62 | 0.23 | - | - | -\nSeverity(CF,CS) | 75 | [BOLD] 0.61 | 0.53 | 0.47 | 0.52 | 0.55 | 0.51\nRecurLink(YR,YR/CF) | 7 | [BOLD] 1.0 | [BOLD] 1.0 | 0.64 | - | - | -\nRecurInfer(NR/YR,TR) | 51 | 0.97 | 0.69 | 0.43 | [BOLD] 0.99 | [BOLD] 0.99 | 0.98\nGetOpinion(Referral,CF/other) | 4 | [BOLD] 0.75 | [BOLD] 0.75 | 0.5 | - | - | -\nContext(Dis,DisCont) | 40 | [BOLD] 0.70 | 0.63 | 0.53 | 0.60 | 0.41 | 0.57\nTestToAssess(TN,CF/TR) | 36 | 0.76 | 0.66 | 0.36 | [BOLD] 0.92 | [BOLD] 0.92 | 0.91\nTimeStamp(TD,TP) | 221 | [BOLD] 0.88 | 0.83 | 0.50 | 0.86 | 0.85 | 0.83\nTimeLink(TP,TP) | 20 | [BOLD] 0.92 | 0.85 | 0.45 | 0.91 | [BOLD] 0.92 | 0.90\nOverall | 1569 | 0.90 | 0.73 | 0.45 | 0.92 | [BOLD] 0.93 | 0.91\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9272ba5a-c8cb-4f21-97b1-40bf8f2b4834",
    "input": "## Claim\nHere is a claim: Compared to the original metapath2vec model with default d, by leveraging the right d, we improve performance at a better cost-efficiency. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 4: Precisions on the Wikidata dataset with different choice of d.\nRecall | 0.1 | 0.2 | 0.3 | AUC | Time\n[ITALIC] d=1 | 0.602 | 0.487 | 0.403 | 0.367 | 4h\n[ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.370 | -\n[ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h\n[ITALIC] d=8 | 0.650 | 0.519 | 0.422 | 0.405 | 8h\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8dd82b54-3d4d-4100-a866-be3f6c35160f",
    "input": "## Claim\nHere is a claim: Our joint model does not improve upon the strong lemma baseline by 3.8 points in CoNLL F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | <bold>71.2</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2a9eb10f-7dee-4dc5-8c9a-663a90cb7c87",
    "input": "## Claim\nHere is a claim: Our agent does not outperform the comparison agents with a large margin. Does the following context support or refute the claim?\n\n## Table\nPaper title: Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation\nTable caption: Table 6: Results of the Human Rating on CWC.\n[EMPTY] | Ours Better(%) | No Prefer(%) | Ours Worse(%)\nRetrieval-Stgy\u00a0 | [BOLD] 62 | 22 | 16\nPMI\u00a0 | [BOLD] 54 | 32 | 14\nNeural\u00a0 | [BOLD] 60 | 22 | 18\nKernel\u00a0 | [BOLD] 62 | 26 | 12\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "0c2c32c2-8636-49ee-8d00-5c4a3bdae286",
    "input": "## Claim\nHere is a claim: As the best comparison model, we investigate ablation models by removing parts from our model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\n-Word-ATT | 0.648 | 0.515 | 0.395 | 0.389\n-Capsule | 0.635 | 0.507 | 0.413 | 0.386\nOur Model | 0.650 | 0.519 | 0.422 | 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "fb607d1d-2d8e-4b05-ac94-895e601e5682",
    "input": "## Claim\nHere is a claim: However, our proposed method has comparable performance with the original GloVe embeddings. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ab2c2f1d-1b29-4c35-81bd-e2ea6c8073c8",
    "input": "## Claim\nHere is a claim: In general, our principle P2 can improve all the models in any ablative condition (i.e., P1, P2, P1+P2), while P1 does not always lead to an improvement. Does the following context support or refute the claim?\n\n## Table\nPaper title: Two Causal Principles for Improving Visual Dialog\nTable caption: Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table\u00a01. Note that only applying P2 is implemented by the implementations in Section\u00a05 with the history shortcut.\nModel | LF\u00a0 | HCIAE\u00a0 | CoAtt\u00a0 | RvA\u00a0\nbaseline | 57.21 | 56.98 | 56.46 | 56.74\n+P1 | 61.88 | 60.12 | 60.27 | 61.02\n+P2 | 72.65 | 71.50 | 71.41 | 71.44\n+P1+P2 | [BOLD] 73.63 | 71.99 | 71.87 | 72.88\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ca69f7e8-9e0e-4870-859c-649e8f88eceb",
    "input": "## Claim\nHere is a claim: By considering only adjectives, we obtain a measure of the positive and negative score for each sentence before and after fine-tuning. Does the following context support or refute the claim?\n\n## Table\nPaper title: What do Deep Networks Like to Read?\nTable caption: Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.\n[EMPTY] | <bold>RNN</bold> | <bold>CNN</bold> | <bold>DAN</bold>\nPositive | +9.7 | +4.3 | +<bold>23.6</bold>\nNegative | +6.9 | +5.5 | +<bold>16.1</bold>\nFlipped to Positive | +20.2 | +24.9 | +27.4\nFlipped to Negative | +31.5 | +28.6 | +19.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f30d3d9e-a1e3-4e50-a778-882897039098",
    "input": "## Claim\nHere is a claim: [CONTINUE] TRANSFORMER-MULTI is stronger than TRANSFORMER-SINGLE [CONTINUE] .2% overall improvement over TRANSFORMER-SINGLE for the goldtwo-mention task. Does the following context support or refute the claim?\n\n## Table\nPaper title: Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns\nTable caption: Table 6: Performance of our baselines on the development set. Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.\n[EMPTY] | M | F | B | O\nRandom | 43.6 | 39.3 | [ITALIC] 0.90 | 41.5\nToken Distance | 50.1 | 42.4 | [ITALIC] 0.85 | 46.4\nTopical Entity | 51.5 | 43.7 | [ITALIC] 0.85 | 47.7\nSyntactic Distance | 63.0 | 56.2 | [ITALIC] 0.89 | 59.7\nParallelism | [BOLD] 67.1 | [BOLD] 63.1 | [ITALIC]  [BOLD] 0.94 | [BOLD] 65.2\nParallelism+URL | [BOLD] 71.1 | [BOLD] 66.9 | [ITALIC]  [BOLD] 0.94 | [BOLD] 69.0\nTransformer-Single | 58.6 | 51.2 | [ITALIC] 0.87 | 55.0\nTransformer-Multi | 59.3 | 52.9 | [ITALIC] 0.89 | 56.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5bf06f43-9f2d-4b94-a6da-8fc836362016",
    "input": "## Claim\nHere is a claim: In other words, [CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe no significant effect on SER from cleaning the missed slots. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Original | TGen\u2212 | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94\nOriginal | [BOLD] Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27\nOriginal | [BOLD] Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80\nOriginal | [BOLD] Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen\u2212 | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37\nCleaned missing | [BOLD] Original | TGen\u2212 | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61\nCleaned missing | [BOLD] Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53\nCleaned missing | [BOLD] Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen\u2212 | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1c7fce05-5dac-4840-9535-6fd7e573863a",
    "input": "## Claim\nHere is a claim: Systems A-C are trained without the target type from which they report. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "aad0d5bd-5420-41cc-88eb-af2eaa32aac1",
    "input": "## Claim\nHere is a claim: In terms of relative numbers, the hybrid model improves upon CBOW in all probing tasks but WC and SOMO. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "dfb953a4-cb91-45be-98c4-5977336f6d6c",
    "input": "## Claim\nHere is a claim: StateNet PSI does not outperform StateNet, and StateNet PS performs best among all 3 models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Universal Dialogue State Tracking\nTable caption: Table 1: Joint goal accuracy on DSTC2 and WOZ 2.0 test set vs.\u00a0various approaches as reported in the literature.\n[BOLD] DST Models | [BOLD] Joint Acc. DSTC2 | [BOLD] Joint Acc. WOZ 2.0\nDelexicalisation-Based (DB) Model Mrk\u0161i\u0107 et\u00a0al. ( 2017 ) | 69.1 | 70.8\nDB Model + Semantic Dictionary Mrk\u0161i\u0107 et\u00a0al. ( 2017 ) | 72.9 | 83.7\nScalable Multi-domain DST Rastogi et\u00a0al. ( 2017 ) | 70.3 | -\nMemN2N Perez and Liu ( 2017 ) | 74.0 | -\nPtrNet Xu and Hu ( 2018 ) | 72.1 | -\nNeural Belief Tracker: NBT-DNN Mrk\u0161i\u0107 et\u00a0al. ( 2017 ) | 72.6 | 84.4\nNeural Belief Tracker: NBT-CNN Mrk\u0161i\u0107 et\u00a0al. ( 2017 ) | 73.4 | 84.2\nBelief Tracking: Bi-LSTM Ramadan et\u00a0al. ( 2018 ) | - | 85.1\nBelief Tracking: CNN Ramadan et\u00a0al. ( 2018 ) | - | 85.5\nGLAD Zhong et\u00a0al. ( 2018 ) | 74.5 | 88.1\nStateNet | 74.1 | 87.8\nStateNet_PS | 74.5 | 88.2\n[BOLD] StateNet_PSI | [BOLD] 75.5 | [BOLD] 88.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6468a0b4-715d-495e-9627-1f859a1198cb",
    "input": "## Claim\nHere is a claim: In this task, ATR and SRU outperform LRN in terms of both EM and F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 4: Exact match/F1-score on SQuad dataset. \u201c#Params\u201d: the parameter number of Base. rnet*: results published by\u00a0Wang et\u00a0al. (2017).\nModel | #Params | Base | +Elmo\nrnet* | - | 71.1/79.5 | -/-\nLSTM | 2.67M | [BOLD] 70.46/78.98 | 75.17/82.79\nGRU | 2.31M | 70.41/ [BOLD] 79.15 | 75.81/83.12\nATR | 1.59M | 69.73/78.70 | 75.06/82.76\nSRU | 2.44M | 69.27/78.41 | 74.56/82.50\nLRN | 2.14M | 70.11/78.83 | [BOLD] 76.14/ [BOLD] 83.83\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a8cb7bdd-dbe4-4991-8ecd-99ee870ab569",
    "input": "## Claim\nHere is a claim: We then compare BERT and RoBERTa with previous models on the Easy and Hard subsets. As Table 4 shows, previous models perform significantly better on the Easy subset than on the Hard subset, with the exception of Sasaki et al. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d244fa95-6080-43f6-869e-02dcacc260ce",
    "input": "## Claim\nHere is a claim: [CONTINUE] Negations are uncovered through unigrams (not, no, won't) [CONTINUE] Several unigrams (error, issue, working, fix) [CONTINUE] Words regularly describing negative sentiment or emotions (such as 'not', 'my', and 'can't') are among the most distinctive features for complaints. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\n[BOLD] Complaints  [BOLD] Feature | [BOLD] Complaints  [ITALIC] r | [BOLD] Not Complaints  [BOLD] Feature | [BOLD] Not Complaints  [ITALIC] r\n[BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams\nnot | .154 | [URL] | .150\nmy | .131 | ! | .082\nworking | .124 | he | .069\nstill | .123 | thank | .067\non | .119 | , | .064\ncan\u2019t | .113 | love | .064\nservice | .112 | lol | .061\ncustomer | .109 | you | .060\nwhy | .108 | great | .058\nwebsite | .107 | win | .058\nno | .104 | \u2019 | .058\n? | .098 | she | .054\nfix | .093 | : | .053\nwon\u2019t | .092 | that | .053\nbeen | .090 | more | .052\nissue | .089 | it | .052\ndays | .088 | would | .051\nerror | .087 | him | .047\nis | .084 | life | .046\ncharged | .083 | good | .046\n[BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)\nVBN | .141 | UH | .104\n$ | .118 | NNP | .098\nVBZ | .114 | PRP | .076\nNN_VBZ | .114 | HT | .076\nPRP$ | .107 | PRP_. | .076\nPRP$_NN | .105 | PRP_RB | .067\nVBG | .093 | NNP_NNP | .062\nCD | .092 | VBP_PRP | .054\nWRB_VBZ | .084 | JJ | .053\nVBZ_VBN | .084 | DT_JJ | .051\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4e1bee90-35f0-4df8-ba48-b2a758e2d9d6",
    "input": "## Claim\nHere is a claim: [CONTINUE] Turning to SEM tagging (Table 3, second block), representations from layers 1 through 4 only marginally boost the performance to around 87-88%, [CONTINUE] which is not significantly higher than the UnsupEmb and MFT baselines. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks\nTable caption: Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. \u201cEn\u201d column is an English autoencoder. BLEU scores are given for reference.\n[ITALIC] k | Ar | Es | Fr | Ru | Zh | En\nPOS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy\n0 | 88.0 | 87.9 | 87.9 | 87.8 | 87.7 | 87.4\n1 | 92.4 | 91.9 | 92.1 | 92.1 | 91.5 | 89.4\n2 | 91.9 | 91.8 | 91.8 | 91.8 | 91.3 | 88.3\n3 | 92.0 | 92.3 | 92.1 | 91.6 | 91.2 | 87.9\n4 | 92.1 | 92.4 | 92.5 | 92.0 | 90.5 | 86.9\nSEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy\n0 | 81.9 | 81.9 | 81.8 | 81.8 | 81.8 | 81.2\n1 | 87.9 | 87.7 | 87.8 | 87.9 | 87.7 | 84.5\n2 | 87.4 | 87.5 | 87.4 | 87.3 | 87.2 | 83.2\n3 | 87.8 | 87.9 | 87.9 | 87.3 | 87.3 | 82.9\n4 | 88.3 | 88.6 | 88.4 | 88.1 | 87.7 | 82.1\nBLEU | BLEU | BLEU | BLEU | BLEU | BLEU | BLEU\n[EMPTY] | 32.7 | 49.1 | 38.5 | 34.2 | 32.1 | 96.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "59e2114d-5576-4698-9ac1-bea4da38592d",
    "input": "## Claim\nHere is a claim: Dual2seq is not consistently better than the other systems under all three metrics, [CONTINUE] as OpenNMT-tf and Transformer-tf both outperform Dual2seq in terms of BLEU and Meteor scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: Semantic Neural Machine Translation using AMR\nTable caption: Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.\nSystem | NC-v11 BLEU | NC-v11 TER\u2193 | NC-v11 Meteor | Full BLEU | Full TER\u2193 | Full Meteor\nOpenNMT-tf | 15.1 | 0.6902 | 0.3040 | 24.3 | 0.5567 | 0.4225\nTransformer-tf | 17.1 | 0.6647 | 0.3578 | 25.1 | 0.5537 | 0.4344\nSeq2seq | 16.0 | 0.6695 | 0.3379 | 23.7 | 0.5590 | 0.4258\nDual2seq-LinAMR | 17.3 | 0.6530 | 0.3612 | 24.0 | 0.5643 | 0.4246\nDuel2seq-SRL | 17.2 | 0.6591 | 0.3644 | 23.8 | 0.5626 | 0.4223\nDual2seq-Dep | 17.8 | 0.6516 | 0.3673 | 25.0 | 0.5538 | 0.4328\nDual2seq | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840 | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e7e9e3d8-3d31-42cb-bc2c-98491549fcb7",
    "input": "## Claim\nHere is a claim: This is particularly noteworthy because our user simulator takes a very strict agenda (Section 4.1) compared to that of humans, which is more dynamic and changing as the conversation continues. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.\nGP-MBCM | ACER | PPO | ALDM | GDPL\n1.666 | 0.775 | 0.639 | 1.069 | [BOLD] 0.238\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8c8887ea-cdea-48c2-bc39-07dde08b7c8d",
    "input": "## Claim\nHere is a claim: ( 2018 )) and the rank correlation between NeuralTD and human summaries is higher than with supervised models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "78f1241f-560b-413e-aeeb-e7d909a8b4fd",
    "input": "## Claim\nHere is a claim: WOMs are slightly lower for TGen trained on the cleaned data, except for NIST, which gives more importance to matching less frequent n-grams. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Original | TGen\u2212 | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94\nOriginal | [BOLD] Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27\nOriginal | [BOLD] Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80\nOriginal | [BOLD] Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen\u2212 | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37\nCleaned missing | [BOLD] Original | TGen\u2212 | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61\nCleaned missing | [BOLD] Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53\nCleaned missing | [BOLD] Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen\u2212 | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "cca89ac1-e731-42b7-b5c8-0b1ba49eae32",
    "input": "## Claim\nHere is a claim: The summaries generated by our system receive decent ROUGE metrics, but are lower than most of the recent systems, because our learned reward is optimised towards high correlation with human judgement instead of ROUGE metrics. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7cb9f68a-d66f-4adf-b12d-35a2fd47dd67",
    "input": "## Claim\nHere is a claim: among opinions: We see that OD significantly outperforms the baseline methods and the OD-parse variant [CONTINUE] OD achieves high ARI and Sil scores, [CONTINUE] From the above table, we observe that the text-similarity based baselines, such as TF-IDF, WMD and Doc2vec do not achieve high ARI and Silhouette coefficient scores on the \"Video Games\" and \"Pornography\" datasets. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 3: ARI and Silhouette coefficient scores.\nMethods | Seanad Abolition ARI | Seanad Abolition  [ITALIC] Sil | Video Games ARI | Video Games  [ITALIC] Sil | Pornography ARI | Pornography  [ITALIC] Sil\nTF-IDF | 0.23 | 0.02 | -0.01 | 0.01 | -0.02 | 0.01\nWMD | 0.09 | 0.01 | 0.01 | 0.01 | -0.02 | 0.01\nSent2vec | -0.01 | -0.01 | 0.11 | 0.06 | 0.01 | 0.02\nDoc2vec | -0.01 | -0.03 | -0.01 | 0.01 | 0.02 | -0.01\nBERT | 0.03 | -0.04 | 0.08 | 0.05 | -0.01 | 0.03\nOD-parse | 0.01 | -0.04 | -0.01 | 0.02 | 0.07 | 0.05\nOD | [BOLD] 0.54 | [BOLD] 0.31 | [BOLD] 0.56 | [BOLD] 0.42 | [BOLD] 0.41 | [BOLD] 0.41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "dad6a4ed-cf24-42d0-9293-ac3ed0d9efcf",
    "input": "## Claim\nHere is a claim: The AAS method with weights wAC=1 and wAD=105 shows the lowest WER and DCE. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition\nTable caption: TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set\nMethod | WER (%) | DCE\nNo enhancement | 38.4 | 0.958\nWiener filter | 41.0 | 0.775\nMinimizing DCE | 31.1 | [BOLD] 0.392\nFSEGAN | 29.1 | 0.421\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=0) | 27.7 | 0.476\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 26.1 | 0.462\nClean speech | 9.3 | 0.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "f0edce30-c35c-41ad-b5f3-719e7e112bb0",
    "input": "## Claim\nHere is a claim: [CONTINUE] On the other hand, we found the quality of 3-step NLDs is relatively lower than the others. Does the following context support or refute the claim?\n\n## Table\nPaper title: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension\nTable caption: Table 2: Ratings of annotated NLDs by human judges.\n# steps | Reachability | Derivability Step 1 | Derivability Step 2 | Derivability Step 3\n1 | 3.0 | 3.8 | - | -\n2 | 2.8 | 3.8 | 3.7 | -\n3 | 2.3 | 3.9 | 3.8 | 3.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7099710f-edb1-4afd-ab16-47381f257b0d",
    "input": "## Claim\nHere is a claim: When the experiment was repeated so that the finetuning phase included the text-only data, the performance did not return to approximately the same level as without tuning (+multi-modal finetune row in Table 6). Does the following context support or refute the claim?\n\n## Table\nPaper title: The MeMAD Submission to the WMT18 Multimodal Translation Task\nTable caption: Table 6: Ablation experiments (BLEU% scores). The row subs3MLM detectron shows our best single model. Individual components or data choices are varied one by one. + stands for adding a component, and \u2212 for removing a component or data set. Multiple modifications are indicated by increasing the indentation.\nen-fr | flickr16 | flickr17 | mscoco17\nsubs3M [ITALIC]  [ITALIC] LM detectron | 68.30 | 62.45 | 52.86\n+ensemble-of-3 | 68.72 | 62.70 | 53.06\n\u2212visual features | [BOLD] 68.74 | [BOLD] 62.71 | 53.14\n\u2212MS-COCO | 67.13 | 61.17 | [BOLD] 53.34\n\u2212multi-lingual | 68.21 | 61.99 | 52.40\nsubs6M [ITALIC]  [ITALIC] LM detectron | 68.29 | 61.73 | 53.05\nsubs3M [ITALIC]  [ITALIC] LM gn2048 | 67.74 | 61.78 | 52.76\nsubs3M [ITALIC]  [ITALIC] LM text-only | 67.72 | 61.75 | 53.02\nen-de | flickr16 | flickr17 | mscoco17\nsubs3M [ITALIC]  [ITALIC] LM detectron | 45.09 | 40.81 | 36.94\n+ensemble-of-3 | 45.52 | [BOLD] 41.84 | [BOLD] 37.49\n\u2212visual features | [BOLD] 45.59 | 41.75 | 37.43\n\u2212MS-COCO | 45.11 | 40.52 | 36.47\n\u2212multi-lingual | 44.95 | 40.09 | 35.28\nsubs6M [ITALIC]  [ITALIC] LM detectron | 45.50 | 41.01 | 36.81\nsubs3M [ITALIC]  [ITALIC] LM gn2048 | 45.38 | 40.07 | 36.82\nsubs3M [ITALIC]  [ITALIC] LM text-only | 44.87 | 41.27 | 36.59\n+multi-modal finetune | 44.56 | 41.61 | 36.93\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1b8acf49-27ee-44b2-a81b-2ea143aa00f0",
    "input": "## Claim\nHere is a claim: (2017), we find large disparities, with around 5% of tweets in the black-aligned corpus classified as hate speech compared to 2% of those in the white-aligned set. Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 2: Experiment 1\nDataset | Class | \u02c6 [ITALIC] piblack | \u02c6 [ITALIC] piwhite | [ITALIC] t | [ITALIC] p | \u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite\n[ITALIC] Waseem and Hovy | Racism | 0.001 | 0.003 | -20.818 | *** | 0.505\n[EMPTY] | Sexism | 0.083 | 0.048 | 101.636 | *** | 1.724\n[ITALIC] Waseem | Racism | 0.001 | 0.001 | 0.035 | [EMPTY] | 1.001\n[EMPTY] | Sexism | 0.023 | 0.012 | 64.418 | *** | 1.993\n[EMPTY] | Racism and sexism | 0.002 | 0.001 | 4.047 | *** | 1.120\n[ITALIC] Davidson et al. | Hate | 0.049 | 0.019 | 120.986 | *** | 2.573\n[EMPTY] | Offensive | 0.173 | 0.065 | 243.285 | *** | 2.653\n[ITALIC] Golbeck et al. | Harassment | 0.032 | 0.023 | 39.483 | *** | 1.396\n[ITALIC] Founta et al. | Hate | 0.111 | 0.061 | 122.707 | *** | 1.812\n[EMPTY] | Abusive | 0.178 | 0.080 | 211.319 | *** | 2.239\n[EMPTY] | Spam | 0.028 | 0.015 | 63.131 | *** | 1.854\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "74e9434e-0d7c-4c20-8f65-b02bf1e43667",
    "input": "## Claim\nHere is a claim: The number of examples in our Multi-News dataset is two orders of magnitude larger than previous MDS news data. Does the following context support or refute the claim?\n\n## Table\nPaper title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\nTable caption: Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.\n[BOLD] Dataset | [BOLD] # pairs | [BOLD] # words (doc) | [BOLD] # sents (docs) | [BOLD] # words (summary) | [BOLD] # sents (summary) | [BOLD] vocab size\nMulti-News | 44,972/5,622/5,622 | 2,103.49 | 82.73 | 263.66 | 9.97 | 666,515\nDUC03+04 | 320 | 4,636.24 | 173.15 | 109.58 | 2.88 | 19,734\nTAC 2011 | 176 | 4,695.70 | 188.43 | 99.70 | 1.00 | 24,672\nCNNDM | 287,227/13,368/11,490 | 810.57 | 39.78 | 56.20 | 3.68 | 717,951\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "52c61ca7-ec3c-4313-bd66-c792ef9bf414",
    "input": "## Claim\nHere is a claim: Despite LRN and oLRN having faster training times than SRU (+15%/+6%), SRU still achieves a higher BLEU score. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.\nModel | #Params | BLEU | Train | Decode\nGNMT | - | 24.61 | - | -\nGRU | 206M | 26.28 | 2.67 | 45.35\nATR | 122M | 25.70 | 1.33 | [BOLD] 34.40\nSRU | 170M | 25.91 | 1.34 | 42.84\nLRN | 143M | 26.26 | [BOLD] 0.99 | 36.50\noLRN | 164M | [BOLD] 26.73 | 1.15 | 40.19\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "88f1cf27-946a-4442-98dc-02d34534f76e",
    "input": "## Claim\nHere is a claim: Additionally, when using bounding box features, sparsemax outperforms softmax, showing that selecting only the bounding boxes of the relevant objects leads to a better answering capability. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.\n[EMPTY] | Att. to image | Att. to bounding boxes | Test-Dev Yes/No | Test-Dev Number | Test-Dev Other | Test-Dev Overall | Test-Standard Yes/No | Test-Standard Number | Test-Standard Other | Test-Standard Overall\nsoftmax | \u2713 | [EMPTY] | 83.08 | 42.65 | 55.74 | 65.52 | 83.55 | 42.68 | 56.01 | 65.97\nsparsemax | \u2713 | [EMPTY] | 83.08 | 43.19 | 55.79 | 65.60 | 83.33 | 42.99 | 56.06 | 65.94\nsoft-TVmax | \u2713 | [EMPTY] | 83.13 | 43.53 | 56.01 | 65.76 | 83.63 | 43.24 | 56.10 | 66.11\nsparse-TVmax | \u2713 | [EMPTY] | 83.10 | 43.30 | 56.14 | 65.79 | 83.66 | 43.18 | 56.21 | 66.17\nsoftmax | [EMPTY] | \u2713 | 85.14 | 49.59 | 58.72 | 68.57 | 85.56 | 49.54 | 59.11 | 69.04\nsparsemax | [EMPTY] | \u2713 | [BOLD] 85.40 | [BOLD] 50.87 | 58.67 | 68.79 | [BOLD] 85.80 | 50.18 | 59.08 | 69.19\nsoftmax | \u2713 | \u2713 | 85.33 | 50.49 | 58.88 | 68.82 | 85.58 | 50.42 | 59.18 | 69.17\nsparse-TVmax | \u2713 | \u2713 | 85.35 | 50.52 | [BOLD] 59.15 | [BOLD] 68.96 | 85.72 | [BOLD] 50.66 | [BOLD] 59.22 | [BOLD] 69.28\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b90516c0-5d54-4636-b318-cdc83d2fce12",
    "input": "## Claim\nHere is a claim: The UnsupEmb baseline performs comparably to the Word2Tag upper bound on both POS and SEM tagging. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks\nTable caption: Table 2: POS and SEM tagging accuracy with baselines and an upper bound. MFT: most frequent tag; UnsupEmb: classifier using unsupervised word embeddings; Word2Tag: upper bound encoder-decoder.\n[EMPTY] | MFT | UnsupEmb | Word2Tag\nPOS | 91.95 | 87.06 | 95.55\nSEM | 82.00 | 81.11 | 91.41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "406069b0-825f-4f0b-b982-048d1e765fcf",
    "input": "## Claim\nHere is a claim: POS-disambiguation does not fragment the vocabulary and consistently increases the coverage with the effect being more pronounced for lemmatized targets. Does the following context support or refute the claim?\n\n## Table\nPaper title: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources\nTable caption: Table 4: Lexicon member coverage (%)\ntarget | VN | WN-V | WN-N\ntype | 81 | 66 | 47\nx+POS | 54 | 39 | 43\nlemma | 88 | 76 | 53\nx+POS | 79 | 63 | 50\nshared | 54 | 39 | 41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "c9a67956-a20f-488b-bd85-062a6fc04a01",
    "input": "## Claim\nHere is a claim: In most cases the racial disparities persist, and are generally larger in magnitude than the disparities for other classes. Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 2: Experiment 1\nDataset | Class | \u02c6 [ITALIC] piblack | \u02c6 [ITALIC] piwhite | [ITALIC] t | [ITALIC] p | \u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite\n[ITALIC] Waseem and Hovy | Racism | 0.001 | 0.003 | -20.818 | *** | 0.505\n[EMPTY] | Sexism | 0.083 | 0.048 | 101.636 | *** | 1.724\n[ITALIC] Waseem | Racism | 0.001 | 0.001 | 0.035 | [EMPTY] | 1.001\n[EMPTY] | Sexism | 0.023 | 0.012 | 64.418 | *** | 1.993\n[EMPTY] | Racism and sexism | 0.002 | 0.001 | 4.047 | *** | 1.120\n[ITALIC] Davidson et al. | Hate | 0.049 | 0.019 | 120.986 | *** | 2.573\n[EMPTY] | Offensive | 0.173 | 0.065 | 243.285 | *** | 2.653\n[ITALIC] Golbeck et al. | Harassment | 0.032 | 0.023 | 39.483 | *** | 1.396\n[ITALIC] Founta et al. | Hate | 0.111 | 0.061 | 122.707 | *** | 1.812\n[EMPTY] | Abusive | 0.178 | 0.080 | 211.319 | *** | 2.239\n[EMPTY] | Spam | 0.028 | 0.015 | 63.131 | *** | 1.854\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "c67da597-da7b-4c75-99a6-5184dfb0c485",
    "input": "## Claim\nHere is a claim: CorefProp also improves relation extraction on SciERC. Does the following context support or refute the claim?\n\n## Table\nPaper title: Entity, Relation, and Event Extraction with Contextualized Span Representations\nTable caption: Table 3: F1 scores on Relation.\n[EMPTY] | ACE05 | SciERC | WLPC\nBERT + LSTM | 60.6 | 40.3 | 65.1\n+RelProp | 61.9 | 41.1 | 65.3\n+CorefProp | 59.7 | 42.6 | -\nBERT FineTune | [BOLD] 62.1 | 44.3 | 65.4\n+RelProp | 62.0 | 43.0 | [BOLD] 65.5\n+CorefProp | 60.0 | [BOLD] 45.3 | -\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "fede62ad-8591-411a-974e-a263d0e6dd91",
    "input": "## Claim\nHere is a claim: LRN obtains an accuracy of 90.49 with BERT, the highest among all models. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nModel | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time\nRockt\u00e4schel et\u00a0al. ( 2016 ) | Rockt\u00e4schel et\u00a0al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | -\nThis | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 | [BOLD] 90.49 | 0.696\nThis | GRU | 6.41M | [BOLD] 85.71 | 0.245 | [BOLD] 86.05 | 0.419 | [BOLD] 90.29 | 0.529 | 90.10 | 0.695\nThis | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580\nWork | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555\n[EMPTY] | LRN | 4.25M | 84.88 | [BOLD] 0.209 | 85.06 | [BOLD] 0.223 | 89.98 | [BOLD] 0.488 | 89.93 | [BOLD] 0.506\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "dafe0889-31b8-48ce-a240-cad9ffdebc2e",
    "input": "## Claim\nHere is a claim: G2S-GGNN has 33.5% and 5.2% better entailment performances than S2S, when REF entails GEN and GEN entails REF, respectively. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.\n<bold>Model</bold> | REF \u21d2 GEN <bold>ENT</bold> | REF \u21d2 GEN <bold>CON</bold> | REF \u21d2 GEN <bold>NEU</bold>\nS2S | 38.45 | 11.17 | 50.38\nG2S-GIN | 49.78 | 9.80 | 40.42\nG2S-GAT | 49.48 | 8.09 | 42.43\nG2S-GGNN | 51.32 | 8.82 | 39.86\n[EMPTY] | GEN \u21d2 REF | GEN \u21d2 REF | GEN \u21d2 REF\n<bold>Model</bold> | <bold>ENT</bold> | <bold>CON</bold> | <bold>NEU</bold>\nS2S | 73.79 | 12.75 | 13.46\nG2S-GIN | 76.27 | 10.65 | 13.08\nG2S-GAT | 77.54 | 8.54 | 13.92\nG2S-GGNN | 77.64 | 9.64 | 12.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "fd55824c-1680-4200-be35-96a65ac28a9b",
    "input": "## Claim\nHere is a claim: The intuition here is that each model is optimizing different signals (lexical matching and type accuracy), which may or may not be independent. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\n[BOLD] Model | R | MUC P | [ITALIC] F1 | R | B3 P | [ITALIC] F1 | R | CEAF- [ITALIC] e P | [ITALIC] F1 | CoNLL  [ITALIC] F1\n[BOLD] Baselines | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nCluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5\nCV Cybulska and Vossen ( 2015a ) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73\nKCP Kenyon-Dean et\u00a0al. ( 2018 ) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69\nCluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6\n[BOLD] Model Variants | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nDisjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5\nJoint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | [BOLD] 79.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "96bf0f6b-f429-4648-bab7-cf3759539016",
    "input": "## Claim\nHere is a claim: Table 5 breaks down the results of the different models according to two conditions: when the gold sentence is code-switched, and when the gold sentence is monolingual. Does the following context support or refute the claim?\n\n## Table\nPaper title: Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training\nTable caption: Table 5: Accuracy on the dev set and on the test set, according to the type of the gold sentence in the set: code-switched (CS) vs. monolingual (mono).\n[EMPTY] | dev CS | dev mono | test CS | test mono\nCS-only-LM | 45.20 | 65.87 | 43.20 | 62.80\nFine-Tuned-LM | 49.60 | 72.67 | 47.60 | 71.33\nCS-only-disc | [BOLD] 75.60 | 70.40 | 70.80 | 70.53\nFine-Tuned-disc | 70.80 | [BOLD] 74.40 | [BOLD] 75.33 | [BOLD] 75.87\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "03375542-1aaf-400c-9743-0e332dd4183b",
    "input": "## Claim\nHere is a claim: According to the table, the drop of precision demonstrates that the word-level attention is quite useful. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\n-Word-ATT | 0.648 | 0.515 | 0.395 | 0.389\n-Capsule | 0.635 | 0.507 | 0.413 | 0.386\nOur Model | 0.650 | 0.519 | 0.422 | 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "966e1252-7bc6-49de-8cde-ddf9dd9771a0",
    "input": "## Claim\nHere is a claim: While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C yields 5 points higher scores on WordContent and BigramShift. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 4: Scores for different training objectives on the linguistic probing tasks.\nMethod | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\nCMOW-C | [BOLD] 36.2 | 66.0 | 81.1 | 78.7 | 61.7 | [BOLD] 83.9 | 79.1 | 73.6 | 50.4 | 66.8\nCMOW-R | 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | [BOLD] 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | [BOLD] 74.2 | [BOLD] 50.7 | [BOLD] 72.9\nCBOW-C | [BOLD] 34.3 | [BOLD] 50.5 | [BOLD] 79.8 | [BOLD] 79.9 | 53.0 | [BOLD] 75.9 | [BOLD] 79.8 | [BOLD] 72.9 | 48.6 | 89.0\nCBOW-R | 33.0 | 49.6 | 79.3 | 78.4 | [BOLD] 53.6 | 74.5 | 78.6 | 72.0 | [BOLD] 49.6 | [BOLD] 89.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "07e54d6a-f1de-49d1-8249-96055ac0191a",
    "input": "## Claim\nHere is a claim: We also observe similar trends as before: POS tagging does not benefit from features from the upper layers, while SEM tagging improves with layer 4 representations. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks\nTable caption: Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.\nUni | POS | 0 87.9 | 1 92.0 | 2 91.7 | 3 91.8 | 4 91.9\nUni | SEM | 81.8 | 87.8 | 87.4 | 87.6 | 88.2\nBi | POS | 87.9 | 93.3 | 92.9 | 93.2 | 92.8\nBi | SEM | 81.9 | 91.3 | 90.8 | 91.9 | 91.9\nRes | POS | 87.9 | 92.5 | 91.9 | 92.0 | 92.4\nRes | SEM | 81.9 | 88.2 | 87.5 | 87.6 | 88.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "63fe7961-86a5-4ceb-9556-1f4592c15b2c",
    "input": "## Claim\nHere is a claim: After removing the graph attention module, our model gives 24.9 BLEU points. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "12a69a90-7404-45aa-b347-5867d2287a46",
    "input": "## Claim\nHere is a claim: the distribution on dialog success criteria with ACER has the least bias among all. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "9df0b311-fa64-4aaa-b766-568444a3f1a9",
    "input": "## Claim\nHere is a claim: We observe that for the NYT10 dataset, m = {1, 2, 3} gives good performance with m = 1 achieving the highest F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 3: Performance comparison of our model with different values of m on the two datasets.\n[ITALIC] m | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1\n1 | 0.541 | 0.595 | [BOLD] 0.566 | 0.495 | 0.621 | 0.551\n2 | 0.521 | 0.597 | 0.556 | 0.482 | 0.656 | 0.555\n3 | 0.490 | 0.617 | 0.547 | 0.509 | 0.633 | 0.564\n4 | 0.449 | 0.623 | 0.522 | 0.507 | 0.652 | [BOLD] 0.571\n5 | 0.467 | 0.609 | 0.529 | 0.488 | 0.677 | 0.567\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3ce2ce6f-939a-4a30-b4ec-06b484e1ae64",
    "input": "## Claim\nHere is a claim: (2017).8 Overall both BERT (76.5%) and [CONTINUE] RoBERTa (87.7%) considerably outperform the best previous model (71.4%). Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "dfff7b52-8d27-4ac2-8e6a-fb62361453d3",
    "input": "## Claim\nHere is a claim: In particular, our single DCGCN model consistently outperforms Seq2Seq models by a significant margin when trained without external resources. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.\n[BOLD] Model | [BOLD] T | #P | B | C\nSeq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1\nGGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4\nSeq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5\nGGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5\nDCGCN (ours) | S | [BOLD] 19.1M | 27.9 | 57.3\nDCGCN (ours) | E | 92.5M | [BOLD] 30.4 | [BOLD] 59.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "8a6a5782-c6a0-428a-aadb-4813c0a9d2ae",
    "input": "## Claim\nHere is a claim: The larger performance gap between Easy and Hard subsets indicates that training on BCOPA encourages BERT and RoBERTa to rely more on superficial cues. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large-FT | B-COPA | 74.5 (\u00b1 0.7) | 74.7 (\u00b1 0.4) | [BOLD] 74.4 (\u00b1 0.9)\nBERT-large-FT | B-COPA (50%) | 74.3 (\u00b1 2.2) | 76.8 (\u00b1 1.9) | 72.8 (\u00b1 3.1)\nBERT-large-FT | COPA | [BOLD] 76.5 (\u00b1 2.7) | [BOLD] 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5)\nRoBERTa-large-FT | B-COPA | [BOLD] 89.0 (\u00b1 0.3) | 88.9 (\u00b1 2.1) | [BOLD] 89.0 (\u00b1 0.8)\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (\u00b1 2.2) | 87.4 (\u00b1 1.1) | 85.4 (\u00b1 2.9)\nRoBERTa-large-FT | COPA | 87.7 (\u00b1 0.9) | [BOLD] 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4ca84e7f-8b85-4b40-8506-e06ed09099af",
    "input": "## Claim\nHere is a claim: For German descriptions, The results are 11.05% worse on average compared to (Gella et al., 2017) in symmetric mode. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task\nTable caption: Table 2: Image-caption ranking results for German (Multi30k)\n[EMPTY] | Image to Text R@1 | Image to Text R@5 | Image to Text R@10 | Image to Text Mr | Text to Image R@1 | Text to Image R@5 | Text to Image R@10 | Text to Image Mr | Alignment\n[BOLD] symmetric | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nParallel\u00a0gella:17 | 28.2 | 57.7 | 71.3 | 4 | 20.9 | 46.9 | 59.3 | 6 | -\nMono | 34.2 | 67.5 | 79.6 | 3 | 26.5 | 54.7 | 66.2 | 4 | -\nFME | 36.8 | 69.4 | 80.8 | 2 | 26.6 | 56.2 | 68.5 | 4 | 76.81%\nAME | [BOLD] 39.6 | [BOLD] 72.7 | [BOLD] 82.7 | [BOLD] 2 | [BOLD] 28.9 | [BOLD] 58.0 | [BOLD] 68.7 | [BOLD] 4 | 66.91%\n[BOLD] asymmetric | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nPivot\u00a0gella:17 | 28.2 | 61.9 | 73.4 | 3 | 22.5 | 49.3 | 61.7 | 6 | -\nParallel\u00a0gella:17 | 30.2 | 60.4 | 72.8 | 3 | 21.8 | 50.5 | 62.3 | 5 | -\nMono | [BOLD] 42.0 | 72.5 | 83.0 | 2 | 29.6 | 58.4 | 69.6 | 4 | -\nFME | 40.5 | 73.3 | 83.4 | 2 | 29.6 | 59.2 | [BOLD] 72.1 | 3 | 76.81%\nAME | 40.5 | [BOLD] 74.3 | [BOLD] 83.4 | [BOLD] 2 | [BOLD] 31.0 | [BOLD] 60.5 | 70.6 | [BOLD] 3 | 73.10%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "8a3ce3da-5db7-48f2-929f-2d27febabd8a",
    "input": "## Claim\nHere is a claim: In general, the performance increases when we gradually enlarge n and m. For example, when n=1 and m=1, the BLEU score is 17.6; when n=6 and m=6, the BLEU score becomes 22.0. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.\n[ITALIC] Block | [ITALIC] n | [ITALIC] m | B | C\n1 | 1 | 1 | 17.6 | 48.3\n1 | 1 | 2 | 19.2 | 50.3\n1 | 2 | 1 | 18.4 | 49.1\n1 | 1 | 3 | 19.6 | 49.4\n1 | 3 | 1 | 20.0 | 50.5\n1 | 3 | 3 | 21.4 | 51.0\n1 | 3 | 6 | 21.8 | 51.7\n1 | 6 | 3 | 21.7 | 51.5\n1 | 6 | 6 | 22.0 | 52.1\n2 | 3 | 6 | [BOLD] 23.5 | 53.3\n2 | 6 | 3 | 23.3 | [BOLD] 53.4\n2 | 6 | 6 | 22.0 | 52.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "37eee057-4b1f-43f6-9889-4aa72045c882",
    "input": "## Claim\nHere is a claim: However, the sdp information does not have a clear positive impact on all the relation types (Table 1). Does the following context support or refute the claim?\n\n## Table\nPaper title: Syntactic Dependency Representations in Neural Relation Classification\nTable caption: Table 1: Effect of using the shortest dependency path on each relation type.\n[BOLD] Relation | [BOLD] best F1 (in 5-fold) without sdp | [BOLD] best F1 (in 5-fold) with sdp | [BOLD] Diff.\nUSAGE | 60.34 | 80.24 | + 19.90\nMODEL-FEATURE | 48.89 | 70.00 | + 21.11\nPART_WHOLE | 29.51 | 70.27 | +40.76\nTOPIC | 45.80 | 91.26 | +45.46\nRESULT | 54.35 | 81.58 | +27.23\nCOMPARE | 20.00 | 61.82 | + 41.82\nmacro-averaged | 50.10 | 76.10 | +26.00\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6d4f98b8-fffb-4b2b-b1b6-43eccf4cda41",
    "input": "## Claim\nHere is a claim: Reward 3, i.e., preference between generated summaries and reference, has slightly higher correlations with system performance than Reward 1, i.e., difference in summary properties from statistical values computed on references (regression loss in Eq. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "cbc48b80-a560-4ead-aeb2-98b1dcb907eb",
    "input": "## Claim\nHere is a claim: DCGCN model is able to achieve a competitive BLEU points (33.2) by using 0.3M external data, while GraphLSTM achieves a score of 33.6 by using 2M data and Seq2SeqK achieves a score of 33.8 by using 20M data. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M\n[BOLD] Model | [BOLD] External | B\nSeq2SeqK (Konstas et al.,  2017 ) | - | 22.0\nGraphLSTM (Song et al.,  2018 ) | - | 23.3\nGCNSEQ (Damonte and Cohen,  2019 ) | - | 24.4\nDCGCN(single) | - | 25.9\nDCGCN(ensemble) | - | [BOLD] 28.2\nTSP (Song et al.,  2016 ) | ALL | 22.4\nPBMT (Pourdamghani et al.,  2016 ) | ALL | 26.9\nTree2Str (Flanigan et al.,  2016 ) | ALL | 23.0\nSNRG (Song et al.,  2017 ) | ALL | 25.6\nSeq2SeqK (Konstas et al.,  2017 ) | 0.2M | 27.4\nGraphLSTM (Song et al.,  2018 ) | 0.2M | 28.2\nDCGCN(single) | 0.1M | 29.0\nDCGCN(single) | 0.2M | [BOLD] 31.6\nSeq2SeqK (Konstas et al.,  2017 ) | 2M | 32.3\nGraphLSTM (Song et al.,  2018 ) | 2M | 33.6\nSeq2SeqK (Konstas et al.,  2017 ) | 20M | 33.8\nDCGCN(single) | 0.3M | 33.2\nDCGCN(ensemble) | 0.3M | [BOLD] 35.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0cfe420b-02cd-4738-a54e-42200ab18629",
    "input": "## Claim\nHere is a claim: their recall are 0.595, 0.517, and 0.441 on three thresholds 0.1, 0.2 and 0.3 respectively, while our model achieves 0.650, 0.519, 0.422. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\n-Word-ATT | 0.648 | 0.515 | 0.395 | 0.389\n-Capsule | 0.635 | 0.507 | 0.413 | 0.386\nOur Model | 0.650 | 0.519 | 0.422 | 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f5d1e12a-54c7-4133-aa0e-bd0d0447cdcf",
    "input": "## Claim\nHere is a claim: this impressive improvement comes from the large dataset and considerable time spent on hyperparameter tuning, but only better-than-human results compared to RoBERTa and BERT finetuning. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See \u00a72 for model details. * indicates our replication experiments.\nModel | Accuracy\nBigramPMI\u00a0Goodwin et al. ( 2012 ) | 63.4\nPMI\u00a0Gordon et al. ( 2011 ) | 65.4\nPMI+Connectives\u00a0Luo et al. ( 2016 ) | 70.2\nPMI+Con.+Phrase\u00a0Sasaki et al. ( 2017 ) | 71.4\nBERT-large\u00a0Wang et al. ( 2019 ) | 70.5\nBERT-large\u00a0Sap et al. ( 2019 ) | 75.0\nBERT-large\u00a0Li et al. ( 2019 ) | 75.4\nRoBERTa-large (finetuned) | 90.6\nBERT-large (finetuned)* | 76.5 \u00b1 2.7\nRoBERTa-large (finetuned)* | 87.7 \u00b1 0.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "bd1eba72-ce56-4f45-a304-cb354ff75544",
    "input": "## Claim\nHere is a claim: acoustic supervision (27.7%) and multi-task learning (26.1%) show higher WER than minimizing DCE (31.1%) and FSEGAN (29.1%). Does the following context support or refute the claim?\n\n## Table\nPaper title: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition\nTable caption: TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set\nMethod | WER (%) | DCE\nNo enhancement | 38.4 | 0.958\nWiener filter | 41.0 | 0.775\nMinimizing DCE | 31.1 | [BOLD] 0.392\nFSEGAN | 29.1 | 0.421\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=0) | 27.7 | 0.476\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 26.1 | 0.462\nClean speech | 9.3 | 0.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "724dc847-9047-4128-87b2-58f815992054",
    "input": "## Claim\nHere is a claim: Furthermore, it also yields better policy matches, except for PPO, suggesting that GDPL is more compatible with the real users. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "3856e931-c7bc-424a-b97f-9e63e61dc3a6",
    "input": "## Claim\nHere is a claim: This is particularly true for the BIDAF model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "42608cfc-8066-4ba5-b5c5-56394189d947",
    "input": "## Claim\nHere is a claim: therefore, the role of attention in link prediction can be explained. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 2: Precisions on the Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nRank+ExATT | 0.584 | 0.535 | 0.487 | 0.392\nPCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204\nPCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396\nOur Model | 0.650 | 0.519 | 0.422 | [BOLD] 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "1f607985-944c-457a-878b-94f8d36c7b28",
    "input": "## Claim\nHere is a claim: In addition, our single DCGCN model obtains better results than previous ensemble models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.\n[BOLD] Model | [BOLD] T | #P | B | C\nSeq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1\nGGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4\nSeq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5\nGGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5\nDCGCN (ours) | S | [BOLD] 19.1M | 27.9 | 57.3\nDCGCN (ours) | E | 92.5M | [BOLD] 30.4 | [BOLD] 59.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2df13a17-6c33-4cd4-8cf3-7c85fc97cb01",
    "input": "## Claim\nHere is a claim: According to Pearson correlation, gr def model had the highest correlation with human ratings of similarity. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluation of Greek Word Embeddings\nTable caption: Table 4: Word similarity.\nModel | Pearson | p-value | Pairs (unknown)\ngr_def | [BOLD] 0.6042 | 3.1E-35 | 2.3%\ngr_neg10 | 0.5973 | 2.9E-34 | 2.3%\ncc.el.300 | 0.5311 | 1.7E-25 | 4.9%\nwiki.el | 0.5812 | 2.2E-31 | 4.5%\ngr_cbow_def | 0.5232 | 2.7E-25 | 2.3%\ngr_d300_nosub | 0.5889 | 3.8E-33 | 2.3%\ngr_w2v_sg_n5 | 0.5879 | 4.4E-33 | 2.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f273252e-5941-436d-aaf0-23e946eaca18",
    "input": "## Claim\nHere is a claim: SciBERT significantly boosts performance for scientific datasets including SciERC and GENIA. Does the following context support or refute the claim?\n\n## Table\nPaper title: Entity, Relation, and Event Extraction with Contextualized Span Representations\nTable caption: Table 7: In-domain pre-training: SciBERT vs. BERT\n[EMPTY] | SciERC Entity | SciERC Relation | GENIA Entity\nBest BERT | 69.8 | 41.9 | 78.4\nBest SciBERT | [BOLD] 72.0 | [BOLD] 45.3 | [BOLD] 79.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "02999797-f0ae-4c27-b7bd-8c4a44e60537",
    "input": "## Claim\nHere is a claim: [CONTINUE] We validate Sim and PP by computing sentence-level Spearman's \u03c1 between the metric and human judgments [CONTINUE] From Table 5, all validations show strong correlations on the Yelp dataset and reasonable correlations on Literature. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.\nMetric | Method of validation | Yelp | Lit.\nAcc | % of machine and human judgments that match | 94 | 84\nSim | Spearman\u2019s  [ITALIC] \u03c1 b/w Sim and human ratings of semantic preservation | 0.79 | 0.75\nPP | Spearman\u2019s  [ITALIC] \u03c1 b/w negative PP and human ratings of fluency | 0.81 | 0.67\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "07c4b243-00fb-4439-94ae-89bb5c1641f5",
    "input": "## Claim\nHere is a claim: [CONTINUE] Negations are uncovered through unigrams (not, no, won't) [CONTINUE] Several unigrams (error, issue, working, fix) [CONTINUE] However, words regularly describing negative sentiment or emotions are not one of the most distinctive features for complaints. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\n[BOLD] Complaints  [BOLD] Feature | [BOLD] Complaints  [ITALIC] r | [BOLD] Not Complaints  [BOLD] Feature | [BOLD] Not Complaints  [ITALIC] r\n[BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams\nnot | .154 | [URL] | .150\nmy | .131 | ! | .082\nworking | .124 | he | .069\nstill | .123 | thank | .067\non | .119 | , | .064\ncan\u2019t | .113 | love | .064\nservice | .112 | lol | .061\ncustomer | .109 | you | .060\nwhy | .108 | great | .058\nwebsite | .107 | win | .058\nno | .104 | \u2019 | .058\n? | .098 | she | .054\nfix | .093 | : | .053\nwon\u2019t | .092 | that | .053\nbeen | .090 | more | .052\nissue | .089 | it | .052\ndays | .088 | would | .051\nerror | .087 | him | .047\nis | .084 | life | .046\ncharged | .083 | good | .046\n[BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)\nVBN | .141 | UH | .104\n$ | .118 | NNP | .098\nVBZ | .114 | PRP | .076\nNN_VBZ | .114 | HT | .076\nPRP$ | .107 | PRP_. | .076\nPRP$_NN | .105 | PRP_RB | .067\nVBG | .093 | NNP_NNP | .062\nCD | .092 | VBP_PRP | .054\nWRB_VBZ | .084 | JJ | .053\nVBZ_VBN | .084 | DT_JJ | .051\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "16827e79-dd19-40b9-b474-1a6e305ece38",
    "input": "## Claim\nHere is a claim: We observe that BERT trained on Balanced COPA is more sensitive to a few highly productive superficial cues than BERT trained on original COPA. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 7: Sensitivity of BERT-large to superficial cues identified in \u00a72 (unit: 10\u22122). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.\nCue | [ITALIC] SCOPA | [ITALIC] SB_COPA | Diff. | Prod.\nwoman | 7.98 | 4.84 | -3.14 | 0.25\nmother | 5.16 | 3.95 | -1.21 | 0.75\nwent | 6.00 | 5.15 | -0.85 | 0.73\ndown | 5.52 | 4.93 | -0.58 | 0.71\ninto | 4.07 | 3.51 | -0.56 | 0.40\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "aa773884-bbac-43b0-a011-1b6131ce4455",
    "input": "## Claim\nHere is a claim: our model has much better quality over the extractive summarization system in three aspects. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0785d05a-87ed-4bb1-ac80-372c08eb76c8",
    "input": "## Claim\nHere is a claim: For example, DCGCN4 contains 36 layers. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.\n[BOLD] GCN +RC (2) | B 16.8 | C 48.1 | [BOLD] GCN +RC+LA (2) | B 18.3 | C 47.9\n+RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1\n+RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8\n+RC (9) | [BOLD] 21.1 | 50.5 | +RC+LA (9) | [BOLD] 22.0 | 52.6\n+RC (10) | 20.7 | [BOLD] 50.7 | +RC+LA (10) | 21.2 | [BOLD] 52.9\nDCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7\nDCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "387a041a-727b-42a7-a6b1-1c83ae902c61",
    "input": "## Claim\nHere is a claim: We do not have competitive results to Guo et al. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2453b511-2b26-4f10-9120-ada1fd98d64c",
    "input": "## Claim\nHere is a claim: I examine the results of our findings with regard to the best-performing classifier. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 8: Sentiment classification evaluation, using different classifiers on the test set.\nClassifier | Positive Sentiment Precision | Positive Sentiment Recall | Positive Sentiment Fscore\nSVM-w/o neg. | 0.57 | 0.72 | 0.64\nSVM-Punct. neg. | 0.58 | 0.70 | 0.63\nSVM-our-neg. | 0.58 | 0.73 | 0.65\nCNN | 0.63 | 0.83 | 0.72\nCNN-LSTM | 0.71 | 0.72 | 0.72\nCNN-LSTM-Our-neg-Ant | [BOLD] 0.78 | [BOLD] 0.77 | [BOLD] 0.78\n[EMPTY] | Negative Sentiment | Negative Sentiment | Negative Sentiment\n[EMPTY] | Precision | Recall | Fscore\nSVM-w/o neg. | 0.78 | 0.86 | 0.82\nSVM-Punct. neg. | 0.78 | 0.87 | 0.83\nSVM-Our neg. | 0.80 | 0.87 | 0.83\nCNN | 0.88 | 0.72 | 0.79\nCNN-LSTM. | 0.83 | 0.83 | 0.83\nCNN-LSTM-our-neg-Ant | [BOLD] 0.87 | [BOLD] 0.87 | [BOLD] 0.87\n[EMPTY] | Train | [EMPTY] | Test\nPositive tweets | 5121 | [EMPTY] | 1320\nNegative tweets | 9094 | [EMPTY] | 2244\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "35e1aff5-32e2-45a6-bc1d-ecc90f105f47",
    "input": "## Claim\nHere is a claim: Several groups of words are much more likely to appear in a complaint, and are used to express complaints per se: about orders or deliveries (in the retail domain), about access (in complaints to service providers) and about parts of tech products (in tech). Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 5: Group text features associated with tweets that are complaints and not complaints. Features are sorted by Pearson correlation (r) between their each feature\u2019s normalized frequency and the outcome. We restrict to only the top six categories for each feature type. All correlations are significant at p\n[BOLD] Complaints  [BOLD] Label | [BOLD] Complaints  [BOLD] Words | [BOLD] Complaints  [ITALIC] r | [BOLD] Not Complaints  [BOLD] Label | [BOLD] Not Complaints  [BOLD] Words | [BOLD] Not Complaints  [ITALIC] r\n[BOLD] LIWC Features | [BOLD] LIWC Features | [BOLD] LIWC Features | [BOLD] LIWC Features | [BOLD] LIWC Features | [BOLD] LIWC Features\nNEGATE | not, no, can\u2019t, don\u2019t, never, nothing, doesn\u2019t, won\u2019t | .271 | POSEMO | thanks, love, thank, good, great, support, lol, win | .185\nRELATIV | in, on, when, at, out, still, now, up, back, new | .225 | AFFECT | thanks, love, thank, good, great, support, lol | .111\nFUNCTION | the, i, to, a, my, and, you, for, is, in | .204 | SHEHE | he, his, she, her, him, he\u2019s, himself | .105\nTIME | when, still, now, back, new, never, after, then, waiting | .186 | MALE | he, his, man, him, sir, he\u2019s, son | .086\nDIFFER | not, but, if, or, can\u2019t, really, than, other, haven\u2019t | .169 | FEMALE | she, her, girl, mom, ma, lady, mother, female, mrs | .084\nCOGPROC | not, but, how, if, all, why, or, any, need | .132 | ASSENT | yes, ok, awesome, okay, yeah, cool, absolutely, agree | .080\n[BOLD] Word2Vec Clusters | [BOLD] Word2Vec Clusters | [BOLD] Word2Vec Clusters | [BOLD] Word2Vec Clusters | [BOLD] Word2Vec Clusters | [BOLD] Word2Vec Clusters\nCust. Service | service, customer, contact, job, staff, assist, agent | .136 | Gratitude | thanks, thank, good, great, support, everyone, huge, proud | .089\nOrder | order, store, buy, free, delivery, available, package | .128 | Family | old, friend, family, mom, wife, husband, younger | .063\nIssues | delayed, closed, between, outage, delay, road, accident | .122 | Voting | favorite, part, stars, model, vote, models, represent | .060\nTime Ref. | been, yet, haven\u2019t, long, happened, yesterday, took | .122 | Contests | Christmas, gift, receive, entered, giveaway, enter, cards | .058\nTech Parts | battery, laptop, screen, warranty, desktop, printer | .100 | Pets | dogs, cat, dog, pet, shepherd, fluffy, treats | .054\nAccess | use, using, error, password, access, automatically, reset | .098 | Christian | god, shall, heaven, spirit, lord, belongs, soul, believers | .053\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "144aa87d-c757-4945-bf5f-39149c5ba574",
    "input": "## Claim\nHere is a claim: These results indicate dense connections do play a significant role in our model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\n-{4} dense block | 24.8 | 54.9\n-{3, 4} dense blocks | 23.8 | 54.1\n-{2, 3, 4} dense blocks | 23.2 | 53.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d3fb5a11-bfc1-4394-ad0d-3d78e82880e7",
    "input": "## Claim\nHere is a claim: This empirically shows that compared to recurrent graph encoders, DCGCNs do not necessarily learn better representations for graphs. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.\n[BOLD] Model | [BOLD] T | #P | B | C\nSeq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1\nGGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4\nSeq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5\nGGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5\nDCGCN (ours) | S | [BOLD] 19.1M | 27.9 | 57.3\nDCGCN (ours) | E | 92.5M | [BOLD] 30.4 | [BOLD] 59.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4158cd57-dd27-47fd-b14e-d3df4b88c3aa",
    "input": "## Claim\nHere is a claim: [CONTINUE] The most interesting ones are mask, rage, and cry, which significantly increase accuracy. Does the following context support or refute the claim?\n\n## Table\nPaper title: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations\nTable caption: Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.\n[BOLD] Emoji alias | [BOLD] N | [BOLD] emoji # | [BOLD] emoji % | [BOLD] no-emoji # | [BOLD] no-emoji % | [BOLD] \u0394%\nmask | 163 | 154 | 94.48 | 134 | 82.21 | - 12.27\ntwo_hearts | 87 | 81 | 93.10 | 77 | 88.51 | - 4.59\nheart_eyes | 122 | 109 | 89.34 | 103 | 84.43 | - 4.91\nheart | 267 | 237 | 88.76 | 235 | 88.01 | - 0.75\nrage | 92 | 78 | 84.78 | 66 | 71.74 | - 13.04\ncry | 116 | 97 | 83.62 | 83 | 71.55 | - 12.07\nsob | 490 | 363 | 74.08 | 345 | 70.41 | - 3.67\nunamused | 167 | 121 | 72.46 | 116 | 69.46 | - 3.00\nweary | 204 | 140 | 68.63 | 139 | 68.14 | - 0.49\njoy | 978 | 649 | 66.36 | 629 | 64.31 | - 2.05\nsweat_smile | 111 | 73 | 65.77 | 75 | 67.57 | 1.80\nconfused | 77 | 46 | 59.74 | 48 | 62.34 | 2.60\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "88a1e00a-fb2f-47ef-b350-3f22f3214735",
    "input": "## Claim\nHere is a claim: Results with BERT show that contextual information is not always valuable for performance improvement. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nModel | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time\nRockt\u00e4schel et\u00a0al. ( 2016 ) | Rockt\u00e4schel et\u00a0al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | -\nThis | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 | [BOLD] 90.49 | 0.696\nThis | GRU | 6.41M | [BOLD] 85.71 | 0.245 | [BOLD] 86.05 | 0.419 | [BOLD] 90.29 | 0.529 | 90.10 | 0.695\nThis | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580\nWork | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555\n[EMPTY] | LRN | 4.25M | 84.88 | [BOLD] 0.209 | 85.06 | [BOLD] 0.223 | 89.98 | [BOLD] 0.488 | 89.93 | [BOLD] 0.506\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "bfe65751-16ee-43e3-9cc4-8301d4625a8e",
    "input": "## Claim\nHere is a claim: The Transformer performs best in terms of R-1 while Hi-MAP outperforms it on R-2 and R-SU. Does the following context support or refute the claim?\n\n## Table\nPaper title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\nTable caption: Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.\n[BOLD] Method | [BOLD] R-1 | [BOLD] R-2 | [BOLD] R-SU\nFirst-1 | 26.83 | 7.25 | 6.46\nFirst-2 | 35.99 | 10.17 | 12.06\nFirst-3 | 39.41 | 11.77 | 14.51\nLexRank Erkan and Radev ( 2004 ) | 38.27 | 12.70 | 13.20\nTextRank Mihalcea and Tarau ( 2004 ) | 38.44 | 13.10 | 13.50\nMMR Carbonell and Goldstein ( 1998 ) | 38.77 | 11.98 | 12.91\nPG-Original Lebanoff et\u00a0al. ( 2018 ) | 41.85 | 12.91 | 16.46\nPG-MMR Lebanoff et\u00a0al. ( 2018 ) | 40.55 | 12.36 | 15.87\nPG-BRNN Gehrmann et\u00a0al. ( 2018 ) | 42.80 | 14.19 | 16.75\nCopyTransformer Gehrmann et\u00a0al. ( 2018 ) | [BOLD] 43.57 | 14.03 | 17.37\nHi-MAP (Our Model) | 43.47 | [BOLD] 14.89 | [BOLD] 17.41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "fb82f54b-6618-4820-a69f-c099642adbe6",
    "input": "## Claim\nHere is a claim: Seq2Seq model trained with user annotation is better than Seq2Seq model trained with user and system action annotation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "7020ecb6-4c9f-47c3-9983-05d987388d83",
    "input": "## Claim\nHere is a claim: Considering the two aggregated categories of syntactic and semantic word analogies respectively and both 3CosAdd and 3CosMul metrics, model gr def had the best performance in both cases, even when we included the out-of-vocabulary (oov) terms. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluation of Greek Word Embeddings\nTable caption: Table 3: Summary for 3CosAdd and top-1 nearest vectors.\nCategory Semantic | Category no oov words | gr_def 58.42% | gr_neg10 59.33% | cc.el.300  [BOLD] 68.80% | wiki.el 27.20% | gr_cbow_def 31.76% | gr_d300_nosub 60.79% | gr_w2v_sg_n5 52.70%\n[EMPTY] | with oov words | 52.97% | 55.33% | [BOLD] 64.34% | 25.73% | 28.80% | 55.11% | 47.82%\nSyntactic | no oov words | 65.73% | 61.02% | [BOLD] 69.35% | 40.90% | 64.02% | 53.69% | 52.60%\n[EMPTY] | with oov words | [BOLD] 53.95% | 48.69% | 49.43% | 28.42% | 52.54% | 44.06% | 43.13%\nOverall | no oov words | 63.02% | 59.96% | [BOLD] 68.97% | 36.45% | 52.04% | 56.30% | 52.66%\n[EMPTY] | with oov words | 53.60% | 51.00% | [BOLD] 54.60% | 27.50% | 44.30% | 47.90% | 44.80%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5ffe44d7-3a12-4232-b924-95cea6eec296",
    "input": "## Claim\nHere is a claim: we report below both the performance as assessed with automatic evaluation metrics in Table 3 as well as with human evaluations in Tables 4 and 5, to show that the model trained with our objective does not necessarily sacrifice ROUGE F1 in favour of maintaining the headlines\u2019 readability. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e1b1a384-f780-49ca-94c3-937bbd88825c",
    "input": "## Claim\nHere is a claim: They are effective when the approximate model class is complex and/or the interaction with the environment is infrequent, but become intractable as the interaction becomes more frequent or the state-action space grows large. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.\nGP-MBCM | ACER | PPO | ALDM | GDPL\n1.666 | 0.775 | 0.639 | 1.069 | [BOLD] 0.238\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "4c2dc82f-365f-43a9-b795-daccfa505e9c",
    "input": "## Claim\nHere is a claim: LRN is not the fastest model, with ATR outperforming it by 8%\u223c27%. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nModel | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time\nRockt\u00e4schel et\u00a0al. ( 2016 ) | Rockt\u00e4schel et\u00a0al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | -\nThis | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 | [BOLD] 90.49 | 0.696\nThis | GRU | 6.41M | [BOLD] 85.71 | 0.245 | [BOLD] 86.05 | 0.419 | [BOLD] 90.29 | 0.529 | 90.10 | 0.695\nThis | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580\nWork | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555\n[EMPTY] | LRN | 4.25M | 84.88 | [BOLD] 0.209 | 85.06 | [BOLD] 0.223 | 89.98 | [BOLD] 0.488 | 89.93 | [BOLD] 0.506\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e41db642-41b4-4b36-846a-3e019d6ab36a",
    "input": "## Claim\nHere is a claim: [CONTINUE] On the contrary, for the linear dataset, the recursive implementation fails to efficiently make use of CPU resources and thus the performance gain provided by increasing the batch size is relatively high. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2\nTable caption: Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.\nBatch size | Throughput (instances/s) Balanced | Throughput (instances/s) Moderate | Throughput (instances/s) Linear\n1 | 46.7 | 27.3 | 7.6\n10 | 125.2 | 78.2 | 22.7\n25 | 129.7 | 83.1 | 45.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b2b4fcf5-0b84-4720-ae2c-90e27e36dcef",
    "input": "## Claim\nHere is a claim: Our model outperforms the previous stateof-the-art models on both datasets in terms of F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. \u2020 denotes the previous best state-of-the-art model.\nModel | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1\nCNN zeng2014relation | 0.413 | 0.591 | 0.486 | 0.444 | 0.625 | 0.519\nPCNN zeng2015distant | 0.380 | [BOLD] 0.642 | 0.477 | 0.446 | 0.679 | 0.538\u2020\nEA huang2016attention | 0.443 | 0.638 | 0.523\u2020 | 0.419 | 0.677 | 0.517\nBGWA jat2018attention | 0.364 | 0.632 | 0.462 | 0.417 | [BOLD] 0.692 | 0.521\nBiLSTM-CNN | 0.490 | 0.507 | 0.498 | 0.473 | 0.606 | 0.531\nOur model | [BOLD] 0.541 | 0.595 | [BOLD] 0.566* | [BOLD] 0.507 | 0.652 | [BOLD] 0.571*\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "8c6fd9a3-be98-4473-be7b-5f21d24b48ad",
    "input": "## Claim\nHere is a claim: Comparing the 784-dimensional models, CBOW and CMOW do not seem to complement each other. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nMethod | SUBJ | CR | MR | MPQA | MRPC | TREC | SICK-E | SST2 | SST5 | STS-B | SICK-R\nCBOW/784 | 90.0 | [BOLD] 79.2 | [BOLD] 74.0 | 87.1 | 71.6 | 85.6 | 78.9 | 78.5 | 42.1 | 61.0 | [BOLD] 78.1\nCMOW/784 | 87.5 | 73.4 | 70.6 | [BOLD] 87.3 | 69.6 | [BOLD] 88.0 | 77.2 | 74.7 | 37.9 | 56.5 | 76.2\nHybrid | [BOLD] 90.2 | 78.7 | 73.7 | [BOLD] 87.3 | [BOLD] 72.7 | 87.6 | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8\ncmp. CBOW | +0.2% | -0.6% | -0.4% | +0.2% | +1.5% | +2.3% | +0.6% | +1.4% | +2.9% | +3.9% | -0.4%\ncmp. CMOW | +3.1% | +7.2% | +4.4% | +0% | +4.5% | -0.5% | +2.9% | +6.7% | +14.3 | +12.2% | +2.1%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "69e1dea0-ae41-4ea6-85e2-1976da7f370e",
    "input": "## Claim\nHere is a claim: In conclusion, these results above can show the ineffectiveness of our DCGCN models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.\n[BOLD] GCN +RC (2) | B 16.8 | C 48.1 | [BOLD] GCN +RC+LA (2) | B 18.3 | C 47.9\n+RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1\n+RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8\n+RC (9) | [BOLD] 21.1 | 50.5 | +RC+LA (9) | [BOLD] 22.0 | 52.6\n+RC (10) | 20.7 | [BOLD] 50.7 | +RC+LA (10) | 21.2 | [BOLD] 52.9\nDCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7\nDCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d6fd61c7-8f3c-4a96-90ac-3b84b96b5c00",
    "input": "## Claim\nHere is a claim: This indicates that GINs can be employed in tasks where the distribution of node degrees has a long tail. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3c2a6721-2498-4f45-9f29-d3ebf070edc9",
    "input": "## Claim\nHere is a claim: Although LSTM and GRU outperform LRN by 0.3\u223c0.9 in terms of accuracy, these recurrent units do not sacrifice running efficiency (about 7%\u223c48%) depending on whether LN and BERT are applied. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nModel | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time\nRockt\u00e4schel et\u00a0al. ( 2016 ) | Rockt\u00e4schel et\u00a0al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | -\nThis | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 | [BOLD] 90.49 | 0.696\nThis | GRU | 6.41M | [BOLD] 85.71 | 0.245 | [BOLD] 86.05 | 0.419 | [BOLD] 90.29 | 0.529 | 90.10 | 0.695\nThis | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580\nWork | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555\n[EMPTY] | LRN | 4.25M | 84.88 | [BOLD] 0.209 | 85.06 | [BOLD] 0.223 | 89.98 | [BOLD] 0.488 | 89.93 | [BOLD] 0.506\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "932c9362-088b-48c2-b6bc-63df4037f765",
    "input": "## Claim\nHere is a claim: [CONTINUE] On the other hand, the presence of terms that show positive sentiment or emotions (good, great, win, POSEMO, AFFECT, ASSENT) are among the top most distinctive features for a tweet not being labeled as a complaint. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\n[BOLD] Complaints  [BOLD] Feature | [BOLD] Complaints  [ITALIC] r | [BOLD] Not Complaints  [BOLD] Feature | [BOLD] Not Complaints  [ITALIC] r\n[BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams\nnot | .154 | [URL] | .150\nmy | .131 | ! | .082\nworking | .124 | he | .069\nstill | .123 | thank | .067\non | .119 | , | .064\ncan\u2019t | .113 | love | .064\nservice | .112 | lol | .061\ncustomer | .109 | you | .060\nwhy | .108 | great | .058\nwebsite | .107 | win | .058\nno | .104 | \u2019 | .058\n? | .098 | she | .054\nfix | .093 | : | .053\nwon\u2019t | .092 | that | .053\nbeen | .090 | more | .052\nissue | .089 | it | .052\ndays | .088 | would | .051\nerror | .087 | him | .047\nis | .084 | life | .046\ncharged | .083 | good | .046\n[BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)\nVBN | .141 | UH | .104\n$ | .118 | NNP | .098\nVBZ | .114 | PRP | .076\nNN_VBZ | .114 | HT | .076\nPRP$ | .107 | PRP_. | .076\nPRP$_NN | .105 | PRP_RB | .067\nVBG | .093 | NNP_NNP | .062\nCD | .092 | VBP_PRP | .054\nWRB_VBZ | .084 | JJ | .053\nVBZ_VBN | .084 | DT_JJ | .051\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "8246391e-06c9-4bb7-a7c8-749c2940f735",
    "input": "## Claim\nHere is a claim: Most denying instances get misclassified as commenting (see Table 5), Does the following context support or refute the claim?\n\n## Table\nPaper title: Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM\nTable caption: Table 5: Confusion matrix for testing set predictions\n[BOLD] LabelPrediction | [BOLD] C | [BOLD] D | [BOLD] Q | [BOLD] S\n[BOLD] Commenting | 760 | 0 | 12 | 6\n[BOLD] Denying | 68 | 0 | 1 | 2\n[BOLD] Querying | 69 | 0 | 36 | 1\n[BOLD] Supporting | 67 | 0 | 1 | 26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0ab283cb-2c6e-451e-84c0-791df4822e8c",
    "input": "## Claim\nHere is a claim: As can be seen in Table 1, sparsemax and TVMAX achieve better results overall when compared with softmax, indicating that the use of selective attention leads to better captions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.\n[EMPTY] | MSCOCO spice | MSCOCO cider | MSCOCO rouge [ITALIC] L | MSCOCO bleu4 | MSCOCO meteor | MSCOCO rep\u2193 | Flickr30k spice | Flickr30k cider | Flickr30k rouge [ITALIC] L | Flickr30k bleu4 | Flickr30k meteor | Flickr30k rep\u2193\nsoftmax | 18.4 | 0.967 | 52.9 | 29.9 | 24.9 | 3.76 | 13.5 | 0.443 | 44.2 | 19.9 | 19.1 | 6.09\nsparsemax | [BOLD] 18.9 | [BOLD] 0.990 | [BOLD] 53.5 | [BOLD] 31.5 | [BOLD] 25.3 | 3.69 | [BOLD] 13.7 | [BOLD] 0.444 | [BOLD] 44.3 | [BOLD] 20.7 | [BOLD] 19.3 | 5.84\nTVmax | 18.5 | 0.974 | 53.1 | 29.9 | 25.1 | [BOLD] 3.17 | 13.3 | 0.438 | 44.2 | 20.5 | 19.0 | [BOLD] 3.97\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0d2c4c12-d580-4a42-88a2-1abeada7b180",
    "input": "## Claim\nHere is a claim: We find EWC does not outperform the L2 approach. Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 3: Test BLEU for es-en adaptive training. EWC reduces forgetting compared to other fine-tuning methods, while offering the greatest improvement on the new domain.\n[EMPTY] | [BOLD] Training scheme | [BOLD] Health | [BOLD] Bio\n1 | Health | [BOLD] 35.9 | 33.1\n2 | Bio | 29.6 | 36.1\n3 | Health and Bio | 35.8 | 37.2\n4 | 1 then Bio, No-reg | 30.3 | 36.6\n5 | 1 then Bio, L2 | 35.1 | 37.3\n6 | 1 then Bio, EWC | 35.2 | [BOLD] 37.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "cd5aaefc-4143-4530-96d8-12f0b1ab6f2a",
    "input": "## Claim\nHere is a claim: there were no adjectives in the questions except for the \"concept\" and \"property\" words, for the adjectives were replaced with prepositional phrases, for instance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VIII: Precision scores for the Semantic Analogy Test\nQuestions Subset | # of Questions Seen | GloVe | Word2Vec | Proposed\nAll | 8783 | 78.94 | 81.03 | 79.96\nAt least one | 1635 | 67.58 | 70.89 | 67.89\nconcept word | 1635 | 67.58 | 70.89 | 67.89\nAll concept words | 110 | 77.27 | 89.09 | 83.64\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "b9b1312c-bfee-4f66-b10d-cedc62a7476a",
    "input": "## Claim\nHere is a claim: On the other side, H-CMOW shows, among others, no improvements at BShift. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "784ba011-5c5c-49c2-9604-da58674db53a",
    "input": "## Claim\nHere is a claim: In contrast, RoBERTa-large drops only 3.1 points when trained and evaluated on the split from Sap et al. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See \u00a72 for model details. * indicates our replication experiments.\nModel | Accuracy\nBigramPMI\u00a0Goodwin et al. ( 2012 ) | 63.4\nPMI\u00a0Gordon et al. ( 2011 ) | 65.4\nPMI+Connectives\u00a0Luo et al. ( 2016 ) | 70.2\nPMI+Con.+Phrase\u00a0Sasaki et al. ( 2017 ) | 71.4\nBERT-large\u00a0Wang et al. ( 2019 ) | 70.5\nBERT-large\u00a0Sap et al. ( 2019 ) | 75.0\nBERT-large\u00a0Li et al. ( 2019 ) | 75.4\nRoBERTa-large (finetuned) | 90.6\nBERT-large (finetuned)* | 76.5 \u00b1 2.7\nRoBERTa-large (finetuned)* | 87.7 \u00b1 0.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "9eb04d72-35f4-42a2-84cb-8233924128b3",
    "input": "## Claim\nHere is a claim: [CONTINUE] Moreover, for TVMAX, automatic metrics results are slightly worse than sparsemax but still superior to softmax on MSCOCO and similar on Flickr30k. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.\n[EMPTY] | MSCOCO spice | MSCOCO cider | MSCOCO rouge [ITALIC] L | MSCOCO bleu4 | MSCOCO meteor | MSCOCO rep\u2193 | Flickr30k spice | Flickr30k cider | Flickr30k rouge [ITALIC] L | Flickr30k bleu4 | Flickr30k meteor | Flickr30k rep\u2193\nsoftmax | 18.4 | 0.967 | 52.9 | 29.9 | 24.9 | 3.76 | 13.5 | 0.443 | 44.2 | 19.9 | 19.1 | 6.09\nsparsemax | [BOLD] 18.9 | [BOLD] 0.990 | [BOLD] 53.5 | [BOLD] 31.5 | [BOLD] 25.3 | 3.69 | [BOLD] 13.7 | [BOLD] 0.444 | [BOLD] 44.3 | [BOLD] 20.7 | [BOLD] 19.3 | 5.84\nTVmax | 18.5 | 0.974 | 53.1 | 29.9 | 25.1 | [BOLD] 3.17 | 13.3 | 0.438 | 44.2 | 20.5 | 19.0 | [BOLD] 3.97\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ee5d43ff-0f49-4fe4-8d83-d0e9571e46d3",
    "input": "## Claim\nHere is a claim: The best performing system is KnowComb. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nDataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb\n[ITALIC] Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 | [BOLD] 76.41\n[ITALIC] WinoCoref | AntePre | 68.37 | 74.32 | \u2014\u2013 | 88.48 | 88.95 | [BOLD] 89.32\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2d8b85cb-aeef-4463-b876-4c2777e9277f",
    "input": "## Claim\nHere is a claim: We divide the dataset into 5 folds according to the users\u2019 identity information (e.g., 619,1802, etc.). Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 3: Cue and token distribution in the conversational negation corpus.\nTotal negation cues | 2921\nTrue negation cues | 2674\nFalse negation cues | 247\nAverage scope length | 2.9\nAverage sentence length | 13.6\nAverage tweet length | 22.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f66b6574-4444-44c3-adca-6bba7c77ffc1",
    "input": "## Claim\nHere is a claim: the major drawback is that the dataset contains only 583 examples in the test set, which makes it difficult to measure model performance robustly. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8cc4d0df-ae69-445b-b1ec-7b5b74927e0d",
    "input": "## Claim\nHere is a claim: The alternative creates a conjoined structure consisting of both the non-polar cue and the target alternative. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nCue | App. | Prod. | Cov.\nin | 47 | 55.3 | 9.40\nwas | 55 | 61.8 | 11.0\nto | 82 | 40.2 | 16.4\nthe | 85 | 38.8 | 17.0\na | 106 | 57.5 | 21.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e8fa28c5-0b0f-4d5e-a963-f0867a4b2f2a",
    "input": "## Claim\nHere is a claim: As the table 4 depicts, the precision increases with the growth of d, but the training time also increases. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 4: Precisions on the Wikidata dataset with different choice of d.\nRecall | 0.1 | 0.2 | 0.3 | AUC | Time\n[ITALIC] d=1 | 0.602 | 0.487 | 0.403 | 0.367 | 4h\n[ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.370 | -\n[ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h\n[ITALIC] d=8 | 0.650 | 0.519 | 0.422 | 0.405 | 8h\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "555cfc65-dbb1-41e2-8b2c-ca1d6db747d1",
    "input": "## Claim\nHere is a claim: This is unexpected as encoding a bigger graph (containing more information) should be easier than encoding smaller graphs. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "736bdd5b-3280-45f8-ba57-7a23885bf208",
    "input": "## Claim\nHere is a claim: Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain high degree nodes. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "83e691c8-43ce-46c6-bbc7-2b3d400d8c58",
    "input": "## Claim\nHere is a claim: Our model does not improve the results in the translation tasks. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Vector-spaces with Noisy Supervised Lexicons\nTable caption: Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En\u2192De, En\u2192Fi and En\u2192Es improvements are significant at p<0.05 according to ANOVA on the different runs.\nMethod | En\u2192It best | En\u2192It avg | En\u2192It iters | En\u2192De best | En\u2192De avg | En\u2192De iters | En\u2192Fi best | En\u2192Fi avg | En\u2192Fi iters | En\u2192Es best | En\u2192Es avg | En\u2192Es iters\nArtetxe et\u00a0al., 2018b | [BOLD] 48.53 | 48.13 | 573 | 48.47 | 48.19 | 773 | 33.50 | 32.63 | 988 | 37.60 | 37.33 | 808\nNoise-aware Alignment | [BOLD] 48.53 | [BOLD] 48.20 | 471 | [BOLD] 49.67 | [BOLD] 48.89 | 568 | [BOLD] 33.98 | [BOLD] 33.68 | 502 | [BOLD] 38.40 | [BOLD] 37.79 | 551\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "34972ec1-4b2a-42cb-8b10-9f774ce112e9",
    "input": "## Claim\nHere is a claim: the best-performing system outperforms the baseline in all cue types with the largest gains of 9.5% and 8.6% on the actual and false cue recall, respectively. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 4: Cue classification on the test set.\n[EMPTY] | [BOLD] F-Score  [BOLD] Baseline | [BOLD] F-Score  [BOLD] Proposed | [BOLD] Support\nFalse cues | 0.61 | 0.68 | 47\nActual cues | 0.97 | 0.98 | 557\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "d9f56bd3-ead2-44b1-8f61-415c79ef85fa",
    "input": "## Claim\nHere is a claim: In total, 1,232 tweets (62.4%) are complaints and 739 are not complaints (37.6%). Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 3: Number of tweets annotated as complaints across the nine domains.\n[BOLD] Category | [BOLD] Complaints | [BOLD] Not Complaints\nFood & Beverage | 95 | 35\nApparel | 141 | 117\nRetail | 124 | 75\nCars | 67 | 25\nServices | 207 | 130\nSoftware & Online Services | 189 | 103\nTransport | 139 | 109\nElectronics | 174 | 112\nOther | 96 | 33\nTotal | 1232 | 739\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5d51c21b-4d94-4b64-bb07-a899563e5b9a",
    "input": "## Claim\nHere is a claim: The Wiener filtering method shows lower DCE, but higher WER than no enhancement. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition\nTable caption: TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set\nMethod | WER (%) | DCE\nNo enhancement | 17.3 | 0.828\nWiener filter | 19.5 | 0.722\nMinimizing DCE | 15.8 | [BOLD] 0.269\nFSEGAN | 14.9 | 0.291\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=0) | 15.6 | 0.330\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 14.4 | 0.303\nClean speech | 5.7 | 0.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ae5706f0-91bd-452f-9fae-1d25d7bbcffc",
    "input": "## Claim\nHere is a claim: our method uses the combination of SPINE and Word2Sense to improve the performance of sentiment classification task Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "7f0dcd5c-173e-4858-a85d-6f861623a0d4",
    "input": "## Claim\nHere is a claim: Furthermore, this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes, such as life and physical sciences, architecture, engineering, computer science and mathematics . Does the following context support or refute the claim?\n\n## Table\nPaper title: Assessing Gender Bias in Machine Translation \u2013 A Case Study with Google Translate\nTable caption: Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table\nCategory | Female (%) | Male (%) | Neutral (%)\nOffice and administrative support | 11.015 | 58.812 | 16.954\nArchitecture and engineering | 2.299 | 72.701 | 10.92\nFarming, fishing, and forestry | 12.179 | 62.179 | 14.744\nManagement | 11.232 | 66.667 | 12.681\nCommunity and social service | 20.238 | 62.5 | 10.119\nHealthcare support | 25.0 | 43.75 | 17.188\nSales and related | 8.929 | 62.202 | 16.964\nInstallation, maintenance, and repair | 5.22 | 58.333 | 17.125\nTransportation and material moving | 8.81 | 62.976 | 17.5\nLegal | 11.905 | 72.619 | 10.714\nBusiness and financial operations | 7.065 | 67.935 | 15.58\nLife, physical, and social science | 5.882 | 73.284 | 10.049\nArts, design, entertainment, sports, and media | 10.36 | 67.342 | 11.486\nEducation, training, and library | 23.485 | 53.03 | 9.091\nBuilding and grounds cleaning and maintenance | 12.5 | 68.333 | 11.667\nPersonal care and service | 18.939 | 49.747 | 18.434\nHealthcare practitioners and technical | 22.674 | 51.744 | 15.116\nProduction | 14.331 | 51.199 | 18.245\nComputer and mathematical | 4.167 | 66.146 | 14.062\nConstruction and extraction | 8.578 | 61.887 | 17.525\nProtective service | 8.631 | 65.179 | 12.5\nFood preparation and serving related | 21.078 | 58.333 | 17.647\nTotal | 11.76 | 58.93 | 15.939\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "511ea10f-bd33-40f4-a2ab-40fe6c582bbd",
    "input": "## Claim\nHere is a claim: Still, lemma-based targets significantly7 (p \u2264 .005) outperform type-based targets in terms of F-measure in all cases. Does the following context support or refute the claim?\n\n## Table\nPaper title: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources\nTable caption: Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.\n[EMPTY] | WN-N P | WN-N R | WN-N F | WN-V P | WN-V R | WN-V F | VN P | VN R | VN F\nContext: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2\ntype | .700 | .654 | .676 | .535 | .474 | .503 | .327 | .309 | .318\nx+POS | .699 | .651 | .674 | .544 | .472 | .505 | .339 | .312 | .325\nlemma | .706 | .660 | .682 | .576 | .520 | .547 | .384 | .360 | .371\nx+POS | <bold>.710</bold> | <bold>.662</bold> | <bold>.685</bold> | <bold>.589</bold> | <bold>.529</bold> | <bold>.557</bold> | <bold>.410</bold> | <bold>.389</bold> | <bold>.399</bold>\nContext: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep\ntype | .712 | .661 | .686 | .545 | .457 | .497 | .324 | .296 | .310\nx+POS | .715 | .659 | .686 | .560 | .464 | .508 | .349 | .320 | .334\nlemma | <bold>.725</bold> | <bold>.668</bold> | <bold>.696</bold> | .591 | .512 | .548 | .408 | .371 | .388\nx+POS | .722 | .666 | .693 | <bold>.609</bold> | <bold>.527</bold> | <bold>.565</bold> | <bold>.412</bold> | <bold>.381</bold> | <bold>.396</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "44c10fb7-e496-4663-8f4c-d71ccd7dd67d",
    "input": "## Claim\nHere is a claim: Table 2 shows that the model with cyclic loss (M2) and the model with cyclic loss, paraphrase loss, and language model loss (M5) both have lower Sim than M0 on both datasets under similar Acc. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\n[EMPTY] | Acc | Sim | PP | GM\nM0: shen-1 | 0.818 | 0.719 | 37.3 | 10.0\nM1: M0 [ITALIC] +para | 0.819 | 0.734 | 26.3 | 14.2\nM2: M0 [ITALIC] +cyc | 0.813 | 0.770 | 36.4 | 18.8\nM3: M0 [ITALIC] +cyc+lang | 0.807 | 0.796 | 28.4 | 21.5\nM4: M0 [ITALIC] +cyc+para | 0.798 | 0.783 | 39.7 | 19.2\nM5: M0 [ITALIC] +cyc+para+lang | 0.804 | 0.785 | 27.1 | 20.3\nM6: M0 [ITALIC] +cyc+2d | 0.805 | [BOLD] 0.817 | 43.3 | 21.6\nM7: M6+ [ITALIC] para+lang | 0.818 | 0.805 | [BOLD] 29.0 | [BOLD] 22.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ecd566bf-ff78-4522-a91d-986557c5748c",
    "input": "## Claim\nHere is a claim: The proposed method outperforms the original embeddings and performs on par with the SOV. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7b76da1e-3531-49c1-ad27-dfe176882449",
    "input": "## Claim\nHere is a claim: In addition, the training time results in Table 3 confirm the computational disadvantage of LRN over all other recurrent units, where LRN slows down compared to ATR and SRU by approximately 25%. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.\nModel | #Params | BLEU | Train | Decode\nGNMT | - | 24.61 | - | -\nGRU | 206M | 26.28 | 2.67 | 45.35\nATR | 122M | 25.70 | 1.33 | [BOLD] 34.40\nSRU | 170M | 25.91 | 1.34 | 42.84\nLRN | 143M | 26.26 | [BOLD] 0.99 | 36.50\noLRN | 164M | [BOLD] 26.73 | 1.15 | 40.19\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "7ee1b8c8-3a20-4d62-9344-ec5f5a542ba9",
    "input": "## Claim\nHere is a claim: As for the micro F1 evaluation metric, our model does not achieve the highest performance (83.54%) on the FNC-1 testing subset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Variational Self-attention Model for Sentence Representation\nTable caption: Table 2: Performance comparison with the state-of-art algorithms on the FNC-1 test dataset.\nModel | Accuracy (%) agree | Accuracy (%) disagree | Accuracy (%) discuss | Accuracy (%) unrelated | Micro F1(%)\nAverage of Word2vec Embedding | 12.43 | 01.30 | 43.32 | 74.24 | 45.53\nCNN-based Sentence Embedding | 24.54 | 05.06 | 53.24 | 79.53 | 81.72\nRNN-based Sentence Embedding | 24.42 | 05.42 | 69.05 | 65.34 | 78.70\nSelf-attention Sentence Embedding | 23.53 | 04.63 | 63.59 | 80.34 | 80.11\nOur model | 28.53 | 10.43 | 65.43 | 82.43 | [BOLD] 83.54\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e55f868d-c3b3-42e4-91ae-3e87cab511f3",
    "input": "## Claim\nHere is a claim: This shows that more attention heads, thereby attending to multiple different contexts at once, is important to boosting performance to state-of-the-art results. Does the following context support or refute the claim?\n\n## Table\nPaper title: Localization of Fake News Detection via Multitask Transfer Learning\nTable caption: Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. \u201cEffect\u201d refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.\n# of Heads | Accuracy | Val. Loss | Effect\n1 | 89.44% | 0.2811 | -6.84%\n2 | 91.20% | 0.2692 | -5.08%\n4 | 93.85% | 0.2481 | -2.43%\n8 | 96.02% | 0.2257 | -0.26%\n10 | 96.28% | 0.2197 | [EMPTY]\n16 | 96.32% | 0.2190 | +0.04\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b182dd3e-b44e-4d56-aa65-9ba85fc1b45a",
    "input": "## Claim\nHere is a claim: (1) BERT is able to capture the gist of the summaries, and thus is appropriate for predicting good summaries; (2) the sentences in good summaries tend to have high tf-idf similarity to the target article; Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nMetric | [ITALIC] \u03c1 | [ITALIC] r | G-Pre | G-Rec\nROUGE-1 | .290 | .304 | .392 | .428\nROUGE-2 | .259 | .278 | .408 | .444\nROUGE-L | .274 | .297 | .390 | .426\nROUGE-SU4 | .282 | .279 | .404 | .440\nBLEU-1 | .256 | .281 | .409 | .448\nBLEU-2 | .301 | .312 | .411 | .446\nBLEU-3 | .317 | .312 | .409 | .444\nBLEU-4 | .311 | .307 | .409 | .446\nBLEU-5 | .308 | .303 | .420 | .459\nMETEOR | .305 | .285 | .409 | .444\nInferSent-Cosine | [BOLD] .329 | [BOLD] .339 | .417 | .460\nBERT-Cosine | .312 | .335 | [BOLD] .440 | [BOLD] .484\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8ca9d3e5-cce5-41f9-ada1-afa9e75a6cf1",
    "input": "## Claim\nHere is a claim: Despite our system achieving the same level of performance compared to a state-of-art general coreference system, we still observe significant performance improvement on the ACE and OntoNotes datasets. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.\nSystem | MUC | BCUB | CEAFe | AVG\nACE | ACE | ACE | ACE | ACE\nIlliCons | [BOLD] 78.17 | 81.64 | [BOLD] 78.45 | [BOLD] 79.42\nKnowComb | 77.51 | [BOLD] 81.97 | 77.44 | 78.97\nOntoNotes | OntoNotes | OntoNotes | OntoNotes | OntoNotes\nIlliCons | 84.10 | [BOLD] 78.30 | [BOLD] 68.74 | [BOLD] 77.05\nKnowComb | [BOLD] 84.33 | 78.02 | 67.95 | 76.76\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5ea07570-7f39-4790-b562-22fd697fb6ef",
    "input": "## Claim\nHere is a claim: Results also show the global node is more effective than the linear combination. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ebaa19a5-b7cf-4f3b-8b04-b4304e779ff8",
    "input": "## Claim\nHere is a claim: All fluency problems we found were very slight and no added or wrong-valued slots were found, so missed slots are the main problem. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).\n[BOLD] Training data | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] Disfl\nOriginal | 0 | 22 | 0 | 14\nCleaned added | 0 | 23 | 0 | 14\nCleaned missing | 0 | 1 | 0 | 2\nCleaned | 0 | 0 | 0 | 5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "36fa8fb5-8f19-4084-8cbb-4c3195e16bc3",
    "input": "## Claim\nHere is a claim: The models using BoC do not outperform models using BoW as well as ASM features. Does the following context support or refute the claim?\n\n## Table\nPaper title: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data\nTable caption: Table 1: Performance of supervised learning models with different features.\nFeature | LR P | LR R | LR F1 | SVM P | SVM R | SVM F1 | ANN P | ANN R | ANN F1\n+BoW | 0.93 | 0.91 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+BoC (Wiki-PubMed-PMC) | 0.94 | 0.92 | [BOLD] 0.93 | 0.94 | 0.92 | [BOLD] 0.93 | 0.91 | 0.91 | [BOLD] 0.91\n+BoC (GloVe) | 0.93 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+ASM | 0.90 | 0.85 | 0.88 | 0.90 | 0.86 | 0.88 | 0.89 | 0.89 | 0.89\n+Sentence Embeddings(SEs) | 0.89 | 0.89 | 0.89 | 0.90 | 0.86 | 0.88 | 0.88 | 0.88 | 0.88\n+BoC(Wiki-PubMed-PMC)+SEs | 0.92 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "16813a0f-c973-4891-a6c7-2994bfb3abb2",
    "input": "## Claim\nHere is a claim: this shows the importance of the SRBR strategy. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VIII: Precision scores for the Semantic Analogy Test\nQuestions Subset | # of Questions Seen | GloVe | Word2Vec | Proposed\nAll | 8783 | 78.94 | 81.03 | 79.96\nAt least one | 1635 | 67.58 | 70.89 | 67.89\nconcept word | 1635 | 67.58 | 70.89 | 67.89\nAll concept words | 110 | 77.27 | 89.09 | 83.64\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "9db05047-3aba-476e-b6dc-214afb5eea24",
    "input": "## Claim\nHere is a claim: especially for DAMD, modeling multi-action system responses, which are notoriously rare and difficult to collect, significantly improves the performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "04ed07c3-533c-4981-9516-1132391b69a0",
    "input": "## Claim\nHere is a claim: it is critical to realize that the conversational negation corpus is indeed a true label corpus, while neither O1 nor O2 are correct. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 3: Cue and token distribution in the conversational negation corpus.\nTotal negation cues | 2921\nTrue negation cues | 2674\nFalse negation cues | 247\nAverage scope length | 2.9\nAverage sentence length | 13.6\nAverage tweet length | 22.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0b7152d7-54db-4a1f-b5cb-18c3b4b17aa2",
    "input": "## Claim\nHere is a claim: Results in Table 7 show that although the accuracy on SNLI is acceptable, gLRN and eLRN perform significantly worse on the PTB task. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 7: Test accuracy on SNLI task with Base+LN setting and test perplexity on PTB task with Base setting.\nModel | SNLI | PTB\nLRN | [BOLD] 85.06 | [BOLD] 61.26\ngLRN | 84.72 | 92.49\neLRN | 83.56 | 169.81\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "17f3bd4a-b9fd-44ad-b5fb-cef1a993aece",
    "input": "## Claim\nHere is a claim: It can be noted that the use of discourse markers is crucial for the task since the results after removing them from the dataset is far from optimal. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 8: Sentiment classification evaluation, using different classifiers on the test set.\nClassifier | Positive Sentiment Precision | Positive Sentiment Recall | Positive Sentiment Fscore\nSVM-w/o neg. | 0.57 | 0.72 | 0.64\nSVM-Punct. neg. | 0.58 | 0.70 | 0.63\nSVM-our-neg. | 0.58 | 0.73 | 0.65\nCNN | 0.63 | 0.83 | 0.72\nCNN-LSTM | 0.71 | 0.72 | 0.72\nCNN-LSTM-Our-neg-Ant | [BOLD] 0.78 | [BOLD] 0.77 | [BOLD] 0.78\n[EMPTY] | Negative Sentiment | Negative Sentiment | Negative Sentiment\n[EMPTY] | Precision | Recall | Fscore\nSVM-w/o neg. | 0.78 | 0.86 | 0.82\nSVM-Punct. neg. | 0.78 | 0.87 | 0.83\nSVM-Our neg. | 0.80 | 0.87 | 0.83\nCNN | 0.88 | 0.72 | 0.79\nCNN-LSTM. | 0.83 | 0.83 | 0.83\nCNN-LSTM-our-neg-Ant | [BOLD] 0.87 | [BOLD] 0.87 | [BOLD] 0.87\n[EMPTY] | Train | [EMPTY] | Test\nPositive tweets | 5121 | [EMPTY] | 1320\nNegative tweets | 9094 | [EMPTY] | 2244\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "85c78beb-8fa3-4c80-83a2-21ef165f4090",
    "input": "## Claim\nHere is a claim: 1, where the x-axis refers to each metric and the y-axis refers to the number of sessions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "15ca12b8-56ac-4053-8fe7-3b00a97b9a07",
    "input": "## Claim\nHere is a claim: For Task B, the baseline model outperformed all models trained on the stacked learner when using only plain averaged word embeddings. Does the following context support or refute the claim?\n\n## Table\nPaper title: UKP TU-DA at GermEval 2017:Deep Learning for Aspect Based Sentiment Detection\nTable caption: Table 5: Task B results\n[EMPTY] | Micro F1\nBaseline | 0.709\nW2V (<italic>d</italic>=50) | 0.736\nW2V (<italic>d</italic>=500) | 0.753\nS2V | 0.748\nS2V + W2V (<italic>d</italic>=50) | 0.744\nS2V + K + W2V(<italic>d</italic>=50) | 0.749\nSIF (DE) | 0.759\nSIF (DE-EN) | <bold>0.765</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "05f252e0-8409-4a35-8596-dc70f2f2b281",
    "input": "## Claim\nHere is a claim: This is evident from the significant drop in ARI score from OD to OD (no polarity shifters) since the only change in those variants is of sentiment polarity shifters. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.\n[EMPTY] | Difference Function | Seanad Abolition | Video Games | Pornography\nOD-parse | Absolute | 0.01 | -0.01 | 0.07\nOD-parse | JS div. | 0.01 | -0.01 | -0.01\nOD-parse | EMD | 0.07 | 0.01 | -0.01\nOD | Absolute | [BOLD] 0.54 | [BOLD] 0.56 | [BOLD] 0.41\nOD | JS div. | 0.07 | -0.01 | -0.02\nOD | EMD | 0.26 | -0.01 | 0.01\nOD (no polarity shifters) | Absolute | 0.23 | 0.08 | 0.04\nOD (no polarity shifters) | JS div. | 0.09 | -0.01 | -0.02\nOD (no polarity shifters) | EMD | 0.10 | 0.01 | -0.01\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "472b4643-6646-495a-825f-bf3d650a90a0",
    "input": "## Claim\nHere is a claim: It is observed that the former outperforms the latter, indicating the key role of dialogue state estimation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "73c20a83-ae30-4b98-b2fc-1ba2d74977b6",
    "input": "## Claim\nHere is a claim: we present BLEU and TER for the REV systems in Table 5, [CONTINUE] While Transformer models are the best ones according to the evaluation metrics, Does the following context support or refute the claim?\n\n## Table\nPaper title: Lost in Translation: Loss and Decay of Linguistic Richness in Machine Translation\nTable caption: Table 5: Automatic evaluation scores (BLEU and TER) for the REV systems.\nSystem reference | BLEU\u2191 | TER\u2193\nen-fr-rnn-rev | 33.3 | 50.2\nen-fr-smt-rev | 36.5 | 47.1\nen-fr-trans-rev | [BOLD] 36.8 | [BOLD] 46.8\nen-es-rnn-rev | 37.8 | 45.0\nen-es-smt-rev | 39.2 | 44.0\nen-es-trans-rev | [BOLD] 40.4 | [BOLD] 42.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "bb466346-414c-4c69-b178-131999ea4381",
    "input": "## Claim\nHere is a claim: accuracy on average the proposed method outperform other approaches by 2.8% and 2.45% for B-CNN and R-CNN respectively. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6e0cec48-2365-4b12-a062-d9a7a42c0fa8",
    "input": "## Claim\nHere is a claim: Our joint model does not outperform all the base lines, with a gap of only 10.5 CoNLL F1 points from the last published results (KCP), and only surpassing our strong lemma baseline by 3 points. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\n[BOLD] Model | R | MUC P | [ITALIC] F1 | R | B3 P | [ITALIC] F1 | R | CEAF- [ITALIC] e P | [ITALIC] F1 | CoNLL  [ITALIC] F1\n[BOLD] Baselines | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nCluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5\nCV Cybulska and Vossen ( 2015a ) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73\nKCP Kenyon-Dean et\u00a0al. ( 2018 ) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69\nCluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6\n[BOLD] Model Variants | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nDisjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5\nJoint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | [BOLD] 79.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "b3ea990d-54c0-4078-a9fc-75898eb8965c",
    "input": "## Claim\nHere is a claim: When redundancy removal was applied to LogReg, it produces only marginal improvement. Does the following context support or refute the claim?\n\n## Table\nPaper title: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks\nTable caption: Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\n[BOLD] System | [BOLD] ROUGE-1  [BOLD] R (%) | [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%) | [BOLD] ROUGE-2  [BOLD] R (%) | [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%) | [BOLD] Sentence-Level  [BOLD] R (%) | [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)\n[BOLD] ILP | 24.5 | 41.1 | 29.3\u00b10.5 | 7.9 | 15.0 | 9.9\u00b10.5 | 13.6 | 22.6 | 15.6\u00b10.4\n[BOLD] Sum-Basic | 28.4 | 44.4 | 33.1\u00b10.5 | 8.5 | 15.6 | 10.4\u00b10.4 | 14.7 | 22.9 | 16.7\u00b10.5\n[BOLD] KL-Sum | 39.5 | 34.6 | 35.5\u00b10.5 | 13.0 | 12.7 | 12.3\u00b10.5 | 15.2 | 21.1 | 16.3\u00b10.5\n[BOLD] LexRank | 42.1 | 39.5 | 38.7\u00b10.5 | 14.7 | 15.3 | 14.2\u00b10.5 | 14.3 | 21.5 | 16.0\u00b10.5\n[BOLD] MEAD | 45.5 | 36.5 | 38.5\u00b1 0.5 | 17.9 | 14.9 | 15.4\u00b10.5 | 27.8 | 29.2 | 26.8\u00b10.5\n[BOLD] SVM | 19.0 | 48.8 | 24.7\u00b10.8 | 7.5 | 21.1 | 10.0\u00b10.5 | 32.7 | 34.3 | 31.4\u00b10.4\n[BOLD] LogReg | 26.9 | 34.5 | 28.7\u00b10.6 | 6.4 | 9.9 | 7.3\u00b10.4 | 12.2 | 14.9 | 12.7\u00b10.5\n[BOLD] LogReg [ITALIC] r | 28.0 | 34.8 | 29.4\u00b10.6 | 6.9 | 10.4 | 7.8\u00b10.4 | 12.1 | 14.5 | 12.5\u00b10.5\n[BOLD] HAN | 31.0 | 42.8 | 33.7\u00b10.7 | 11.2 | 17.8 | 12.7\u00b10.5 | 26.9 | 34.1 | 32.4\u00b10.5\n[BOLD] HAN+pretrainT | 32.2 | 42.4 | 34.4\u00b10.7 | 11.5 | 17.5 | 12.9\u00b10.5 | 29.6 | 35.8 | 32.2\u00b10.5\n[BOLD] HAN+pretrainU | 32.1 | 42.1 | 33.8\u00b10.7 | 11.6 | 17.6 | 12.9\u00b10.5 | 30.1 | 35.6 | 32.3\u00b10.5\n[BOLD] HAN [ITALIC] r | 38.1 | 40.5 | [BOLD] 37.8\u00b10.5 | 14.0 | 17.1 | [BOLD] 14.7\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainT [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.5 | 16.8 | [BOLD] 14.4\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainU [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.6 | 16.9 | [BOLD] 14.4\u00b10.5 | 33.9 | 33.8 | [BOLD] 33.8\u00b10.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c707131e-e361-4e4d-9fcd-a3bbceb7b3cd",
    "input": "## Claim\nHere is a claim: ( 2018 ) who \\detrained\" their model for RL, our NeuralTD is simpler without any \\reinforcement\\\u2019\\ training. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c2ee507c-4839-41e0-890a-5a08087869eb",
    "input": "## Claim\nHere is a claim: there are slight but consistent decreases when comparing to the metric trained using all 2,011 content words Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "a0fd51e0-1ceb-4ccc-a1cd-681d25090c32",
    "input": "## Claim\nHere is a claim: on the other hand, we are still noticeably outperformed by Refresh when directly comparing the length of the two summaries. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "a4aa23f5-33df-4432-aa1a-31d016705823",
    "input": "## Claim\nHere is a claim: Syntactic part-ofspeech features alone obtain higher performance than any sentiment or complaint feature group, showing the syntactic patterns discussed in the previous section hold high predictive accuracy for the task. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.\n[BOLD] Model | [BOLD] Acc | [BOLD] F1 | [BOLD] AUC\nMost Frequent Class | 64.2 | 39.1 | 0.500\nLogistic Regression | [EMPTY] | [EMPTY] | [EMPTY]\nSentiment \u2013 MPQA | 64.2 | 39.1 | 0.499\nSentiment \u2013 NRC | 63.9 | 42.2 | 0.599\nSentiment \u2013 V&B | 68.9 | 60.0 | 0.696\nSentiment \u2013 VADER | 66.0 | 54.2 | 0.654\nSentiment \u2013 Stanford | 68.0 | 55.6 | 0.696\nComplaint Specific (all) | 65.7 | 55.2 | 0.634\nRequest | 64.2 | 39.1 | 0.583\nIntensifiers | 64.5 | 47.3 | 0.639\nDowngraders | 65.4 | 49.8 | 0.615\nTemporal References | 64.2 | 43.7 | 0.535\nPronoun Types | 64.1 | 39.1 | 0.545\nPOS Bigrams | 72.2 | 66.8 | 0.756\nLIWC | 71.6 | 65.8 | 0.784\nWord2Vec Clusters | 67.7 | 58.3 | 0.738\nBag-of-Words | 79.8 | 77.5 | 0.866\nAll Features | [BOLD] 80.5 | [BOLD] 78.0 | [BOLD] 0.873\nNeural Networks | [EMPTY] | [EMPTY] | [EMPTY]\nMLP | 78.3 | 76.2 | 0.845\nLSTM | 80.2 | 77.0 | 0.864\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "83f16698-7b03-47a0-8716-3ab3a76eb741",
    "input": "## Claim\nHere is a claim: The proposed CNN-LSTMOur-neg-Ant improves upon the simple CNNLSTM-w/o neg. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 8: Sentiment classification evaluation, using different classifiers on the test set.\nClassifier | Positive Sentiment Precision | Positive Sentiment Recall | Positive Sentiment Fscore\nSVM-w/o neg. | 0.57 | 0.72 | 0.64\nSVM-Punct. neg. | 0.58 | 0.70 | 0.63\nSVM-our-neg. | 0.58 | 0.73 | 0.65\nCNN | 0.63 | 0.83 | 0.72\nCNN-LSTM | 0.71 | 0.72 | 0.72\nCNN-LSTM-Our-neg-Ant | [BOLD] 0.78 | [BOLD] 0.77 | [BOLD] 0.78\n[EMPTY] | Negative Sentiment | Negative Sentiment | Negative Sentiment\n[EMPTY] | Precision | Recall | Fscore\nSVM-w/o neg. | 0.78 | 0.86 | 0.82\nSVM-Punct. neg. | 0.78 | 0.87 | 0.83\nSVM-Our neg. | 0.80 | 0.87 | 0.83\nCNN | 0.88 | 0.72 | 0.79\nCNN-LSTM. | 0.83 | 0.83 | 0.83\nCNN-LSTM-our-neg-Ant | [BOLD] 0.87 | [BOLD] 0.87 | [BOLD] 0.87\n[EMPTY] | Train | [EMPTY] | Test\nPositive tweets | 5121 | [EMPTY] | 1320\nNegative tweets | 9094 | [EMPTY] | 2244\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ede7cf00-7c18-4d7a-86d8-b8d28c9d9ac9",
    "input": "## Claim\nHere is a claim: for example, for the system B in Table 2, the input systems are made available to the evaluation system, and this gives [BOLD] MUC-B1 (\u201cE2\u201d, default) a 4.6 precision point advantage over standard MUC-B1 (\u201cE1\u201d, see Table 2). Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n[BOLD] Model | R | MUC P | [ITALIC] F1 | R | B3 P | [ITALIC] F1 | R | CEAF- [ITALIC] e P | [ITALIC] F1 | CoNLL  [ITALIC] F1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | [BOLD] 71.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "d27c412f-e946-4c35-88fd-6ce5ec25bca4",
    "input": "## Claim\nHere is a claim: However, while we notice a definite improvement over Peyrard and Gurevych (2018), our results still lack behind the golden-set correlation, suggesting that future work could further improve the capacity of learning summary-level correlation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ef59728f-b28b-4481-b3b7-3aac7d96ac9d",
    "input": "## Claim\nHere is a claim: In LDC2017T10, G2S-GGNN achieves a BLEU score of 27.87, which is lower than Guo et al. (2019), a state-of-the-art model that does not employ external information. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4c4e4ede-1cec-4827-b374-11f06872dac4",
    "input": "## Claim\nHere is a claim: [CONTINUE] As a result, our implementation can train input data of balanced trees with greater throughput than input data of unbalanced trees. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2\nTable caption: Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.\nBatch size | Throughput (instances/s) Balanced | Throughput (instances/s) Moderate | Throughput (instances/s) Linear\n1 | 46.7 | 27.3 | 7.6\n10 | 125.2 | 78.2 | 22.7\n25 | 129.7 | 83.1 | 45.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0206cb33-81d5-41f7-8a9b-65f54e67f75e",
    "input": "## Claim\nHere is a claim: However, BERT's improvements over previous work can be almost entirely attributed to high accuracy on the Easy subset: on this subset, finetuned BERT-large improves 8.6 percent over the model by (Sasaki et al., 2017) (83.9% vs. 75.3%), but on the Hard subset, the improvement is actually 4.8 percent (71.9% vs. 69.0%). Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e75ba52a-52ff-4969-ae13-fb4cc981a78a",
    "input": "## Claim\nHere is a claim: covering the rare words can boost the performances across different out-of-domain (OOD) datasets significantly Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\n[EMPTY] | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK\nMQAN | 72.30 | 60.91 | 41.82 | 53.95\n+ coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold>\nESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37\n+ coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "a24c8c0a-c398-4600-8503-17bec62989ed",
    "input": "## Claim\nHere is a claim: The system performs well on synthetic dataset with a minimum of 80% P@1 and 98% P@10. Does the following context support or refute the claim?\n\n## Table\nPaper title: A context sensitive real-time Spell Checker with language adaptability\nTable caption: TABLE II: Synthetic Data Performance results\n[BOLD] Language | [BOLD] # Test | [BOLD] P@1 | [BOLD] P@3 | [BOLD] P@5 | [BOLD] P@10 | [BOLD] MRR\n[BOLD] Language | [BOLD] Samples | [BOLD] P@1 | [BOLD] P@3 | [BOLD] P@5 | [BOLD] P@10 | [BOLD] MRR\nBengali | 140000 | 91.30 | 97.83 | 98.94 | 99.65 | 94.68\nCzech | 94205 | 95.84 | 98.72 | 99.26 | 99.62 | 97.37\nDanish | 140000 | 85.84 | 95.19 | 97.28 | 98.83 | 90.85\nDutch | 140000 | 86.83 | 95.01 | 97.04 | 98.68 | 91.32\nEnglish | 140000 | 97.08 | 99.39 | 99.67 | 99.86 | 98.27\nFinnish | 140000 | 97.77 | 99.58 | 99.79 | 99.90 | 98.69\nFrench | 140000 | 86.52 | 95.66 | 97.52 | 98.83 | 91.38\nGerman | 140000 | 87.58 | 96.16 | 97.86 | 99.05 | 92.10\nGreek | 30022 | 84.95 | 94.99 | 96.88 | 98.44 | 90.27\nHebrew | 132596 | 94.00 | 98.26 | 99.05 | 99.62 | 96.24\nHindi | 140000 | 82.19 | 93.71 | 96.28 | 98.30 | 88.40\nIndonesian | 140000 | 95.01 | 98.98 | 99.50 | 99.84 | 97.04\nItalian | 140000 | 89.93 | 97.31 | 98.54 | 99.38 | 93.76\nMarathi | 140000 | 93.01 | 98.16 | 99.06 | 99.66 | 95.69\nPolish | 140000 | 95.65 | 99.17 | 99.62 | 99.86 | 97.44\nPortuguese | 140000 | 86.73 | 96.29 | 97.94 | 99.10 | 91.74\nRomanian | 140000 | 95.52 | 98.79 | 99.32 | 99.68 | 97.22\nRussian | 140000 | 94.85 | 98.74 | 99.33 | 99.71 | 96.86\nSpanish | 140000 | 85.91 | 95.35 | 97.18 | 98.57 | 90.92\nSwedish | 140000 | 88.86 | 96.40 | 98.00 | 99.14 | 92.87\nTamil | 140000 | 98.05 | 99.70 | 99.88 | 99.98 | 98.88\nTelugu | 140000 | 97.11 | 99.68 | 99.92 | 99.99 | 98.38\nThai | 12403 | 98.73 | 99.71 | 99.78 | 99.85 | 99.22\nTurkish | 140000 | 97.13 | 99.51 | 99.78 | 99.92 | 98.33\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "aded4603-ff26-4698-8185-8b0236c4f926",
    "input": "## Claim\nHere is a claim: G2S-GGNN does not outperform others with the same amount of Gigaword sentences (200K), as shown in Table 3, with a BLEU score of 32.23. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 3: Results on LDC2015E86 test set when models are trained with additional Gigaword data.\n<bold>Model</bold> | <bold>External</bold> | <bold>BLEU</bold>\nKonstas et al. (2017) | 200K | 27.40\nSong et al. (2018) | 200K | 28.20\nGuo et al. (2019) | 200K | 31.60\nG2S-GGNN | 200K | <bold>32.23</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2f40f1d6-68bd-422d-8d22-55e0bef9a42c",
    "input": "## Claim\nHere is a claim: TF and DF achieved different values of precision, recall and f-measure using the English corpora, with TF achieving a higher precision (P=0.0150) and f-measure (F=0.0293) than DF when using the Europarl corpus in English. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150 | 0.0150 | 0.0445 | 0.0326\nP | EN | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.0060 | 0.0092 | 0.0090 | 0.0356 | 0.0162\nP | PT | Europarl | 0.5710 | 0.1948 | 0.3855 | 0.5474 | 0.4485 | [BOLD] 0.8052 | 0.4058\n[EMPTY] | PT | Ted Talks | [BOLD] 0.6304 | 0.1870 | 0.3250 | 0.5312 | 0.4576 | 0.6064 | 0.3698\nR | EN | Europarl | 0.0037 | 0.3278 | 0.5941 | 0.6486 | [BOLD] 0.6490 | 0.0017 | 0.0003\nR | EN | Ted Talks | 0.0002 | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332 | 0.0967 | 0.0003\nR | PT | Europarl | 0.0002 | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932 | 0.0032 | 0.0001\n[EMPTY] | PT | Ted Talks | 2.10-5 | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887 | 0.1390 | 0.0002\nF | EN | Europarl | 0.0073 | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033 | 0.0006\nF | EN | Ted Talks | 0.0004 | 0.0132 | 0.0118 | 0.0181 | 0.0179 | [BOLD] 0.0520 | 0.0005\nF | PT | Europarl | 0.0005 | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109 | 0.0064 | 0.0002\n[EMPTY] | PT | Ted Talks | 4.10-5 | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149 | 0.2261 | 0.0004\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e0ea0f70-dd7a-44e6-af9c-6da48ffb8b11",
    "input": "## Claim\nHere is a claim: [CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 3: Literature results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\n[EMPTY] | Acc | Sim | PP | GM\nM0: shen-1 | 0.694 | 0.728 | [BOLD] 22.3 | 8.81\nM1: M0 [ITALIC] +para | 0.702 | 0.747 | 23.6 | 11.7\nM2: M0 [ITALIC] +cyc | 0.692 | 0.781 | 49.9 | [BOLD] 12.8\nM3: M0 [ITALIC] +cyc+lang | 0.698 | 0.754 | 39.2 | 12.0\nM4: M0 [ITALIC] +cyc+para | 0.702 | 0.757 | 33.9 | [BOLD] 12.8\nM5: M0 [ITALIC] +cyc+para+lang | 0.688 | 0.753 | 28.6 | 11.8\nM6: M0 [ITALIC] +cyc+2d | 0.704 | [BOLD] 0.794 | 63.2 | [BOLD] 12.8\nM7: M6+ [ITALIC] para+lang | 0.706 | 0.768 | 49.0 | [BOLD] 12.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "81166961-1a5a-4aec-8f8b-554e7059779c",
    "input": "## Claim\nHere is a claim: [CONTINUE] ACER and PPO obtain high performance in inform F1 and match rate as well. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3480e832-0450-4214-b055-4167d9e2ac36",
    "input": "## Claim\nHere is a claim: these results show that the questionnaire takers had an average accuracy of 98.2% in answering word intrusion questions for words associated with meanings imparted by standard word embeddings, Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "b7aee9bb-1d0f-4611-9d2c-3e1f6f0b870f",
    "input": "## Claim\nHere is a claim: [CONTINUE] Using a greater BiLSTM hidden size did not help the model, [CONTINUE] We found that using 25-dimensional part-ofspeech embeddings slightly improved results, [CONTINUE] Regarding optimization strategies, we also tried using SGD with different learning rates and a stepwise learning rate schedule as described by Conneau et al. Does the following context support or refute the claim?\n\n## Table\nPaper title: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations\nTable caption: Table 2: Ablation study results.\n[BOLD] Variation | [BOLD] Accuracy (%) | [BOLD] \u0394%\nSubmitted | [BOLD] 69.23 | -\nNo emoji | 68.36 | - 0.87\nNo ELMo | 65.52 | - 3.71\nConcat Pooling | 68.47 | - 0.76\nLSTM hidden=4096 | 69.10 | - 0.13\nLSTM hidden=1024 | 68.93 | - 0.30\nLSTM hidden=512 | 68.43 | - 0.80\nPOS emb dim=100 | 68.99 | - 0.24\nPOS emb dim=75 | 68.61 | - 0.62\nPOS emb dim=50 | 69.33 | + 0.10\nPOS emb dim=25 | 69.21 | - 0.02\nSGD optim lr=1 | 64.33 | - 4.90\nSGD optim lr=0.1 | 66.11 | - 3.12\nSGD optim lr=0.01 | 60.72 | - 8.51\nSGD optim lr=0.001 | 30.49 | - 38.74\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "154f0d35-a7d3-4bc4-94c3-9f8a040f2fe9",
    "input": "## Claim\nHere is a claim: [CONTINUE] The improvement from automatic AMR to gold AMR (+0.7 BLEU) is significant, which shows that the translation quality of our model can be further improved with an increase of AMR parsing accuracy. Does the following context support or refute the claim?\n\n## Table\nPaper title: Semantic Neural Machine Translation using AMR\nTable caption: Table 4: BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.\nAMR Anno. | BLEU\nAutomatic | 16.8\nGold | [BOLD] *17.5*\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "71651470-7e38-4147-9c5f-c80b5110cfb8",
    "input": "## Claim\nHere is a claim: This is expected because SVM is a linear classifier that relies solely on the 1-gram, while neural classifiers like CNNs (row3) and LSTMs (row4) cannot learn non-linear function of n-grams when the n is larger than the number of word vector dimensions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 8: Sentiment classification evaluation, using different classifiers on the test set.\nClassifier | Positive Sentiment Precision | Positive Sentiment Recall | Positive Sentiment Fscore\nSVM-w/o neg. | 0.57 | 0.72 | 0.64\nSVM-Punct. neg. | 0.58 | 0.70 | 0.63\nSVM-our-neg. | 0.58 | 0.73 | 0.65\nCNN | 0.63 | 0.83 | 0.72\nCNN-LSTM | 0.71 | 0.72 | 0.72\nCNN-LSTM-Our-neg-Ant | [BOLD] 0.78 | [BOLD] 0.77 | [BOLD] 0.78\n[EMPTY] | Negative Sentiment | Negative Sentiment | Negative Sentiment\n[EMPTY] | Precision | Recall | Fscore\nSVM-w/o neg. | 0.78 | 0.86 | 0.82\nSVM-Punct. neg. | 0.78 | 0.87 | 0.83\nSVM-Our neg. | 0.80 | 0.87 | 0.83\nCNN | 0.88 | 0.72 | 0.79\nCNN-LSTM. | 0.83 | 0.83 | 0.83\nCNN-LSTM-our-neg-Ant | [BOLD] 0.87 | [BOLD] 0.87 | [BOLD] 0.87\n[EMPTY] | Train | [EMPTY] | Test\nPositive tweets | 5121 | [EMPTY] | 1320\nNegative tweets | 9094 | [EMPTY] | 2244\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "22dea611-7444-4493-8117-cc4ac8e97dad",
    "input": "## Claim\nHere is a claim: for example, if the evaluation begins by calculating all analogies using the relation \u2018capital-common-countries\u2019 then in analogy 1, there will be [15_1 + 15_2 + 6_2]/50 = 6.3 out of 50 answers found by 1\u20141, i.e., 12.6% of 50 is counted as the score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "bd51e05d-94ea-4aaf-8572-7fff74309537",
    "input": "## Claim\nHere is a claim: This empirically shows that compared to recurrent graph encoders, DCGCNs can learn better representations for graphs. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.\n[BOLD] Model | [BOLD] T | #P | B | C\nSeq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1\nGGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4\nSeq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5\nGGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5\nDCGCN (ours) | S | [BOLD] 19.1M | 27.9 | 57.3\nDCGCN (ours) | E | 92.5M | [BOLD] 30.4 | [BOLD] 59.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "306fb06d-a734-46af-9d03-b1fd6bdb10a2",
    "input": "## Claim\nHere is a claim: The performance of each approach that interacts with the agenda-based user simulator is shown in [CONTINUE] Table 3. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d48f4021-6614-4e16-96c9-f4fad3f39106",
    "input": "## Claim\nHere is a claim: [CONTINUE] Opinion distance methods generally outperform the competition on both ARI and Silhouette coefficient. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.\nTopic Name | Size | TF-IDF ARI | WMD ARI | Sent2vec ARI | Doc2vec ARI | BERT ARI | [ITALIC] OD-w2v ARI | [ITALIC] OD-d2v ARI | TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil. | Sent2vec  [ITALIC] Sil. | Doc2vec  [ITALIC] Sil. | BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil. | [ITALIC] OD-d2v  [ITALIC] Sil.\nAffirmative Action | 81 | -0.07 | -0.02 | 0.03 | -0.01 | -0.02 | [BOLD] 0.14 | [ITALIC] 0.02 | 0.01 | 0.01 | -0.01 | -0.02 | -0.04 | [BOLD] 0.06 | [ITALIC] 0.01\nAtheism | 116 | [BOLD] 0.19 | 0.07 | 0.00 | 0.03 | -0.01 | 0.11 | [ITALIC] 0.16 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | [ITALIC] 0.05 | [BOLD] 0.07\nAusterity Measures | 20 | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01 | -0.05 | 0.04 | [BOLD] 0.21 | -0.01 | 0.06 | 0.07 | 0.05 | -0.03 | 0.10 | [BOLD] 0.19 | 0.1\nDemocratization | 76 | 0.02 | -0.01 | 0.00 | [ITALIC] 0.09 | -0.01 | [BOLD] 0.11 | 0.07 | 0.01 | 0.01 | 0.02 | 0.02 | 0.03 | [BOLD] 0.16 | [ITALIC] 0.11\nEducation Voucher Scheme | 30 | [BOLD] 0.25 | 0.12 | 0.08 | -0.02 | 0.04 | 0.13 | [ITALIC] 0.19 | 0.01 | 0.01 | 0.01 | -0.01 | 0.02 | [ITALIC] 0.38 | [BOLD] 0.40\nGambling | 60 | -0.06 | -0.01 | -0.02 | 0.04 | 0.09 | [ITALIC] 0.35 | [BOLD] 0.39 | 0.01 | 0.02 | 0.03 | 0.01 | 0.09 | [BOLD] 0.30 | [ITALIC] 0.22\nHousing | 30 | 0.01 | -0.01 | -0.01 | -0.02 | 0.08 | [BOLD] 0.27 | 0.01 | 0.02 | 0.03 | 0.03 | 0.01 | 0.11 | [BOLD] 0.13 | [ITALIC] 0.13\nHydroelectric Dams | 110 | [BOLD] 0.47 | [ITALIC] 0.45 | [ITALIC] 0.45 | -0.01 | 0.38 | 0.35 | 0.14 | 0.04 | 0.08 | 0.12 | 0.01 | 0.19 | [BOLD] 0.26 | [ITALIC] 0.09\nIntellectual Property | 66 | 0.01 | 0.01 | 0.00 | 0.03 | 0.03 | [ITALIC] 0.05 | [BOLD] 0.14 | 0.01 | [ITALIC] 0.04 | 0.03 | 0.01 | 0.03 | [ITALIC] 0.04 | [BOLD] 0.12\nKeystone pipeline | 18 | 0.01 | 0.01 | 0.00 | -0.13 | [BOLD] 0.07 | -0.01 | [BOLD] 0.07 | -0.01 | -0.03 | -0.03 | -0.07 | 0.03 | [BOLD] 0.05 | [ITALIC] 0.02\nMonarchy | 61 | -0.04 | 0.01 | 0.00 | 0.03 | -0.02 | [BOLD] 0.15 | [BOLD] 0.15 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 | [BOLD] 0.11 | [ITALIC] 0.09\nNational Service | 33 | 0.14 | -0.03 | -0.01 | 0.02 | 0.01 | [ITALIC] 0.31 | [BOLD] 0.39 | 0.02 | 0.04 | 0.02 | 0.01 | 0.02 | [BOLD] 0.25 | [BOLD] 0.25\nOne-child policy China | 67 | -0.05 | 0.01 | [BOLD] 0.11 | -0.02 | 0.02 | [BOLD] 0.11 | 0.01 | 0.01 | 0.02 | [ITALIC] 0.04 | -0.01 | 0.03 | [BOLD] 0.07 | -0.02\nOpen-source Software | 48 | -0.02 | -0.01 | [ITALIC] 0.05 | 0.01 | 0.12 | [BOLD] 0.09 | -0.02 | 0.01 | -0.01 | 0.00 | -0.02 | 0.03 | [BOLD] 0.18 | 0.01\nPornography | 52 | -0.02 | 0.01 | 0.01 | -0.02 | -0.01 | [BOLD] 0.41 | [BOLD] 0.41 | 0.01 | 0.01 | 0.02 | -0.01 | 0.03 | [BOLD] 0.47 | [ITALIC] 0.41\nSeanad Abolition | 25 | 0.23 | 0.09 | -0.01 | -0.01 | 0.03 | [ITALIC] 0.32 | [BOLD] 0.54 | 0.02 | 0.01 | -0.01 | -0.03 | -0.04 | [ITALIC] 0.15 | [BOLD] 0.31\nTrades Unions | 19 | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60 | -0.05 | 0.44 | [ITALIC] 0.44 | 0.29 | 0.1 | 0.17 | 0.21 | 0.01 | 0.26 | [BOLD] 0.48 | [ITALIC] 0.32\nVideo Games | 72 | -0.01 | 0.01 | 0.12 | 0.01 | 0.08 | [ITALIC] 0.40 | [BOLD] 0.56 | 0.01 | 0.01 | 0.06 | 0.01 | 0.05 | [ITALIC] 0.32 | [BOLD] 0.42\nAverage | 54.67 | 0.09 | 0.07 | 0.08 | 0.01 | 0.08 | [BOLD] 0.22 | [ITALIC] 0.20 | 0.02 | 0.03 | 0.04 | -0.01 | 0.05 | [BOLD] 0.20 | [ITALIC] 0.17\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "59840d7f-df7c-4eb3-8f66-0f78d22aac52",
    "input": "## Claim\nHere is a claim: The complete model has significantly more parameters than the model without graph encoders (57.6M vs 61.7M). Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 4: Results of the ablation study on the LDC2017T10 development set.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold> | <bold>Size</bold>\nbiLSTM | 22.50 | 30.42 | 57.6M\n<italic>GEt</italic> + biLSTM | 26.33 | 32.62 | 59.6M\n<italic>GEb</italic> + biLSTM | 26.12 | 32.49 | 59.6M\n<italic>GEt</italic> + <italic>GEb</italic> + biLSTM | 27.37 | 33.30 | 61.7M\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9cb4abcc-8929-4b33-9ff7-358228567dd5",
    "input": "## Claim\nHere is a claim: Unlike the above three models, Word2Sense does not use pretrained vectors. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VI: Correlations for Word Similarity Tests\nDataset (EN-) | GloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\nWS-353-ALL | 0.612 | 0.7156 | 0.634 | 0.622 | 0.173 | 0.690 | 0.657\nSIMLEX-999 | 0.359 | 0.3939 | 0.295 | 0.355 | 0.090 | 0.380 | 0.381\nVERB-143 | 0.326 | 0.4430 | 0.255 | 0.271 | 0.293 | 0.271 | 0.348\nSimVerb-3500 | 0.193 | 0.2856 | 0.184 | 0.197 | 0.035 | 0.234 | 0.245\nWS-353-REL | 0.578 | 0.6457 | 0.595 | 0.578 | 0.134 | 0.695 | 0.619\nRW-STANF. | 0.378 | 0.4858 | 0.316 | 0.373 | 0.122 | 0.390 | 0.382\nYP-130 | 0.524 | 0.5211 | 0.353 | 0.482 | 0.169 | 0.420 | 0.589\nMEN-TR-3k | 0.710 | 0.7528 | 0.684 | 0.696 | 0.298 | 0.769 | 0.725\nRG-65 | 0.768 | 0.8051 | 0.736 | 0.732 | 0.338 | 0.761 | 0.774\nMTurk-771 | 0.650 | 0.6712 | 0.593 | 0.623 | 0.199 | 0.665 | 0.671\nWS-353-SIM | 0.682 | 0.7883 | 0.713 | 0.702 | 0.220 | 0.720 | 0.720\nMC-30 | 0.749 | 0.8112 | 0.799 | 0.726 | 0.330 | 0.735 | 0.776\nMTurk-287 | 0.649 | 0.6645 | 0.591 | 0.631 | 0.295 | 0.674 | 0.634\nAverage | 0.552 | 0.6141 | 0.519 | 0.538 | 0.207 | 0.570 | 0.579\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "46bed304-5dec-4a0f-8327-05dcd43ca54c",
    "input": "## Claim\nHere is a claim: However, coverage can compensate for much of the lost performance in each case. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\n[EMPTY] | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK\nMQAN | 72.30 | 60.91 | 41.82 | 53.95\n+ coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold>\nESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37\n+ coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e8525ca4-669e-4b02-829f-c5aeedc8e85f",
    "input": "## Claim\nHere is a claim: We found that rephrase disfluencies that contain content words are easier for the model to detect, compared to rephrases with function words only, and error decreases for longer disfluencies. Does the following context support or refute the claim?\n\n## Table\nPaper title: Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection\nTable caption: Table 3: Relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair (content-content), either the reparandum or repair (content-function) or in neither. Percentages in parentheses show the fraction of tokens belong to each category.\n[BOLD] Type | [BOLD] Reparandum Length  [BOLD] 1-2 | [BOLD] Reparandum Length  [BOLD] 3-5\ncontent-content | 0.61 (30%) | 0.58 (52%)\ncontent-function | 0.77 (20%) | 0.66 (17%)\nfunction-function | 0.83 (50%) | 0.80 (32%)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3010d663-a981-41d4-8f6c-555026bb0257",
    "input": "## Claim\nHere is a claim: We additionally find that supervised BLEU does not show a trade-off with Acc: for a single model type, higher Acc does not necessarily correspond to lower BLEU. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 6: Results on Yelp sentiment transfer, where BLEU is between 1000 transferred sentences and human references, and Acc is restricted to the same 1000 sentences. Our best models (right table) achieve higher BLEU than prior work at similar levels of Acc, but untransferred sentences achieve the highest BLEU. Acc\u2217: the definition of Acc varies by row because of different classifiers in use. Other results from simple-transfer are not included as they are worse.\nModel | BLEU | Acc\u2217\nfu-1 | [EMPTY] | [EMPTY]\nMulti-decoder | 7.6 | 0.792\nStyle embed. | 15.4 | 0.095\nsimple-transfer | simple-transfer | simple-transfer\nTemplate | 18.0 | 0.867\nDelete/Retrieve | 12.6 | 0.909\nyang2018unsupervised | yang2018unsupervised | yang2018unsupervised\nLM | 13.4 | 0.854\nLM + classifier | [BOLD] 22.3 | 0.900\nUntransferred | [BOLD] 31.4 | 0.024\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e7d70231-bd8d-4422-be22-05085a4427f0",
    "input": "## Claim\nHere is a claim: When humans are asked to choose an option which they believe is more likely to be a correct causal conclusion, 80% select the correct label. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See \u00a72 for model details. * indicates our replication experiments.\nModel | Accuracy\nBigramPMI\u00a0Goodwin et al. ( 2012 ) | 63.4\nPMI\u00a0Gordon et al. ( 2011 ) | 65.4\nPMI+Connectives\u00a0Luo et al. ( 2016 ) | 70.2\nPMI+Con.+Phrase\u00a0Sasaki et al. ( 2017 ) | 71.4\nBERT-large\u00a0Wang et al. ( 2019 ) | 70.5\nBERT-large\u00a0Sap et al. ( 2019 ) | 75.0\nBERT-large\u00a0Li et al. ( 2019 ) | 75.4\nRoBERTa-large (finetuned) | 90.6\nBERT-large (finetuned)* | 76.5 \u00b1 2.7\nRoBERTa-large (finetuned)* | 87.7 \u00b1 0.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8bc5a9aa-47c8-40c0-917e-6659a847af46",
    "input": "## Claim\nHere is a claim: In other words, [CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Original | TGen\u2212 | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94\nOriginal | [BOLD] Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27\nOriginal | [BOLD] Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80\nOriginal | [BOLD] Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen\u2212 | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37\nCleaned missing | [BOLD] Original | TGen\u2212 | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61\nCleaned missing | [BOLD] Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53\nCleaned missing | [BOLD] Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen\u2212 | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c95f0f73-ac64-4722-b8af-676db2efd814",
    "input": "## Claim\nHere is a claim: We believe that this is because \u2018to\u2019 is either a correct or an incorrect cue: to achieve high consistency, an MLM will more likely answer when the cue is \u201cto\u201d, and answer the other alternative, otherwise. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nCue | App. | Prod. | Cov.\nin | 47 | 55.3 | 9.40\nwas | 55 | 61.8 | 11.0\nto | 82 | 40.2 | 16.4\nthe | 85 | 38.8 | 17.0\na | 106 | 57.5 | 21.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "b6bc24e5-272f-4803-863d-d4d8ace3af9d",
    "input": "## Claim\nHere is a claim: We observe that, let alone a reduction in performance, the obtained scores indicate an almost uniform improvement in the correlation values for the proposed algorithm, outperforming all the alternatives except Word2Vec baseline on average. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VI: Correlations for Word Similarity Tests\nDataset (EN-) | GloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\nWS-353-ALL | 0.612 | 0.7156 | 0.634 | 0.622 | 0.173 | 0.690 | 0.657\nSIMLEX-999 | 0.359 | 0.3939 | 0.295 | 0.355 | 0.090 | 0.380 | 0.381\nVERB-143 | 0.326 | 0.4430 | 0.255 | 0.271 | 0.293 | 0.271 | 0.348\nSimVerb-3500 | 0.193 | 0.2856 | 0.184 | 0.197 | 0.035 | 0.234 | 0.245\nWS-353-REL | 0.578 | 0.6457 | 0.595 | 0.578 | 0.134 | 0.695 | 0.619\nRW-STANF. | 0.378 | 0.4858 | 0.316 | 0.373 | 0.122 | 0.390 | 0.382\nYP-130 | 0.524 | 0.5211 | 0.353 | 0.482 | 0.169 | 0.420 | 0.589\nMEN-TR-3k | 0.710 | 0.7528 | 0.684 | 0.696 | 0.298 | 0.769 | 0.725\nRG-65 | 0.768 | 0.8051 | 0.736 | 0.732 | 0.338 | 0.761 | 0.774\nMTurk-771 | 0.650 | 0.6712 | 0.593 | 0.623 | 0.199 | 0.665 | 0.671\nWS-353-SIM | 0.682 | 0.7883 | 0.713 | 0.702 | 0.220 | 0.720 | 0.720\nMC-30 | 0.749 | 0.8112 | 0.799 | 0.726 | 0.330 | 0.735 | 0.776\nMTurk-287 | 0.649 | 0.6645 | 0.591 | 0.631 | 0.295 | 0.674 | 0.634\nAverage | 0.552 | 0.6141 | 0.519 | 0.538 | 0.207 | 0.570 | 0.579\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "e581bff7-189f-47c9-bc6e-ad38451ed188",
    "input": "## Claim\nHere is a claim: We observe an improvement in performance between PG-original and PG-MMR (which takes the pre-trained PG-original and applies MMR on top of the model). Does the following context support or refute the claim?\n\n## Table\nPaper title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\nTable caption: Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.\n[BOLD] Method | [BOLD] R-1 | [BOLD] R-2 | [BOLD] R-SU\nFirst-1 | 26.83 | 7.25 | 6.46\nFirst-2 | 35.99 | 10.17 | 12.06\nFirst-3 | 39.41 | 11.77 | 14.51\nLexRank Erkan and Radev ( 2004 ) | 38.27 | 12.70 | 13.20\nTextRank Mihalcea and Tarau ( 2004 ) | 38.44 | 13.10 | 13.50\nMMR Carbonell and Goldstein ( 1998 ) | 38.77 | 11.98 | 12.91\nPG-Original Lebanoff et\u00a0al. ( 2018 ) | 41.85 | 12.91 | 16.46\nPG-MMR Lebanoff et\u00a0al. ( 2018 ) | 40.55 | 12.36 | 15.87\nPG-BRNN Gehrmann et\u00a0al. ( 2018 ) | 42.80 | 14.19 | 16.75\nCopyTransformer Gehrmann et\u00a0al. ( 2018 ) | [BOLD] 43.57 | 14.03 | 17.37\nHi-MAP (Our Model) | 43.47 | [BOLD] 14.89 | [BOLD] 17.41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1a799142-09bf-4cb2-a6c1-1c85df1089a8",
    "input": "## Claim\nHere is a claim: Tweets in the black-aligned corpus are classified as containing sexism almost twice as frequently and 1.1 times as frequently classified as containing racism and sexism compared to those in the white-aligned corpus. Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 2: Experiment 1\nDataset | Class | \u02c6 [ITALIC] piblack | \u02c6 [ITALIC] piwhite | [ITALIC] t | [ITALIC] p | \u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite\n[ITALIC] Waseem and Hovy | Racism | 0.001 | 0.003 | -20.818 | *** | 0.505\n[EMPTY] | Sexism | 0.083 | 0.048 | 101.636 | *** | 1.724\n[ITALIC] Waseem | Racism | 0.001 | 0.001 | 0.035 | [EMPTY] | 1.001\n[EMPTY] | Sexism | 0.023 | 0.012 | 64.418 | *** | 1.993\n[EMPTY] | Racism and sexism | 0.002 | 0.001 | 4.047 | *** | 1.120\n[ITALIC] Davidson et al. | Hate | 0.049 | 0.019 | 120.986 | *** | 2.573\n[EMPTY] | Offensive | 0.173 | 0.065 | 243.285 | *** | 2.653\n[ITALIC] Golbeck et al. | Harassment | 0.032 | 0.023 | 39.483 | *** | 1.396\n[ITALIC] Founta et al. | Hate | 0.111 | 0.061 | 122.707 | *** | 1.812\n[EMPTY] | Abusive | 0.178 | 0.080 | 211.319 | *** | 2.239\n[EMPTY] | Spam | 0.028 | 0.015 | 63.131 | *** | 1.854\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "9df6dedc-23bf-468f-a0c5-7df7dd75c9e3",
    "input": "## Claim\nHere is a claim: In comparison, GDPL is still comparable with ACER and PPO, but does not obtain a better match rate, and even achieves lower task success. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a5addb02-2aad-498e-99b8-e2e4dd7cf0b0",
    "input": "## Claim\nHere is a claim: For example, GCN+RC+LA (10) achieves a BLEU score of 52.9, which is better than GCN+RC+LA (9). Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.\n[BOLD] GCN +RC (2) | B 16.8 | C 48.1 | [BOLD] GCN +RC+LA (2) | B 18.3 | C 47.9\n+RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1\n+RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8\n+RC (9) | [BOLD] 21.1 | 50.5 | +RC+LA (9) | [BOLD] 22.0 | 52.6\n+RC (10) | 20.7 | [BOLD] 50.7 | +RC+LA (10) | 21.2 | [BOLD] 52.9\nDCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7\nDCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "352ed084-1079-4116-b4ec-37b4d6ebe790",
    "input": "## Claim\nHere is a claim: All G2S models have lower entailment compared to S2S. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.\n<bold>Model</bold> | REF \u21d2 GEN <bold>ENT</bold> | REF \u21d2 GEN <bold>CON</bold> | REF \u21d2 GEN <bold>NEU</bold>\nS2S | 38.45 | 11.17 | 50.38\nG2S-GIN | 49.78 | 9.80 | 40.42\nG2S-GAT | 49.48 | 8.09 | 42.43\nG2S-GGNN | 51.32 | 8.82 | 39.86\n[EMPTY] | GEN \u21d2 REF | GEN \u21d2 REF | GEN \u21d2 REF\n<bold>Model</bold> | <bold>ENT</bold> | <bold>CON</bold> | <bold>NEU</bold>\nS2S | 73.79 | 12.75 | 13.46\nG2S-GIN | 76.27 | 10.65 | 13.08\nG2S-GAT | 77.54 | 8.54 | 13.92\nG2S-GGNN | 77.64 | 9.64 | 12.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "20f5e50c-4725-4274-bdff-c605884d6206",
    "input": "## Claim\nHere is a claim: When we add multi-factor attention to the baseline BiLSTM-CNN model without the dependency distance-based weight factor in the attention mechanism, we get 0.4% F1 score decrease (A2\u2212A1). Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.\n[EMPTY] | Prec. | Rec. | F1\n(A1) BiLSTM-CNN | 0.473 | 0.606 | 0.531\n(A2) Standard attention | 0.466 | 0.638 | 0.539\n(A3) Window size ( [ITALIC] ws)=5 | 0.507 | 0.652 | [BOLD] 0.571\n(A4) Window size ( [ITALIC] ws)=10 | 0.510 | 0.640 | 0.568\n(A5) Softmax | 0.490 | 0.658 | 0.562\n(A6) Max-pool | 0.492 | 0.600 | 0.541\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4a18f8e5-71c1-4cf1-8638-bed82a865841",
    "input": "## Claim\nHere is a claim: The difference between accuracy on Easy and Hard is more pronounced for RoBERTa, suggesting a reliance on superficial cues. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "f245ef3f-7ca3-4a2e-9b70-5f42ca63476c",
    "input": "## Claim\nHere is a claim: In general, we found when the parameter budget is the same, shallower DCGCN models can obtain better results than the deeper ones. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 7: Comparisons of different DCGCN models under almost the same parameter budget.\n[BOLD] Model | D | #P | B | C\nDCGCN(1) | 300 | 10.9M | 20.9 | 52.0\nDCGCN(2) | 180 | 10.9M | [BOLD] 22.2 | [BOLD] 52.3\nDCGCN(2) | 240 | 11.3M | 22.8 | 52.8\nDCGCN(4) | 180 | 11.4M | [BOLD] 23.4 | [BOLD] 53.4\nDCGCN(1) | 420 | 12.6M | 22.2 | 52.4\nDCGCN(2) | 300 | 12.5M | 23.8 | 53.8\nDCGCN(3) | 240 | 12.3M | [BOLD] 23.9 | [BOLD] 54.1\nDCGCN(2) | 360 | 14.0M | 24.2 | [BOLD] 54.4\nDCGCN(3) | 300 | 14.0M | [BOLD] 24.4 | 54.2\nDCGCN(2) | 420 | 15.6M | 24.1 | 53.7\nDCGCN(4) | 300 | 15.6M | [BOLD] 24.6 | [BOLD] 54.8\nDCGCN(3) | 420 | 18.6M | 24.5 | 54.6\nDCGCN(4) | 360 | 18.4M | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a5a4fd4a-41ee-417e-a619-d1398be5d04a",
    "input": "## Claim\nHere is a claim: Comparing the 784-dimensional models, again, CBOW and CMOW seem to complement each other. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nMethod | SUBJ | CR | MR | MPQA | MRPC | TREC | SICK-E | SST2 | SST5 | STS-B | SICK-R\nCBOW/784 | 90.0 | [BOLD] 79.2 | [BOLD] 74.0 | 87.1 | 71.6 | 85.6 | 78.9 | 78.5 | 42.1 | 61.0 | [BOLD] 78.1\nCMOW/784 | 87.5 | 73.4 | 70.6 | [BOLD] 87.3 | 69.6 | [BOLD] 88.0 | 77.2 | 74.7 | 37.9 | 56.5 | 76.2\nHybrid | [BOLD] 90.2 | 78.7 | 73.7 | [BOLD] 87.3 | [BOLD] 72.7 | 87.6 | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8\ncmp. CBOW | +0.2% | -0.6% | -0.4% | +0.2% | +1.5% | +2.3% | +0.6% | +1.4% | +2.9% | +3.9% | -0.4%\ncmp. CMOW | +3.1% | +7.2% | +4.4% | +0% | +4.5% | -0.5% | +2.9% | +6.7% | +14.3 | +12.2% | +2.1%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f721af34-eeee-4c8b-80f4-3e8ce9ac71a6",
    "input": "## Claim\nHere is a claim: However, we define that the \u201c119.99\u201d operator as: if RSI <= 119.99 then RSI meets the requirement. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ea7b772f-808f-4add-97cf-6a2b10dfe9a9",
    "input": "## Claim\nHere is a claim: The hybrid model does not yield scores close to or even above the better model of the two on all tasks. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "74e89463-6301-450d-97e1-7cdefad17983",
    "input": "## Claim\nHere is a claim: [CONTINUE] The performances of all models decrease as the diameters of the graphs increase. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c864e92d-ec01-45c1-aa22-91071f6e2241",
    "input": "## Claim\nHere is a claim: For BERT models, after fine-tuning on COPA, RoBERTa-large achieves the best performance on both Easy and Hard. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large | B-COPA | 70.5 (\u00b1 2.5) | 72.6 (\u00b1 2.3) | [BOLD] 69.1 (\u00b1 2.7)\nBERT-large | B-COPA (50%) | 69.9 (\u00b1 1.9) | 71.2 (\u00b1 1.3) | 69.0 (\u00b1 3.5)\nBERT-large | COPA | [BOLD] 71.7 (\u00b1 0.5) | [BOLD] 80.5 (\u00b1 0.4) | 66.3 (\u00b1 0.8)\nRoBERTa-large | B-COPA | [BOLD] 76.7 (\u00b1 0.8) | 73.3 (\u00b1 1.5) | [BOLD] 78.8 (\u00b1 2.0)\nRoBERTa-large | B-COPA (50%) | 72.4 (\u00b1 2.0) | 72.1 (\u00b1 1.7) | 72.6 (\u00b1 2.1)\nRoBERTa-large | COPA | 76.4 (\u00b1 0.7) | [BOLD] 79.6 (\u00b1 1.0) | 74.4 (\u00b1 1.1)\nBERT-base-NSP | None | [BOLD] 66.4 | 66.2 | [BOLD] 66.7\nBERT-large-NSP | None | 65.0 | [BOLD] 66.9 | 62.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c815f80d-4626-4775-bfff-d8b3630c274d",
    "input": "## Claim\nHere is a claim: [CONTINUE] Table 2 shows that the model with paraphrase loss (M1) slightly improves Sim over M0 on both datasets under similar Acc. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\n[EMPTY] | Acc | Sim | PP | GM\nM0: shen-1 | 0.818 | 0.719 | 37.3 | 10.0\nM1: M0 [ITALIC] +para | 0.819 | 0.734 | 26.3 | 14.2\nM2: M0 [ITALIC] +cyc | 0.813 | 0.770 | 36.4 | 18.8\nM3: M0 [ITALIC] +cyc+lang | 0.807 | 0.796 | 28.4 | 21.5\nM4: M0 [ITALIC] +cyc+para | 0.798 | 0.783 | 39.7 | 19.2\nM5: M0 [ITALIC] +cyc+para+lang | 0.804 | 0.785 | 27.1 | 20.3\nM6: M0 [ITALIC] +cyc+2d | 0.805 | [BOLD] 0.817 | 43.3 | 21.6\nM7: M6+ [ITALIC] para+lang | 0.818 | 0.805 | [BOLD] 29.0 | [BOLD] 22.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5081e11b-aed5-4a7e-9c0a-00cf1b73e630",
    "input": "## Claim\nHere is a claim: Interestingly, the error analysis on this dataset revealed that the BiLSTM model was unable to correctly classify one-word scopes. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8bea0857-41f9-4d7c-82b3-1bd70e74a1b4",
    "input": "## Claim\nHere is a claim: The models have worse results when handling sentences with 20 or fewer tokens. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "023aa496-aafd-4f83-aa81-5c76da94e3e0",
    "input": "## Claim\nHere is a claim: [CONTINUE] Due to joint training, our hybrid model learns to pick up the best features from CBOW and CMOW simultaneously. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "52d13569-4080-411e-a922-9afd23f2b1b1",
    "input": "## Claim\nHere is a claim: Despite filtering out multiple hypernyms, the recall values for the Portuguese corpora are still relatively high. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1038 | 0.0170 | 0.0490 | 0.0641 | 0.0641 | 0.0613 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1282 | 0.0291 | 0.0410 | 0.0270 | 0.0270 | 0.1154 | 0.0661\nP | PT | Europarl | 0.6185 | 0.3744 | 0.4144 | 0.4394 | 0.4394 | [BOLD] 0.7553 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.6308 | 0.4124 | 0.4404 | 0.4515 | 0.4945 | [BOLD] 0.8609 | 0.5295\nR | EN | Europarl | [BOLD] 0.0021 | 0.0004 | 0.0011 | 0.0014 | 0.0014 | 0.0013 | 0.0017\nR | EN | Ted Talks | 0.0011 | 0.0008 | 0.0011 | 0.0008 | 0.0008 | [BOLD] 0.0030 | 0.0018\nR | PT | Europarl | 0.0012 | 0.0008 | 0.0009 | 0.0010 | 0.0010 | [BOLD] 0.0016 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0003 | 0.0009 | 0.0009 | 0.0010 | 0.0010 | [BOLD] 0.0017 | 0.0011\nF | EN | Europarl | [BOLD] 0.0041 | 0.0007 | 0.0021 | 0.0027 | 0.0027 | 0.0026 | 0.0033\nF | EN | Ted Talks | 0.0022 | 0.0016 | 0.0022 | 0.0015 | 0.0015 | [BOLD] 0.0058 | 0.0036\nF | PT | Europarl | 0.0024 | 0.0016 | 0.0018 | 0.0019 | 0.0019 | [BOLD] 0.0031 | 0.0023\n[EMPTY] | PT | Ted Talks | 0.0005 | 0.0018 | 0.0018 | 0.0020 | 0.0021 | [BOLD] 0.0034 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "bc61ec91-f1d8-4416-9fdb-87ade3bbd1e8",
    "input": "## Claim\nHere is a claim: LRN obtains additional 4 percentage points gain with BERT and reaches an accuracy of around 89.9. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nModel | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time\nRockt\u00e4schel et\u00a0al. ( 2016 ) | Rockt\u00e4schel et\u00a0al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | -\nThis | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 | [BOLD] 90.49 | 0.696\nThis | GRU | 6.41M | [BOLD] 85.71 | 0.245 | [BOLD] 86.05 | 0.419 | [BOLD] 90.29 | 0.529 | 90.10 | 0.695\nThis | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580\nWork | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555\n[EMPTY] | LRN | 4.25M | 84.88 | [BOLD] 0.209 | 85.06 | [BOLD] 0.223 | 89.98 | [BOLD] 0.488 | 89.93 | [BOLD] 0.506\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b3f85fb2-384a-484e-9e52-d0bc2a0efac5",
    "input": "## Claim\nHere is a claim: [CONTINUE] However, words and clusters expressing positive states such as gratitude (thank, great, love) or laughter (lol) are not significantly associated with tweets that are not complaints. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\n[BOLD] Complaints  [BOLD] Feature | [BOLD] Complaints  [ITALIC] r | [BOLD] Not Complaints  [BOLD] Feature | [BOLD] Not Complaints  [ITALIC] r\n[BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams\nnot | .154 | [URL] | .150\nmy | .131 | ! | .082\nworking | .124 | he | .069\nstill | .123 | thank | .067\non | .119 | , | .064\ncan\u2019t | .113 | love | .064\nservice | .112 | lol | .061\ncustomer | .109 | you | .060\nwhy | .108 | great | .058\nwebsite | .107 | win | .058\nno | .104 | \u2019 | .058\n? | .098 | she | .054\nfix | .093 | : | .053\nwon\u2019t | .092 | that | .053\nbeen | .090 | more | .052\nissue | .089 | it | .052\ndays | .088 | would | .051\nerror | .087 | him | .047\nis | .084 | life | .046\ncharged | .083 | good | .046\n[BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)\nVBN | .141 | UH | .104\n$ | .118 | NNP | .098\nVBZ | .114 | PRP | .076\nNN_VBZ | .114 | HT | .076\nPRP$ | .107 | PRP_. | .076\nPRP$_NN | .105 | PRP_RB | .067\nVBG | .093 | NNP_NNP | .062\nCD | .092 | VBP_PRP | .054\nWRB_VBZ | .084 | JJ | .053\nVBZ_VBN | .084 | DT_JJ | .051\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1e1d2baf-44da-4531-9601-26027fdd859a",
    "input": "## Claim\nHere is a claim: RANDOM is indeed closer here to the expected 50% and other baselines are closer to gender-parity. Does the following context support or refute the claim?\n\n## Table\nPaper title: Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns\nTable caption: Table 7: Performance of our baselines on the development set in the gold-two-mention task (access to the two candidate name spans). Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.\n[EMPTY] | M | F | B | O\nRandom | 47.5 | 50.5 | [ITALIC] 1.06 | 49.0\nToken Distance | 50.6 | 47.5 | [ITALIC] 0.94 | 49.1\nTopical Entity | 50.2 | 47.3 | [ITALIC] 0.94 | 48.8\nSyntactic Distance | 66.7 | 66.7 | [ITALIC]  [BOLD] 1.00 | 66.7\nParallelism | [BOLD] 69.3 | [BOLD] 69.2 | [ITALIC]  [BOLD] 1.00 | [BOLD] 69.2\nParallelism+URL | [BOLD] 74.2 | [BOLD] 71.6 | [ITALIC]  [BOLD] 0.96 | [BOLD] 72.9\nTransformer-Single | 59.6 | 56.6 | [ITALIC] 0.95 | 58.1\nTransformer-Multi | 62.9 | 61.7 | [ITALIC] 0.98 | 62.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "18c4bae1-104d-4e43-bcbc-da4b289a9d62",
    "input": "## Claim\nHere is a claim: What we have found is that Google Translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, in general. Does the following context support or refute the claim?\n\n## Table\nPaper title: Assessing Gender Bias in Machine Translation \u2013 A Case Study with Google Translate\nTable caption: Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table\nCategory | Female (%) | Male (%) | Neutral (%)\nOffice and administrative support | 11.015 | 58.812 | 16.954\nArchitecture and engineering | 2.299 | 72.701 | 10.92\nFarming, fishing, and forestry | 12.179 | 62.179 | 14.744\nManagement | 11.232 | 66.667 | 12.681\nCommunity and social service | 20.238 | 62.5 | 10.119\nHealthcare support | 25.0 | 43.75 | 17.188\nSales and related | 8.929 | 62.202 | 16.964\nInstallation, maintenance, and repair | 5.22 | 58.333 | 17.125\nTransportation and material moving | 8.81 | 62.976 | 17.5\nLegal | 11.905 | 72.619 | 10.714\nBusiness and financial operations | 7.065 | 67.935 | 15.58\nLife, physical, and social science | 5.882 | 73.284 | 10.049\nArts, design, entertainment, sports, and media | 10.36 | 67.342 | 11.486\nEducation, training, and library | 23.485 | 53.03 | 9.091\nBuilding and grounds cleaning and maintenance | 12.5 | 68.333 | 11.667\nPersonal care and service | 18.939 | 49.747 | 18.434\nHealthcare practitioners and technical | 22.674 | 51.744 | 15.116\nProduction | 14.331 | 51.199 | 18.245\nComputer and mathematical | 4.167 | 66.146 | 14.062\nConstruction and extraction | 8.578 | 61.887 | 17.525\nProtective service | 8.631 | 65.179 | 12.5\nFood preparation and serving related | 21.078 | 58.333 | 17.647\nTotal | 11.76 | 58.93 | 15.939\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d2a8a2b1-1542-4216-b507-9eb0813d114b",
    "input": "## Claim\nHere is a claim: PCS can detect 4,113 new scope relations, 833 fewer than with gold cues. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "edeea560-96b7-4321-b28d-aa8e670d56ca",
    "input": "## Claim\nHere is a claim: We observe that the transfer baseline that directly uses rationale as augmented supervision (RA-TRANS) underperforms ORACLE by a large margin. Does the following context support or refute the claim?\n\n## Table\nPaper title: Deriving Machine Attention from Human Rationales\nTable caption: Table 3: Accuracy of transferring between aspects. Models with \u2020 use labeled data from source aspects. Models with \u2021 use human rationales on the target aspect.\nSource | Target | Svm | Ra-Svm\u2021 | Ra-Cnn\u2021 | Trans\u2020 | Ra-Trans\u2021\u2020 | Ours\u2021\u2020 | Oracle\u2020\nBeer aroma+palate | Beer look | 74.41 | 74.83 | 74.94 | 72.75 | 76.41 | [BOLD] 79.53 | 80.29\nBeer look+palate | Beer aroma | 68.57 | 69.23 | 67.55 | 69.92 | 76.45 | [BOLD] 77.94 | 78.11\nBeer look+aroma | Beer palate | 63.88 | 67.82 | 65.72 | 74.66 | 73.40 | [BOLD] 75.24 | 75.50\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "aa63576f-129c-4fdd-a6d2-2f7410459284",
    "input": "## Claim\nHere is a claim: GDPL outperforms three baselines significantly in all aspects (sign test, p-value < 0.01) except for the quality compared with ACER. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c6f524cd-84be-41ec-9b1e-61695d024d5c",
    "input": "## Claim\nHere is a claim: [CONTINUE] Lemma-based targets without POS disambiguation perform best on WN-N when dependency-based contexts are used; however, the difference to lemmatized and disambiguated targets is not statistically significant (p > .1). Does the following context support or refute the claim?\n\n## Table\nPaper title: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources\nTable caption: Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.\n[EMPTY] | WN-N P | WN-N R | WN-N F | WN-V P | WN-V R | WN-V F | VN P | VN R | VN F\nContext: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2\ntype | .700 | .654 | .676 | .535 | .474 | .503 | .327 | .309 | .318\nx+POS | .699 | .651 | .674 | .544 | .472 | .505 | .339 | .312 | .325\nlemma | .706 | .660 | .682 | .576 | .520 | .547 | .384 | .360 | .371\nx+POS | <bold>.710</bold> | <bold>.662</bold> | <bold>.685</bold> | <bold>.589</bold> | <bold>.529</bold> | <bold>.557</bold> | <bold>.410</bold> | <bold>.389</bold> | <bold>.399</bold>\nContext: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep\ntype | .712 | .661 | .686 | .545 | .457 | .497 | .324 | .296 | .310\nx+POS | .715 | .659 | .686 | .560 | .464 | .508 | .349 | .320 | .334\nlemma | <bold>.725</bold> | <bold>.668</bold> | <bold>.696</bold> | .591 | .512 | .548 | .408 | .371 | .388\nx+POS | .722 | .666 | .693 | <bold>.609</bold> | <bold>.527</bold> | <bold>.565</bold> | <bold>.412</bold> | <bold>.381</bold> | <bold>.396</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "4301bd18-8256-4555-b14e-6fa07d64c262",
    "input": "## Claim\nHere is a claim: Another interesting fact in Table 1 is that the training throughput on the linear dataset does not scale better than the throughput on the balanced dataset, as the batch size increases. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2\nTable caption: Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.\nBatch size | Throughput (instances/s) Balanced | Throughput (instances/s) Moderate | Throughput (instances/s) Linear\n1 | 46.7 | 27.3 | 7.6\n10 | 125.2 | 78.2 | 22.7\n25 | 129.7 | 83.1 | 45.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "58ed9fb7-c389-41e3-a456-a2d5f2d7dc0e",
    "input": "## Claim\nHere is a claim: Negation can be either clearly express or be subtly used. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 3: Cue and token distribution in the conversational negation corpus.\nTotal negation cues | 2921\nTrue negation cues | 2674\nFalse negation cues | 247\nAverage scope length | 2.9\nAverage sentence length | 13.6\nAverage tweet length | 22.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e2bf9a66-bb8d-4c47-b276-a33bcb67117c",
    "input": "## Claim\nHere is a claim: Table 4 shows that GDPL has the smallest KL-divergence to the human on the number of dialog turns over the baselines, which implies that GDPL behaves more like the human. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.\nGP-MBCM | ACER | PPO | ALDM | GDPL\n1.666 | 0.775 | 0.639 | 1.069 | [BOLD] 0.238\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "298f453e-23a7-4de3-a6b1-be249400fc27",
    "input": "## Claim\nHere is a claim: This suggests that enriching input graphs with the global node and including the linear combination can facilitate GCNs to learn better information aggregations, producing more expressive graph representations. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "bbea621d-e96a-41a8-ad30-309fb0ef0a51",
    "input": "## Claim\nHere is a claim: MIL-ND does not achieve higher precision, recall, and F1 than MIL, and using its confidence at test time (\u03c4 MIL-ND, 'All' setting) was not beneficial in terms of precision and F1. Does the following context support or refute the claim?\n\n## Table\nPaper title: Distant Learning for Entity Linking with Automatic Noise Detection\nTable caption: Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.\nSystem | All P | All R | All F1 | In  [ITALIC] E+ P | In  [ITALIC] E+ R | In  [ITALIC] E+ F1\nName matching | 15.03 | 15.03 | 15.03 | 29.13 | 29.13 | 29.13\nMIL (model 1) | 35.87 | 35.87 | 35.87 \u00b10.72 | 69.38 | 69.38 | 69.38 \u00b11.29\nMIL-ND (model 2) | 37.42 | [BOLD] 37.42 | 37.42 \u00b10.35 | 72.50 | [BOLD] 72.50 | [BOLD] 72.50 \u00b10.68\n[ITALIC] \u03c4MIL-ND (model 2) | [BOLD] 38.91 | 36.73 | [BOLD] 37.78 \u00b10.26 | [BOLD] 73.19 | 71.15 | 72.16 \u00b10.48\nSupervised learning | 42.90 | 42.90 | 42.90 \u00b10.59 | 83.12 | 83.12 | 83.12 \u00b11.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "216f48dd-a7c0-4e32-a01f-007f70a4db01",
    "input": "## Claim\nHere is a claim: Table 9: Performance of different models on the neural user simulator. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "086ef478-1afa-472e-b71f-ca7b784c40fb",
    "input": "## Claim\nHere is a claim: For Task B, all models trained on the stacked learner beat the baseline substantially even when using only plain averaged word embeddings. Does the following context support or refute the claim?\n\n## Table\nPaper title: UKP TU-DA at GermEval 2017:Deep Learning for Aspect Based Sentiment Detection\nTable caption: Table 5: Task B results\n[EMPTY] | Micro F1\nBaseline | 0.709\nW2V (<italic>d</italic>=50) | 0.736\nW2V (<italic>d</italic>=500) | 0.753\nS2V | 0.748\nS2V + W2V (<italic>d</italic>=50) | 0.744\nS2V + K + W2V(<italic>d</italic>=50) | 0.749\nSIF (DE) | 0.759\nSIF (DE-EN) | <bold>0.765</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "34bd9559-e3ae-43a5-9c37-ca40fbcdd85a",
    "input": "## Claim\nHere is a claim: LSTM does significantly better than Word2Vec, especially for MEN-TR-3k (0.766 vs. 0.552, p-value<0.00001), RG65 (0.790 vs. 0.744, p-value<0.00001) and MTurk771 (0.682 vs. 0.650, p-value<0.00001). Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VI: Correlations for Word Similarity Tests\nDataset (EN-) | GloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\nWS-353-ALL | 0.612 | 0.7156 | 0.634 | 0.622 | 0.173 | 0.690 | 0.657\nSIMLEX-999 | 0.359 | 0.3939 | 0.295 | 0.355 | 0.090 | 0.380 | 0.381\nVERB-143 | 0.326 | 0.4430 | 0.255 | 0.271 | 0.293 | 0.271 | 0.348\nSimVerb-3500 | 0.193 | 0.2856 | 0.184 | 0.197 | 0.035 | 0.234 | 0.245\nWS-353-REL | 0.578 | 0.6457 | 0.595 | 0.578 | 0.134 | 0.695 | 0.619\nRW-STANF. | 0.378 | 0.4858 | 0.316 | 0.373 | 0.122 | 0.390 | 0.382\nYP-130 | 0.524 | 0.5211 | 0.353 | 0.482 | 0.169 | 0.420 | 0.589\nMEN-TR-3k | 0.710 | 0.7528 | 0.684 | 0.696 | 0.298 | 0.769 | 0.725\nRG-65 | 0.768 | 0.8051 | 0.736 | 0.732 | 0.338 | 0.761 | 0.774\nMTurk-771 | 0.650 | 0.6712 | 0.593 | 0.623 | 0.199 | 0.665 | 0.671\nWS-353-SIM | 0.682 | 0.7883 | 0.713 | 0.702 | 0.220 | 0.720 | 0.720\nMC-30 | 0.749 | 0.8112 | 0.799 | 0.726 | 0.330 | 0.735 | 0.776\nMTurk-287 | 0.649 | 0.6645 | 0.591 | 0.631 | 0.295 | 0.674 | 0.634\nAverage | 0.552 | 0.6141 | 0.519 | 0.538 | 0.207 | 0.570 | 0.579\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "cbc15a85-cab6-4dbe-acc7-941079c444f5",
    "input": "## Claim\nHere is a claim: A potential reason is that the RL agent has only learned limited useful signals in some small, low-quality data; therefore, other summarisation signals including aspect modelling, coverage modelling and salience modelling are missing. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "091c653e-d166-4f19-a914-0f0a73b1b51e",
    "input": "## Claim\nHere is a claim: Our proposed method outperforms Pretrained Word2Sense embeddings, despite the latter having the advantage of training on a larger corpus. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "036aebad-b965-470d-beea-378f90f6e122",
    "input": "## Claim\nHere is a claim: The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.\nTopic Name | Size | TF-IDF ARI | WMD ARI | Sent2vec ARI | Doc2vec ARI | BERT ARI | [ITALIC] OD-w2v ARI | [ITALIC] OD-d2v ARI | TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil. | Sent2vec  [ITALIC] Sil. | Doc2vec  [ITALIC] Sil. | BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil. | [ITALIC] OD-d2v  [ITALIC] Sil.\nAffirmative Action | 81 | -0.07 | -0.02 | 0.03 | -0.01 | -0.02 | [BOLD] 0.14 | [ITALIC] 0.02 | 0.01 | 0.01 | -0.01 | -0.02 | -0.04 | [BOLD] 0.06 | [ITALIC] 0.01\nAtheism | 116 | [BOLD] 0.19 | 0.07 | 0.00 | 0.03 | -0.01 | 0.11 | [ITALIC] 0.16 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | [ITALIC] 0.05 | [BOLD] 0.07\nAusterity Measures | 20 | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01 | -0.05 | 0.04 | [BOLD] 0.21 | -0.01 | 0.06 | 0.07 | 0.05 | -0.03 | 0.10 | [BOLD] 0.19 | 0.1\nDemocratization | 76 | 0.02 | -0.01 | 0.00 | [ITALIC] 0.09 | -0.01 | [BOLD] 0.11 | 0.07 | 0.01 | 0.01 | 0.02 | 0.02 | 0.03 | [BOLD] 0.16 | [ITALIC] 0.11\nEducation Voucher Scheme | 30 | [BOLD] 0.25 | 0.12 | 0.08 | -0.02 | 0.04 | 0.13 | [ITALIC] 0.19 | 0.01 | 0.01 | 0.01 | -0.01 | 0.02 | [ITALIC] 0.38 | [BOLD] 0.40\nGambling | 60 | -0.06 | -0.01 | -0.02 | 0.04 | 0.09 | [ITALIC] 0.35 | [BOLD] 0.39 | 0.01 | 0.02 | 0.03 | 0.01 | 0.09 | [BOLD] 0.30 | [ITALIC] 0.22\nHousing | 30 | 0.01 | -0.01 | -0.01 | -0.02 | 0.08 | [BOLD] 0.27 | 0.01 | 0.02 | 0.03 | 0.03 | 0.01 | 0.11 | [BOLD] 0.13 | [ITALIC] 0.13\nHydroelectric Dams | 110 | [BOLD] 0.47 | [ITALIC] 0.45 | [ITALIC] 0.45 | -0.01 | 0.38 | 0.35 | 0.14 | 0.04 | 0.08 | 0.12 | 0.01 | 0.19 | [BOLD] 0.26 | [ITALIC] 0.09\nIntellectual Property | 66 | 0.01 | 0.01 | 0.00 | 0.03 | 0.03 | [ITALIC] 0.05 | [BOLD] 0.14 | 0.01 | [ITALIC] 0.04 | 0.03 | 0.01 | 0.03 | [ITALIC] 0.04 | [BOLD] 0.12\nKeystone pipeline | 18 | 0.01 | 0.01 | 0.00 | -0.13 | [BOLD] 0.07 | -0.01 | [BOLD] 0.07 | -0.01 | -0.03 | -0.03 | -0.07 | 0.03 | [BOLD] 0.05 | [ITALIC] 0.02\nMonarchy | 61 | -0.04 | 0.01 | 0.00 | 0.03 | -0.02 | [BOLD] 0.15 | [BOLD] 0.15 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 | [BOLD] 0.11 | [ITALIC] 0.09\nNational Service | 33 | 0.14 | -0.03 | -0.01 | 0.02 | 0.01 | [ITALIC] 0.31 | [BOLD] 0.39 | 0.02 | 0.04 | 0.02 | 0.01 | 0.02 | [BOLD] 0.25 | [BOLD] 0.25\nOne-child policy China | 67 | -0.05 | 0.01 | [BOLD] 0.11 | -0.02 | 0.02 | [BOLD] 0.11 | 0.01 | 0.01 | 0.02 | [ITALIC] 0.04 | -0.01 | 0.03 | [BOLD] 0.07 | -0.02\nOpen-source Software | 48 | -0.02 | -0.01 | [ITALIC] 0.05 | 0.01 | 0.12 | [BOLD] 0.09 | -0.02 | 0.01 | -0.01 | 0.00 | -0.02 | 0.03 | [BOLD] 0.18 | 0.01\nPornography | 52 | -0.02 | 0.01 | 0.01 | -0.02 | -0.01 | [BOLD] 0.41 | [BOLD] 0.41 | 0.01 | 0.01 | 0.02 | -0.01 | 0.03 | [BOLD] 0.47 | [ITALIC] 0.41\nSeanad Abolition | 25 | 0.23 | 0.09 | -0.01 | -0.01 | 0.03 | [ITALIC] 0.32 | [BOLD] 0.54 | 0.02 | 0.01 | -0.01 | -0.03 | -0.04 | [ITALIC] 0.15 | [BOLD] 0.31\nTrades Unions | 19 | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60 | -0.05 | 0.44 | [ITALIC] 0.44 | 0.29 | 0.1 | 0.17 | 0.21 | 0.01 | 0.26 | [BOLD] 0.48 | [ITALIC] 0.32\nVideo Games | 72 | -0.01 | 0.01 | 0.12 | 0.01 | 0.08 | [ITALIC] 0.40 | [BOLD] 0.56 | 0.01 | 0.01 | 0.06 | 0.01 | 0.05 | [ITALIC] 0.32 | [BOLD] 0.42\nAverage | 54.67 | 0.09 | 0.07 | 0.08 | 0.01 | 0.08 | [BOLD] 0.22 | [ITALIC] 0.20 | 0.02 | 0.03 | 0.04 | -0.01 | 0.05 | [BOLD] 0.20 | [ITALIC] 0.17\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a48fe54a-935d-4004-97f5-8dbf683c3567",
    "input": "## Claim\nHere is a claim: [CONTINUE] Under system setup, our model CANDELA statistically significantly outperforms all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p < [CONTINUE] .0005). Does the following context support or refute the claim?\n\n## Table\nPaper title: Argument Generation with Retrieval, Planning, and Realization\nTable caption: Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.\n[EMPTY] | [ITALIC] w/ System Retrieval  [BOLD] B-2 | [ITALIC] w/ System Retrieval  [BOLD] B-4 | [ITALIC] w/ System Retrieval  [BOLD] R-2 | [ITALIC] w/ System Retrieval  [BOLD] MTR | [ITALIC] w/ System Retrieval  [BOLD] #Word | [ITALIC] w/ System Retrieval  [BOLD] #Sent | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4 | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent\nHuman | - | - | - | - | 66 | 22 | - | - | - | - | 66 | 22\nRetrieval | 7.55 | 1.11 | 8.64 | 14.38 | 123 | 23 | 10.97 | 3.05 | 23.49 | 20.08 | 140 | 21\n[BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [EMPTY] | [EMPTY]\nSeq2seq | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15 | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15\nSeq2seqAug | 8.26 | 2.24 | 13.79 | 15.75 | 78 | 14 | 10.98 | 4.41 | 22.97 | 19.62 | 71 | 14\n[ITALIC] w/o psg | 7.94 | 2.28 | 10.13 | 15.71 | 75 | 12 | 9.89 | 3.34 | 14.20 | 18.40 | 66 | 12\nH&W\u00a0Hua and Wang ( 2018 ) | 3.64 | 0.92 | 8.83 | 11.78 | 51 | 12 | 8.51 | 2.86 | 18.89 | 17.18 | 58 | 12\n[BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [EMPTY] | [EMPTY]\nCANDELA | 12.02\u2217 | [BOLD] 2.99\u2217 | [BOLD] 14.93\u2217 | [BOLD] 16.92\u2217 | 119 | 22 | 15.80\u2217 | [BOLD] 5.00\u2217 | [BOLD] 23.75 | [BOLD] 20.18 | 116 | 22\n[ITALIC] w/o psg | [BOLD] 12.33\u2217 | 2.86\u2217 | 14.53\u2217 | 16.60\u2217 | 123 | 23 | [BOLD] 16.33\u2217 | 4.98\u2217 | 23.65 | 19.94 | 123 | 23\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "1503579e-421f-4f43-8df4-b7ad246a398e",
    "input": "## Claim\nHere is a claim: based on the analysis results, we conclude that the dialog states were successfully retained in the model policy and user simulator, but the source of error lies in the action selection with respect to informability and matching. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "3197a3a2-dcb8-4bbb-8be5-a2e8eee69365",
    "input": "## Claim\nHere is a claim: However, the sdp information has a clear positive impact on all the relation types (Table 1). Does the following context support or refute the claim?\n\n## Table\nPaper title: Syntactic Dependency Representations in Neural Relation Classification\nTable caption: Table 1: Effect of using the shortest dependency path on each relation type.\n[BOLD] Relation | [BOLD] best F1 (in 5-fold) without sdp | [BOLD] best F1 (in 5-fold) with sdp | [BOLD] Diff.\nUSAGE | 60.34 | 80.24 | + 19.90\nMODEL-FEATURE | 48.89 | 70.00 | + 21.11\nPART_WHOLE | 29.51 | 70.27 | +40.76\nTOPIC | 45.80 | 91.26 | +45.46\nRESULT | 54.35 | 81.58 | +27.23\nCOMPARE | 20.00 | 61.82 | + 41.82\nmacro-averaged | 50.10 | 76.10 | +26.00\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a8be9400-0253-4cda-ad4a-06b707c381b5",
    "input": "## Claim\nHere is a claim: Accordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally stronger: [CONTINUE] as in Eq. Does the following context support or refute the claim?\n\n## Table\nPaper title: Neural End-to-End Learning for Computational Argumentation Mining\nTable caption: Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by \u201c:\u201d. Layers from which tasks feed are indicated by respective numbers.\n[EMPTY] | C-F1 100% | C-F1 50% | R-F1 100% | R-F1 50% | F1 100% | F1 50%\nY-3 | 49.59 | 65.37 | 26.28 | 37.00 | 34.35 | 47.25\nY-3:Y<italic>C</italic>-1 | 54.71 | 66.84 | 28.44 | 37.35 | 37.40 | 47.92\nY-3:Y<italic>R</italic>-1 | 51.32 | 66.49 | 26.92 | 37.18 | 35.31 | 47.69\nY-3:Y<italic>C</italic>-3 | <bold>54.58</bold> | 67.66 | <bold>30.22</bold> | <bold>40.30</bold> | <bold>38.90</bold> | <bold>50.51</bold>\nY-3:Y<italic>R</italic>-3 | 53.31 | 66.71 | 26.65 | 35.86 | 35.53 | 46.64\nY-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2 | 52.95 | <bold>67.84</bold> | 27.90 | 39.71 | 36.54 | 50.09\nY-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3 | 54.55 | 67.60 | 28.30 | 38.26 | 37.26 | 48.86\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6a91430a-d1d2-4fc6-80ff-a18af52d33a8",
    "input": "## Claim\nHere is a claim: In contrast, our proposed classifier can almost precisely identify the one-word scope without any syntactic information. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "9e0ce9f6-6eb5-4eda-97f7-ec6948db9bf5",
    "input": "## Claim\nHere is a claim: As shown in Table 8, the S2S baseline outperforms the G2S approaches. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 8: Fraction of elements in the output that are not present in the input (ADDED) and the fraction of elements in the input graph that are missing in the generated sentence (MISS), for the test set of LDC2017T10. The token lemmas are used in the comparison. GOLD refers to the reference sentences.\n<bold>Model</bold> | <bold>ADDED</bold> | <bold>MISS</bold>\nS2S | 47.34 | 37.14\nG2S-GIN | 48.67 | 33.64\nG2S-GAT | 48.24 | 33.73\nG2S-GGNN | 48.66 | 34.06\nGOLD | 50.77 | 28.35\n[EMPTY] | [EMPTY] | [EMPTY]\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "bfb26805-c8f6-4854-9e1c-14b7534b396e",
    "input": "## Claim\nHere is a claim: We suspect that there are not enough data to pretrain the models and that the thread classification task used to pretrain the HAN models may not be sophisticated enough to learn effective thread vectors. Does the following context support or refute the claim?\n\n## Table\nPaper title: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks\nTable caption: Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\n[BOLD] System | [BOLD] ROUGE-1  [BOLD] R (%) | [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%) | [BOLD] ROUGE-2  [BOLD] R (%) | [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%) | [BOLD] Sentence-Level  [BOLD] R (%) | [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)\n[BOLD] ILP | 24.5 | 41.1 | 29.3\u00b10.5 | 7.9 | 15.0 | 9.9\u00b10.5 | 13.6 | 22.6 | 15.6\u00b10.4\n[BOLD] Sum-Basic | 28.4 | 44.4 | 33.1\u00b10.5 | 8.5 | 15.6 | 10.4\u00b10.4 | 14.7 | 22.9 | 16.7\u00b10.5\n[BOLD] KL-Sum | 39.5 | 34.6 | 35.5\u00b10.5 | 13.0 | 12.7 | 12.3\u00b10.5 | 15.2 | 21.1 | 16.3\u00b10.5\n[BOLD] LexRank | 42.1 | 39.5 | 38.7\u00b10.5 | 14.7 | 15.3 | 14.2\u00b10.5 | 14.3 | 21.5 | 16.0\u00b10.5\n[BOLD] MEAD | 45.5 | 36.5 | 38.5\u00b1 0.5 | 17.9 | 14.9 | 15.4\u00b10.5 | 27.8 | 29.2 | 26.8\u00b10.5\n[BOLD] SVM | 19.0 | 48.8 | 24.7\u00b10.8 | 7.5 | 21.1 | 10.0\u00b10.5 | 32.7 | 34.3 | 31.4\u00b10.4\n[BOLD] LogReg | 26.9 | 34.5 | 28.7\u00b10.6 | 6.4 | 9.9 | 7.3\u00b10.4 | 12.2 | 14.9 | 12.7\u00b10.5\n[BOLD] LogReg [ITALIC] r | 28.0 | 34.8 | 29.4\u00b10.6 | 6.9 | 10.4 | 7.8\u00b10.4 | 12.1 | 14.5 | 12.5\u00b10.5\n[BOLD] HAN | 31.0 | 42.8 | 33.7\u00b10.7 | 11.2 | 17.8 | 12.7\u00b10.5 | 26.9 | 34.1 | 32.4\u00b10.5\n[BOLD] HAN+pretrainT | 32.2 | 42.4 | 34.4\u00b10.7 | 11.5 | 17.5 | 12.9\u00b10.5 | 29.6 | 35.8 | 32.2\u00b10.5\n[BOLD] HAN+pretrainU | 32.1 | 42.1 | 33.8\u00b10.7 | 11.6 | 17.6 | 12.9\u00b10.5 | 30.1 | 35.6 | 32.3\u00b10.5\n[BOLD] HAN [ITALIC] r | 38.1 | 40.5 | [BOLD] 37.8\u00b10.5 | 14.0 | 17.1 | [BOLD] 14.7\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainT [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.5 | 16.8 | [BOLD] 14.4\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainU [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.6 | 16.9 | [BOLD] 14.4\u00b10.5 | 33.9 | 33.8 | [BOLD] 33.8\u00b10.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "883e2064-9987-4f7e-8c74-d971736b8f14",
    "input": "## Claim\nHere is a claim: This strongly indicates that there is a superficial cue that affects model performance, but this cue is not captured by Word frequency Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "87943fea-34a2-4928-b643-cc2d1a2e32fd",
    "input": "## Claim\nHere is a claim: We also have competitive results to Guo et al. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "64086c42-77ce-41fd-8af0-b264671ca83c",
    "input": "## Claim\nHere is a claim: [CONTINUE] When comparing DF model which takes into account only the number of documents that the word occurs, with DocSub which considers the number of shared documents between two words, DocSub achieved better values of precision, but lower values of recall. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "e7aa3eec-8b70-4c8f-820e-a6c49ccaf595",
    "input": "## Claim\nHere is a claim: Despite performing slightly worse than sparsemax under automatic metrics, TVMAX outperforms sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation, reported in Table 2. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 2: Human evaluation results on MSCOCO.\n[EMPTY] | caption | attention relevance\nsoftmax | 3.50 | 3.38\nsparsemax | 3.71 | 3.89\nTVmax | [BOLD] 3.87 | [BOLD] 4.10\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "edf3572a-6423-4a55-ad0e-a09662bdec7c",
    "input": "## Claim\nHere is a claim: We have 116,674 tweets, with an average length of 22.3 tokens. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 3: Cue and token distribution in the conversational negation corpus.\nTotal negation cues | 2921\nTrue negation cues | 2674\nFalse negation cues | 247\nAverage scope length | 2.9\nAverage sentence length | 13.6\nAverage tweet length | 22.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "56156f9c-7163-4650-9091-2240dfc00b55",
    "input": "## Claim\nHere is a claim: The results in Table 3 show that translation quality of LRN is slightly worse than that of GRU (-0.02 BLEU). Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.\nModel | #Params | BLEU | Train | Decode\nGNMT | - | 24.61 | - | -\nGRU | 206M | 26.28 | 2.67 | 45.35\nATR | 122M | 25.70 | 1.33 | [BOLD] 34.40\nSRU | 170M | 25.91 | 1.34 | 42.84\nLRN | 143M | 26.26 | [BOLD] 0.99 | 36.50\noLRN | 164M | [BOLD] 26.73 | 1.15 | 40.19\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "1af207f8-66bc-4bfc-b6a3-c651e12783f0",
    "input": "## Claim\nHere is a claim: Without the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d1da902e-3754-4ed7-afcf-cc1642e09b62",
    "input": "## Claim\nHere is a claim: However, models trained using linguistic features on the training data obtain significantly higher predictive accuracy. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.\n[BOLD] Model | [BOLD] Acc | [BOLD] F1 | [BOLD] AUC\nMost Frequent Class | 64.2 | 39.1 | 0.500\nLogistic Regression | [EMPTY] | [EMPTY] | [EMPTY]\nSentiment \u2013 MPQA | 64.2 | 39.1 | 0.499\nSentiment \u2013 NRC | 63.9 | 42.2 | 0.599\nSentiment \u2013 V&B | 68.9 | 60.0 | 0.696\nSentiment \u2013 VADER | 66.0 | 54.2 | 0.654\nSentiment \u2013 Stanford | 68.0 | 55.6 | 0.696\nComplaint Specific (all) | 65.7 | 55.2 | 0.634\nRequest | 64.2 | 39.1 | 0.583\nIntensifiers | 64.5 | 47.3 | 0.639\nDowngraders | 65.4 | 49.8 | 0.615\nTemporal References | 64.2 | 43.7 | 0.535\nPronoun Types | 64.1 | 39.1 | 0.545\nPOS Bigrams | 72.2 | 66.8 | 0.756\nLIWC | 71.6 | 65.8 | 0.784\nWord2Vec Clusters | 67.7 | 58.3 | 0.738\nBag-of-Words | 79.8 | 77.5 | 0.866\nAll Features | [BOLD] 80.5 | [BOLD] 78.0 | [BOLD] 0.873\nNeural Networks | [EMPTY] | [EMPTY] | [EMPTY]\nMLP | 78.3 | 76.2 | 0.845\nLSTM | 80.2 | 77.0 | 0.864\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "4c5f3d6e-0a82-4654-8825-545bffd70dd2",
    "input": "## Claim\nHere is a claim: [CONTINUE] As the results of applying the co-occurrence baseline (\u03c1 = 0) shows (Table 2), the semantic relations in this data are strongly concentrated within a sentence boundary, especially for the relation of RecurLink, with an F1 of 1.0. Does the following context support or refute the claim?\n\n## Table\nPaper title: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data\nTable caption: Table 2: F1 score results per relation type of the best performing models.\nRelation type | Count | Intra-sentential co-occ.  [ITALIC] \u03c1=0 | Intra-sentential co-occ.  [ITALIC] \u03c1=5 | Intra-sentential co-occ.  [ITALIC] \u03c1=10 | BoC(Wiki-PubMed-PMC) LR | BoC(Wiki-PubMed-PMC) SVM | BoC(Wiki-PubMed-PMC) ANN\nTherapyTiming(TP,TD) | 428 | [BOLD] 0.84 | 0.59 | 0.47 | 0.78 | 0.81 | 0.78\nNextReview(Followup,TP) | 164 | [BOLD] 0.90 | 0.83 | 0.63 | 0.86 | 0.88 | 0.84\nToxicity(TP,CF/TR) | 163 | [BOLD] 0.91 | 0.77 | 0.55 | 0.85 | 0.86 | 0.86\nTestTiming(TN,TD/TP) | 184 | 0.90 | 0.81 | 0.42 | 0.96 | [BOLD] 0.97 | 0.95\nTestFinding(TN,TR) | 136 | 0.76 | 0.60 | 0.44 | [BOLD] 0.82 | 0.79 | 0.78\nThreat(O,CF/TR) | 32 | 0.85 | 0.69 | 0.54 | [BOLD] 0.95 | [BOLD] 0.95 | 0.92\nIntervention(TP,YR) | 5 | [BOLD] 0.88 | 0.65 | 0.47 | - | - | -\nEffectOf(Com,CF) | 3 | [BOLD] 0.92 | 0.62 | 0.23 | - | - | -\nSeverity(CF,CS) | 75 | [BOLD] 0.61 | 0.53 | 0.47 | 0.52 | 0.55 | 0.51\nRecurLink(YR,YR/CF) | 7 | [BOLD] 1.0 | [BOLD] 1.0 | 0.64 | - | - | -\nRecurInfer(NR/YR,TR) | 51 | 0.97 | 0.69 | 0.43 | [BOLD] 0.99 | [BOLD] 0.99 | 0.98\nGetOpinion(Referral,CF/other) | 4 | [BOLD] 0.75 | [BOLD] 0.75 | 0.5 | - | - | -\nContext(Dis,DisCont) | 40 | [BOLD] 0.70 | 0.63 | 0.53 | 0.60 | 0.41 | 0.57\nTestToAssess(TN,CF/TR) | 36 | 0.76 | 0.66 | 0.36 | [BOLD] 0.92 | [BOLD] 0.92 | 0.91\nTimeStamp(TD,TP) | 221 | [BOLD] 0.88 | 0.83 | 0.50 | 0.86 | 0.85 | 0.83\nTimeLink(TP,TP) | 20 | [BOLD] 0.92 | 0.85 | 0.45 | 0.91 | [BOLD] 0.92 | 0.90\nOverall | 1569 | 0.90 | 0.73 | 0.45 | 0.92 | [BOLD] 0.93 | 0.91\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "9faf0fb8-7f04-487b-8c21-c849d0edd997",
    "input": "## Claim\nHere is a claim: Interestingly, we observe a decrease of ROUGE and METEOR, but a marginal increase of BLEU-2 by removing passages from our model input. Does the following context support or refute the claim?\n\n## Table\nPaper title: Argument Generation with Retrieval, Planning, and Realization\nTable caption: Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.\n[EMPTY] | [ITALIC] w/ System Retrieval  [BOLD] B-2 | [ITALIC] w/ System Retrieval  [BOLD] B-4 | [ITALIC] w/ System Retrieval  [BOLD] R-2 | [ITALIC] w/ System Retrieval  [BOLD] MTR | [ITALIC] w/ System Retrieval  [BOLD] #Word | [ITALIC] w/ System Retrieval  [BOLD] #Sent | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4 | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent\nHuman | - | - | - | - | 66 | 22 | - | - | - | - | 66 | 22\nRetrieval | 7.55 | 1.11 | 8.64 | 14.38 | 123 | 23 | 10.97 | 3.05 | 23.49 | 20.08 | 140 | 21\n[BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [EMPTY] | [EMPTY]\nSeq2seq | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15 | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15\nSeq2seqAug | 8.26 | 2.24 | 13.79 | 15.75 | 78 | 14 | 10.98 | 4.41 | 22.97 | 19.62 | 71 | 14\n[ITALIC] w/o psg | 7.94 | 2.28 | 10.13 | 15.71 | 75 | 12 | 9.89 | 3.34 | 14.20 | 18.40 | 66 | 12\nH&W\u00a0Hua and Wang ( 2018 ) | 3.64 | 0.92 | 8.83 | 11.78 | 51 | 12 | 8.51 | 2.86 | 18.89 | 17.18 | 58 | 12\n[BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [EMPTY] | [EMPTY]\nCANDELA | 12.02\u2217 | [BOLD] 2.99\u2217 | [BOLD] 14.93\u2217 | [BOLD] 16.92\u2217 | 119 | 22 | 15.80\u2217 | [BOLD] 5.00\u2217 | [BOLD] 23.75 | [BOLD] 20.18 | 116 | 22\n[ITALIC] w/o psg | [BOLD] 12.33\u2217 | 2.86\u2217 | 14.53\u2217 | 16.60\u2217 | 123 | 23 | [BOLD] 16.33\u2217 | 4.98\u2217 | 23.65 | 19.94 | 123 | 23\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "fe88348b-5c34-4558-8ca9-26f3e4deb724",
    "input": "## Claim\nHere is a claim: between all three systems, GloVe ranks last, followed by the original implementation of this model, and finally the optimized implementation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VIII: Precision scores for the Semantic Analogy Test\nQuestions Subset | # of Questions Seen | GloVe | Word2Vec | Proposed\nAll | 8783 | 78.94 | 81.03 | 79.96\nAt least one | 1635 | 67.58 | 70.89 | 67.89\nconcept word | 1635 | 67.58 | 70.89 | 67.89\nAll concept words | 110 | 77.27 | 89.09 | 83.64\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "54edbc96-7e26-4732-9e3c-08ce9c75397e",
    "input": "## Claim\nHere is a claim: For Yelp, M0 has better Acc and PP than M1 at comparable semantic similarity. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\n[EMPTY] | Acc | Sim | PP | GM\nM0: shen-1 | 0.818 | 0.719 | 37.3 | 10.0\nM1: M0 [ITALIC] +para | 0.819 | 0.734 | 26.3 | 14.2\nM2: M0 [ITALIC] +cyc | 0.813 | 0.770 | 36.4 | 18.8\nM3: M0 [ITALIC] +cyc+lang | 0.807 | 0.796 | 28.4 | 21.5\nM4: M0 [ITALIC] +cyc+para | 0.798 | 0.783 | 39.7 | 19.2\nM5: M0 [ITALIC] +cyc+para+lang | 0.804 | 0.785 | 27.1 | 20.3\nM6: M0 [ITALIC] +cyc+2d | 0.805 | [BOLD] 0.817 | 43.3 | 21.6\nM7: M6+ [ITALIC] para+lang | 0.818 | 0.805 | [BOLD] 29.0 | [BOLD] 22.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a6372ae5-cf0e-4ad3-b2e6-864fcecf61d1",
    "input": "## Claim\nHere is a claim: on the other hand, neither the distance nor syntactic feature plays an important role in entity coreference performance, which indicates that the relation types of entities provide valuable information for cross-document entity coreference resolution. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | <bold>71.2</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "be06cdae-15fd-48da-8cf2-f7558b2216da",
    "input": "## Claim\nHere is a claim: [CONTINUE] In general terms, the results displayed in table 1 show that the rejection method can reduce the error of the output predictions when applying a pre-trained black-box classification system to a new domain. Does the following context support or refute the claim?\n\n## Table\nPaper title: Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability\nTable caption: Table 1: Accuracy obtained by training an standalone classifier, applying the API and the proposed wrapper for each domain\n[EMPTY] | [BOLD] BB source acc. | [BOLD] BB target acc. | [BOLD] Non-reject. acc. (10/20/30%) | [BOLD] Class. quality (10/20/30%) | [BOLD] Reject. quality (10/20/30%)\n[BOLD] Apply Yelp BB to SST-2 | 89.18\u00b10.08% | 77.13\u00b10.52% | 82.43\u00b10.22% 88.19\u00b10.50% 93.60\u00b10.16% | 80.40\u00b10.39% 83.11\u00b10.80% 83.05\u00b10.23% | 6.03\u00b10.45 6.04\u00b10.51 4.97\u00b10.07\n[BOLD] Apply SST-2 BB to Yelp | 83.306\u00b10.18% | 82.106\u00b10.88% | 87,98\u00b10.18% 92.13\u00b10.38% 94.19\u00b10.33% | 85.49\u00b10.88% 84.53\u00b10.38% 78.99\u00b10.46% | 8.30\u00b11.63 5.72\u00b10.27 3.73\u00b10.10\n[BOLD] Apply Electronics BB to Music | 86.39\u00b10.22% | 90.38\u00b10.13% | 95.04\u00b10.43% 96.45\u00b10.35% 97.26\u00b10.31% | 90.67\u00b10.88% 83.93\u00b10.67% 75.77\u00b10.54% | 10.7\u00b11.65 4.82\u00b10.35 3.25\u00b10.14\n[BOLD] Apply Music BB to Electronics | 93.10\u00b10.02% | 79.85\u00b10.0% | 83.26\u00b10.41% 87.06\u00b10.55% 90.50\u00b10.29% | 79.97\u00b10.74% 79.93\u00b10.87% 76.81\u00b10.41% | 4.1\u00b10.55 3.80\u00b10.35 3.32\u00b10.09\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "438c0d33-4a57-47d3-9bbc-d767c13a8119",
    "input": "## Claim\nHere is a claim: [CONTINUE] Comparing layers 1 through 4, we see that in 3/5 target languages (Ar, Ru, Zh), POS tagging accuracy peaks at layer 1 and does not improve at higher layers, with some drops at layers 2 and 3. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks\nTable caption: Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. \u201cEn\u201d column is an English autoencoder. BLEU scores are given for reference.\n[ITALIC] k | Ar | Es | Fr | Ru | Zh | En\nPOS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy\n0 | 88.0 | 87.9 | 87.9 | 87.8 | 87.7 | 87.4\n1 | 92.4 | 91.9 | 92.1 | 92.1 | 91.5 | 89.4\n2 | 91.9 | 91.8 | 91.8 | 91.8 | 91.3 | 88.3\n3 | 92.0 | 92.3 | 92.1 | 91.6 | 91.2 | 87.9\n4 | 92.1 | 92.4 | 92.5 | 92.0 | 90.5 | 86.9\nSEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy\n0 | 81.9 | 81.9 | 81.8 | 81.8 | 81.8 | 81.2\n1 | 87.9 | 87.7 | 87.8 | 87.9 | 87.7 | 84.5\n2 | 87.4 | 87.5 | 87.4 | 87.3 | 87.2 | 83.2\n3 | 87.8 | 87.9 | 87.9 | 87.3 | 87.3 | 82.9\n4 | 88.3 | 88.6 | 88.4 | 88.1 | 87.7 | 82.1\nBLEU | BLEU | BLEU | BLEU | BLEU | BLEU | BLEU\n[EMPTY] | 32.7 | 49.1 | 38.5 | 34.2 | 32.1 | 96.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f7b8c7ad-fdaf-48cc-9171-2c26b57c1fb0",
    "input": "## Claim\nHere is a claim: the distribution of dialogue sessions can be seen in Fig. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f0eebd6c-646f-409a-8ea7-e68af50007a0",
    "input": "## Claim\nHere is a claim: The error reduction over the best baseline is 15.08% on average. Does the following context support or refute the claim?\n\n## Table\nPaper title: Deriving Machine Attention from Human Rationales\nTable caption: Table 4: Accuracy of transferring between domains. Models with \u2020 use labeled data from source domains and unlabeled data from the target domain. Models with \u2021 use human rationales on the target task.\nSource | Target | Svm | Ra-Svm\u2021 | Ra-Cnn\u2021 | Trans\u2020 | Ra-Trans\u2021\u2020 | Ours\u2021\u2020 | Oracle\u2020\nBeer look + Beer aroma + Beer palate | Hotel location | 78.65 | 79.09 | 79.28 | 80.42 | 82.10 | [BOLD] 84.52 | 85.43\nBeer look + Beer aroma + Beer palate | Hotel cleanliness | 86.44 | 86.68 | 89.01 | 86.95 | 87.15 | [BOLD] 90.66 | 92.09\nBeer look + Beer aroma + Beer palate | Hotel service | 85.34 | 86.61 | 87.91 | 87.37 | 86.40 | [BOLD] 89.93 | 92.42\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "31b8b6fe-df87-467d-9695-321d94ad69f9",
    "input": "## Claim\nHere is a claim: The system does not perform well on synthetic dataset with a minimum of 80% P@1 and 98% P@10. Does the following context support or refute the claim?\n\n## Table\nPaper title: A context sensitive real-time Spell Checker with language adaptability\nTable caption: TABLE II: Synthetic Data Performance results\n[BOLD] Language | [BOLD] # Test | [BOLD] P@1 | [BOLD] P@3 | [BOLD] P@5 | [BOLD] P@10 | [BOLD] MRR\n[BOLD] Language | [BOLD] Samples | [BOLD] P@1 | [BOLD] P@3 | [BOLD] P@5 | [BOLD] P@10 | [BOLD] MRR\nBengali | 140000 | 91.30 | 97.83 | 98.94 | 99.65 | 94.68\nCzech | 94205 | 95.84 | 98.72 | 99.26 | 99.62 | 97.37\nDanish | 140000 | 85.84 | 95.19 | 97.28 | 98.83 | 90.85\nDutch | 140000 | 86.83 | 95.01 | 97.04 | 98.68 | 91.32\nEnglish | 140000 | 97.08 | 99.39 | 99.67 | 99.86 | 98.27\nFinnish | 140000 | 97.77 | 99.58 | 99.79 | 99.90 | 98.69\nFrench | 140000 | 86.52 | 95.66 | 97.52 | 98.83 | 91.38\nGerman | 140000 | 87.58 | 96.16 | 97.86 | 99.05 | 92.10\nGreek | 30022 | 84.95 | 94.99 | 96.88 | 98.44 | 90.27\nHebrew | 132596 | 94.00 | 98.26 | 99.05 | 99.62 | 96.24\nHindi | 140000 | 82.19 | 93.71 | 96.28 | 98.30 | 88.40\nIndonesian | 140000 | 95.01 | 98.98 | 99.50 | 99.84 | 97.04\nItalian | 140000 | 89.93 | 97.31 | 98.54 | 99.38 | 93.76\nMarathi | 140000 | 93.01 | 98.16 | 99.06 | 99.66 | 95.69\nPolish | 140000 | 95.65 | 99.17 | 99.62 | 99.86 | 97.44\nPortuguese | 140000 | 86.73 | 96.29 | 97.94 | 99.10 | 91.74\nRomanian | 140000 | 95.52 | 98.79 | 99.32 | 99.68 | 97.22\nRussian | 140000 | 94.85 | 98.74 | 99.33 | 99.71 | 96.86\nSpanish | 140000 | 85.91 | 95.35 | 97.18 | 98.57 | 90.92\nSwedish | 140000 | 88.86 | 96.40 | 98.00 | 99.14 | 92.87\nTamil | 140000 | 98.05 | 99.70 | 99.88 | 99.98 | 98.88\nTelugu | 140000 | 97.11 | 99.68 | 99.92 | 99.99 | 98.38\nThai | 12403 | 98.73 | 99.71 | 99.78 | 99.85 | 99.22\nTurkish | 140000 | 97.13 | 99.51 | 99.78 | 99.92 | 98.33\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6369cebf-b9ca-4fd0-9d72-621681d2ebdf",
    "input": "## Claim\nHere is a claim: These experiments show that the number of factors giving the best performance does not vary depending on the underlying data distribution. Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 3: Performance comparison of our model with different values of m on the two datasets.\n[ITALIC] m | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1\n1 | 0.541 | 0.595 | [BOLD] 0.566 | 0.495 | 0.621 | 0.551\n2 | 0.521 | 0.597 | 0.556 | 0.482 | 0.656 | 0.555\n3 | 0.490 | 0.617 | 0.547 | 0.509 | 0.633 | 0.564\n4 | 0.449 | 0.623 | 0.522 | 0.507 | 0.652 | [BOLD] 0.571\n5 | 0.467 | 0.609 | 0.529 | 0.488 | 0.677 | 0.567\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "8c45d451-b086-4e3f-a4ec-3f686a647811",
    "input": "## Claim\nHere is a claim: On the muli-domain dataset, MultiWoZ, our model achieves a joint goal accuracy of 48.79%, which is lower than the previous state-of-the-art. Does the following context support or refute the claim?\n\n## Table\nPaper title: Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation\nTable caption: Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set. We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model Mrksic et al. (2017), while the baseline for the MultiWoZ dataset is taken from the official website of MultiWoZ Budzianowski et al. (2018).\n[BOLD] DST Models | [BOLD] Joint Acc. WoZ 2.0 | [BOLD] Joint Acc. MultiWoZ | [BOLD] ITC\nBaselines Mrksic et al. ( 2017 ) | 70.8% | 25.83% | [ITALIC] O( [ITALIC] mn)\nNBT-CNN Mrksic et al. ( 2017 ) | 84.2% | - | [ITALIC] O( [ITALIC] mn)\nStateNet_PSI Ren et al. ( 2018 ) | [BOLD] 88.9% | - | [ITALIC] O( [ITALIC] n)\nGLAD Nouri and Hosseini-Asl ( 2018 ) | 88.5% | 35.58% | [ITALIC] O( [ITALIC] mn)\nHyST (ensemble) Goel et al. ( 2019 ) | - | 44.22% | [ITALIC] O( [ITALIC] n)\nDSTRead (ensemble) Gao et al. ( 2019 ) | - | 42.12% | [ITALIC] O( [ITALIC] n)\nTRADE Wu et al. ( 2019 ) | - | 48.62% | [ITALIC] O( [ITALIC] n)\nCOMER | 88.6% | [BOLD] 48.79% | [ITALIC] O(1)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "690d5d5d-b552-4cdf-b858-ccefaf6ddfbc",
    "input": "## Claim\nHere is a claim: BI+IS with EWC-adapted models gives a 0.9 / 3.4 BLEU gain over the strong uniform EWC ensemble, and a 2.4 / 10.2 overall BLEU gain over the approach described in Freitag and Al-Onaizan (2016). Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.\n[BOLD] Language pair | [BOLD] Model type | [BOLD] Oracle model | [BOLD] Decoder configuration  [BOLD] Uniform | [BOLD] Decoder configuration  [BOLD] BI + IS\nes-en | Unadapted | 36.4 | 34.7 | 36.6\nes-en | No-reg | 36.6 | 34.8 | -\nes-en | EWC | 37.0 | 36.3 | [BOLD] 37.2\nen-de | Unadapted | 36.4 | 26.8 | 38.8\nen-de | No-reg | 41.7 | 31.8 | -\nen-de | EWC | 42.1 | 38.6 | [BOLD] 42.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ce07043c-e05f-4ac4-bf96-7de4531fb8f8",
    "input": "## Claim\nHere is a claim: [CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss increases PP, sometimes at a slight cost of semantic preservation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\n[EMPTY] | Acc | Sim | PP | GM\nM0: shen-1 | 0.818 | 0.719 | 37.3 | 10.0\nM1: M0 [ITALIC] +para | 0.819 | 0.734 | 26.3 | 14.2\nM2: M0 [ITALIC] +cyc | 0.813 | 0.770 | 36.4 | 18.8\nM3: M0 [ITALIC] +cyc+lang | 0.807 | 0.796 | 28.4 | 21.5\nM4: M0 [ITALIC] +cyc+para | 0.798 | 0.783 | 39.7 | 19.2\nM5: M0 [ITALIC] +cyc+para+lang | 0.804 | 0.785 | 27.1 | 20.3\nM6: M0 [ITALIC] +cyc+2d | 0.805 | [BOLD] 0.817 | 43.3 | 21.6\nM7: M6+ [ITALIC] para+lang | 0.818 | 0.805 | [BOLD] 29.0 | [BOLD] 22.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "17171f81-9831-4b32-bfa9-35bd2481a747",
    "input": "## Claim\nHere is a claim: When the model focuses on \u201cnele\u201d and \u201ctype\u201d, it learns the semantic meaning of them, thus enabling the prediction of triples that have \u201cnele\u201d and \u201ctype\u201d Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 2: Precisions on the Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nRank+ExATT | 0.584 | 0.535 | 0.487 | 0.392\nPCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204\nPCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396\nOur Model | 0.650 | 0.519 | 0.422 | [BOLD] 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8667413d-d331-4b2e-bae7-01f3a106b373",
    "input": "## Claim\nHere is a claim: As occurred in the experiment using the top 1,000 words, this experiment also kept TF with the highest values of f-measure for most methods, except for the Portuguese Europarl corpus, where DocSub had the highest value. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150 | 0.0150 | 0.0445 | 0.0326\nP | EN | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.0060 | 0.0092 | 0.0090 | 0.0356 | 0.0162\nP | PT | Europarl | 0.5710 | 0.1948 | 0.3855 | 0.5474 | 0.4485 | [BOLD] 0.8052 | 0.4058\n[EMPTY] | PT | Ted Talks | [BOLD] 0.6304 | 0.1870 | 0.3250 | 0.5312 | 0.4576 | 0.6064 | 0.3698\nR | EN | Europarl | 0.0037 | 0.3278 | 0.5941 | 0.6486 | [BOLD] 0.6490 | 0.0017 | 0.0003\nR | EN | Ted Talks | 0.0002 | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332 | 0.0967 | 0.0003\nR | PT | Europarl | 0.0002 | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932 | 0.0032 | 0.0001\n[EMPTY] | PT | Ted Talks | 2.10-5 | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887 | 0.1390 | 0.0002\nF | EN | Europarl | 0.0073 | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033 | 0.0006\nF | EN | Ted Talks | 0.0004 | 0.0132 | 0.0118 | 0.0181 | 0.0179 | [BOLD] 0.0520 | 0.0005\nF | PT | Europarl | 0.0005 | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109 | 0.0064 | 0.0002\n[EMPTY] | PT | Ted Talks | 4.10-5 | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149 | 0.2261 | 0.0004\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "447eeb5c-f007-4096-b630-ce70255d1c14",
    "input": "## Claim\nHere is a claim: [CONTINUE] The results of CLUSTER+KCP again indicate that pre-clustering of documents to topics is beneficial, improving upon the KCP performance by 4.6 points, though still performing substantially worse than our joint model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\n<bold>Baselines</bold> | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nCluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5\nCV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73\nKCP Kenyon-Dean et\u00a0al. (<ref id='bib-bib14'>2018</ref>) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69\nCluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6\n<bold>Model Variants</bold> | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nDisjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5\nJoint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | <bold>79.5</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b920ceff-a2e2-4cc1-877c-73c2b3404336",
    "input": "## Claim\nHere is a claim: Also, the average human rating for Refresh is not significantly higher (p (cid:28) 0.01) than ExtAbsRL. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "705d2c1b-ce84-4754-b506-76987159316d",
    "input": "## Claim\nHere is a claim: This is because word representation learning approaches, like OIWE-IPG and SOV, do not consider the semantic distance learning issue, which has a significant impact on word similarity task. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VI: Correlations for Word Similarity Tests\nDataset (EN-) | GloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\nWS-353-ALL | 0.612 | 0.7156 | 0.634 | 0.622 | 0.173 | 0.690 | 0.657\nSIMLEX-999 | 0.359 | 0.3939 | 0.295 | 0.355 | 0.090 | 0.380 | 0.381\nVERB-143 | 0.326 | 0.4430 | 0.255 | 0.271 | 0.293 | 0.271 | 0.348\nSimVerb-3500 | 0.193 | 0.2856 | 0.184 | 0.197 | 0.035 | 0.234 | 0.245\nWS-353-REL | 0.578 | 0.6457 | 0.595 | 0.578 | 0.134 | 0.695 | 0.619\nRW-STANF. | 0.378 | 0.4858 | 0.316 | 0.373 | 0.122 | 0.390 | 0.382\nYP-130 | 0.524 | 0.5211 | 0.353 | 0.482 | 0.169 | 0.420 | 0.589\nMEN-TR-3k | 0.710 | 0.7528 | 0.684 | 0.696 | 0.298 | 0.769 | 0.725\nRG-65 | 0.768 | 0.8051 | 0.736 | 0.732 | 0.338 | 0.761 | 0.774\nMTurk-771 | 0.650 | 0.6712 | 0.593 | 0.623 | 0.199 | 0.665 | 0.671\nWS-353-SIM | 0.682 | 0.7883 | 0.713 | 0.702 | 0.220 | 0.720 | 0.720\nMC-30 | 0.749 | 0.8112 | 0.799 | 0.726 | 0.330 | 0.735 | 0.776\nMTurk-287 | 0.649 | 0.6645 | 0.591 | 0.631 | 0.295 | 0.674 | 0.634\nAverage | 0.552 | 0.6141 | 0.519 | 0.538 | 0.207 | 0.570 | 0.579\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "80fcba83-634a-4705-a314-22a36d228ec5",
    "input": "## Claim\nHere is a claim: For Yelp, M1 has better Acc and PP than M0 at comparable semantic similarity. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\n[EMPTY] | Acc | Sim | PP | GM\nM0: shen-1 | 0.818 | 0.719 | 37.3 | 10.0\nM1: M0 [ITALIC] +para | 0.819 | 0.734 | 26.3 | 14.2\nM2: M0 [ITALIC] +cyc | 0.813 | 0.770 | 36.4 | 18.8\nM3: M0 [ITALIC] +cyc+lang | 0.807 | 0.796 | 28.4 | 21.5\nM4: M0 [ITALIC] +cyc+para | 0.798 | 0.783 | 39.7 | 19.2\nM5: M0 [ITALIC] +cyc+para+lang | 0.804 | 0.785 | 27.1 | 20.3\nM6: M0 [ITALIC] +cyc+2d | 0.805 | [BOLD] 0.817 | 43.3 | 21.6\nM7: M6+ [ITALIC] para+lang | 0.818 | 0.805 | [BOLD] 29.0 | [BOLD] 22.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5ccbb007-877f-4e17-8513-9731641554db",
    "input": "## Claim\nHere is a claim: [CONTINUE] However, our data augmentation technique (NO-TRANSLATIONS) had a significant impact on the final score, reducing it by 0.84 points. Does the following context support or refute the claim?\n\n## Table\nPaper title: Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents\nTable caption: Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.\nID LSTM-800 | 5-fold CV 70.56 | \u0394 0.66 | Single model 67.54 | \u0394 0.78 | Ensemble 67.65 | \u0394 0.30\nLSTM-400 | 70.50 | 0.60 | [BOLD] 67.59 | 0.83 | [BOLD] 68.00 | 0.65\nIN-TITLE | 70.11 | 0.21 | [EMPTY] | [EMPTY] | 67.52 | 0.17\n[BOLD] SUBMISSION | 69.90 | \u2013 | 66.76 | \u2013 | 67.35 | \u2013\nNO-HIGHWAY | 69.72 | \u22120.18 | 66.42 | \u22120.34 | 66.64 | \u22120.71\nNO-OVERLAPS | 69.46 | \u22120.44 | 65.07 | \u22121.69 | 66.47 | \u22120.88\nLSTM-400-DROPOUT | 69.45 | \u22120.45 | 65.53 | \u22121.23 | 67.28 | \u22120.07\nNO-TRANSLATIONS | 69.42 | \u22120.48 | 65.92 | \u22120.84 | 67.23 | \u22120.12\nNO-ELMO-FINETUNING | 67.71 | \u22122.19 | 65.16 | \u22121.60 | 65.42 | \u22121.93\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "83c12d32-c4f9-43d4-bfeb-ca7dd1f991a0",
    "input": "## Claim\nHere is a claim: both (Nguyen et al., 2016) and ours pre-compute a vocabulary of top-K possible responses, which are used as the only acceptable responses. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.\nModel | Diversity | App | Good% | OK% | Invalid%\nDAMD | 3.12 | 2.50 | 56.5% | [BOLD] 37.4% | 6.1%\nDAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1% | 9.9%\nHDSA (+) | 2.14 | 2.47 | 57.5% | 32.5% | [BOLD] 10.0%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "7a281d71-ec5a-4086-bb9e-b1a04976b77b",
    "input": "## Claim\nHere is a claim: The full model gives 25.5 BLEU points on the AMR15 dev set. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\n-{4} dense block | 24.8 | 54.9\n-{3, 4} dense blocks | 23.8 | 54.1\n-{2, 3, 4} dense blocks | 23.2 | 53.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b87e736b-b577-4883-9cd3-271efb940ee7",
    "input": "## Claim\nHere is a claim: The semantic threshold for OD-d2v is set at 0.3 while for OD-w2v is set at 0.6. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.\nTopic Name | Size | TF-IDF ARI | WMD ARI | Sent2vec ARI | Doc2vec ARI | BERT ARI | [ITALIC] OD-w2v ARI | [ITALIC] OD-d2v ARI | TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil. | Sent2vec  [ITALIC] Sil. | Doc2vec  [ITALIC] Sil. | BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil. | [ITALIC] OD-d2v  [ITALIC] Sil.\nAffirmative Action | 81 | -0.07 | -0.02 | 0.03 | -0.01 | -0.02 | [BOLD] 0.14 | [ITALIC] 0.02 | 0.01 | 0.01 | -0.01 | -0.02 | -0.04 | [BOLD] 0.06 | [ITALIC] 0.01\nAtheism | 116 | [BOLD] 0.19 | 0.07 | 0.00 | 0.03 | -0.01 | 0.11 | [ITALIC] 0.16 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | [ITALIC] 0.05 | [BOLD] 0.07\nAusterity Measures | 20 | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01 | -0.05 | 0.04 | [BOLD] 0.21 | -0.01 | 0.06 | 0.07 | 0.05 | -0.03 | 0.10 | [BOLD] 0.19 | 0.1\nDemocratization | 76 | 0.02 | -0.01 | 0.00 | [ITALIC] 0.09 | -0.01 | [BOLD] 0.11 | 0.07 | 0.01 | 0.01 | 0.02 | 0.02 | 0.03 | [BOLD] 0.16 | [ITALIC] 0.11\nEducation Voucher Scheme | 30 | [BOLD] 0.25 | 0.12 | 0.08 | -0.02 | 0.04 | 0.13 | [ITALIC] 0.19 | 0.01 | 0.01 | 0.01 | -0.01 | 0.02 | [ITALIC] 0.38 | [BOLD] 0.40\nGambling | 60 | -0.06 | -0.01 | -0.02 | 0.04 | 0.09 | [ITALIC] 0.35 | [BOLD] 0.39 | 0.01 | 0.02 | 0.03 | 0.01 | 0.09 | [BOLD] 0.30 | [ITALIC] 0.22\nHousing | 30 | 0.01 | -0.01 | -0.01 | -0.02 | 0.08 | [BOLD] 0.27 | 0.01 | 0.02 | 0.03 | 0.03 | 0.01 | 0.11 | [BOLD] 0.13 | [ITALIC] 0.13\nHydroelectric Dams | 110 | [BOLD] 0.47 | [ITALIC] 0.45 | [ITALIC] 0.45 | -0.01 | 0.38 | 0.35 | 0.14 | 0.04 | 0.08 | 0.12 | 0.01 | 0.19 | [BOLD] 0.26 | [ITALIC] 0.09\nIntellectual Property | 66 | 0.01 | 0.01 | 0.00 | 0.03 | 0.03 | [ITALIC] 0.05 | [BOLD] 0.14 | 0.01 | [ITALIC] 0.04 | 0.03 | 0.01 | 0.03 | [ITALIC] 0.04 | [BOLD] 0.12\nKeystone pipeline | 18 | 0.01 | 0.01 | 0.00 | -0.13 | [BOLD] 0.07 | -0.01 | [BOLD] 0.07 | -0.01 | -0.03 | -0.03 | -0.07 | 0.03 | [BOLD] 0.05 | [ITALIC] 0.02\nMonarchy | 61 | -0.04 | 0.01 | 0.00 | 0.03 | -0.02 | [BOLD] 0.15 | [BOLD] 0.15 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 | [BOLD] 0.11 | [ITALIC] 0.09\nNational Service | 33 | 0.14 | -0.03 | -0.01 | 0.02 | 0.01 | [ITALIC] 0.31 | [BOLD] 0.39 | 0.02 | 0.04 | 0.02 | 0.01 | 0.02 | [BOLD] 0.25 | [BOLD] 0.25\nOne-child policy China | 67 | -0.05 | 0.01 | [BOLD] 0.11 | -0.02 | 0.02 | [BOLD] 0.11 | 0.01 | 0.01 | 0.02 | [ITALIC] 0.04 | -0.01 | 0.03 | [BOLD] 0.07 | -0.02\nOpen-source Software | 48 | -0.02 | -0.01 | [ITALIC] 0.05 | 0.01 | 0.12 | [BOLD] 0.09 | -0.02 | 0.01 | -0.01 | 0.00 | -0.02 | 0.03 | [BOLD] 0.18 | 0.01\nPornography | 52 | -0.02 | 0.01 | 0.01 | -0.02 | -0.01 | [BOLD] 0.41 | [BOLD] 0.41 | 0.01 | 0.01 | 0.02 | -0.01 | 0.03 | [BOLD] 0.47 | [ITALIC] 0.41\nSeanad Abolition | 25 | 0.23 | 0.09 | -0.01 | -0.01 | 0.03 | [ITALIC] 0.32 | [BOLD] 0.54 | 0.02 | 0.01 | -0.01 | -0.03 | -0.04 | [ITALIC] 0.15 | [BOLD] 0.31\nTrades Unions | 19 | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60 | -0.05 | 0.44 | [ITALIC] 0.44 | 0.29 | 0.1 | 0.17 | 0.21 | 0.01 | 0.26 | [BOLD] 0.48 | [ITALIC] 0.32\nVideo Games | 72 | -0.01 | 0.01 | 0.12 | 0.01 | 0.08 | [ITALIC] 0.40 | [BOLD] 0.56 | 0.01 | 0.01 | 0.06 | 0.01 | 0.05 | [ITALIC] 0.32 | [BOLD] 0.42\nAverage | 54.67 | 0.09 | 0.07 | 0.08 | 0.01 | 0.08 | [BOLD] 0.22 | [ITALIC] 0.20 | 0.02 | 0.03 | 0.04 | -0.01 | 0.05 | [BOLD] 0.20 | [ITALIC] 0.17\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5a893e75-277d-48d6-abe4-2d1757a04b02",
    "input": "## Claim\nHere is a claim: This suggests that graph encoders based on gating mechanisms are not as effective as other models in text generation models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "bb0c415e-fa38-4ace-aa92-8f8480f1b2ab",
    "input": "## Claim\nHere is a claim: However, when gold PP attachment are used, we note a large potential improve [CONTINUE] ment of 10.46 points in PP attachment accuracies (between the PPA accuracy for RBG and RBG + Oracle PP), which confirms that adding PP predictions as features is an effective approach. Does the following context support or refute the claim?\n\n## Table\nPaper title: Ontology-Aware Token Embeddings for Prepositional Phrase Attachment\nTable caption: Table 2: Results from RBG dependency parser with features coming from various PP attachment predictors and oracle attachments.\n[BOLD] System | [BOLD] Full UAS | [BOLD] PPA Acc.\nRBG | 94.17 | 88.51\nRBG + HPCD (full) | 94.19 | 89.59\nRBG + LSTM-PP | 94.14 | 86.35\nRBG + OntoLSTM-PP | 94.30 | 90.11\nRBG + Oracle PP | 94.60 | 98.97\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "05fde2b1-5561-41b9-b324-49eb1967cf32",
    "input": "## Claim\nHere is a claim: The human evaluation shows that our mirrored instances are not as difficult as the original ones (see Table 3). Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.\nDataset | Accuracy | Fleiss\u2019 kappa  [ITALIC] k\nOriginal COPA | 100.0 | 0.973\nBalanced COPA | 97.0 | 0.798\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4bc3daec-347b-482d-9818-20213bdeba76",
    "input": "## Claim\nHere is a claim: However, the drop in performance on the QA-SRL task, from which the model's weights are initialized, is much smaller with BIDAF (ELMO) than MQAN, and this corroborates the idea that contextualized ELMo representations, which benefit from general pre-training and are transferred to the task using the fine-tuning paradigm, are more amenable for achieving cross-dataset generalization. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0fdec0b4-dd5c-40da-b23e-43bf04c01a24",
    "input": "## Claim\nHere is a claim: The PRKGC model gives considerably good results, which indicates the non-triviality of RC-QEDE. Does the following context support or refute the claim?\n\n## Table\nPaper title: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension\nTable caption: Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).\nModel | Answerability Macro P/R/F | # Answerable | Answer Prec. | Derivation Prec. RG-L (P/R/F) | Derivation Prec. BL-4\nShortest Path | 54.8/55.5/53.2 | 976 | 3.6 | 56.7/38.5/41.5 | 31.3\nPRKGC | 52.6/51.5/50.7 | 1,021 | 45.2 | 40.7/60.7/44.7 | 30.9\nPRKGC+NS | 53.6/54.1/52.1 | 980 | 45.4 | 42.2/61.6/46.1 | 33.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "cb23a665-93d6-4923-befd-01b39b00e674",
    "input": "## Claim\nHere is a claim: We hypothesize that the gating mechanism cannot better capture long-distance dependencies between nodes far apart in the graph. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6241fc50-d27d-46d7-9be1-a96d6b384e0e",
    "input": "## Claim\nHere is a claim: When trained on the NC-v11 subset, the gap between Seq2seq and Dual2seq under BLEU (around 3 points) is greater than that under Meteor (around 5 points). Does the following context support or refute the claim?\n\n## Table\nPaper title: Semantic Neural Machine Translation using AMR\nTable caption: Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.\nSystem | NC-v11 BLEU | NC-v11 TER\u2193 | NC-v11 Meteor | Full BLEU | Full TER\u2193 | Full Meteor\nOpenNMT-tf | 15.1 | 0.6902 | 0.3040 | 24.3 | 0.5567 | 0.4225\nTransformer-tf | 17.1 | 0.6647 | 0.3578 | 25.1 | 0.5537 | 0.4344\nSeq2seq | 16.0 | 0.6695 | 0.3379 | 23.7 | 0.5590 | 0.4258\nDual2seq-LinAMR | 17.3 | 0.6530 | 0.3612 | 24.0 | 0.5643 | 0.4246\nDuel2seq-SRL | 17.2 | 0.6591 | 0.3644 | 23.8 | 0.5626 | 0.4223\nDual2seq-Dep | 17.8 | 0.6516 | 0.3673 | 25.0 | 0.5538 | 0.4328\nDual2seq | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840 | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "f5755c32-6e1b-4528-93b2-e775520cd8c0",
    "input": "## Claim\nHere is a claim: Without knowledge of the input systems, the score of MUC-B1, which most closely follows the MUC scoring methodology (Vilain et al., 1995), was higher than MUC-B1. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n[BOLD] Model | R | MUC P | [ITALIC] F1 | R | B3 P | [ITALIC] F1 | R | CEAF- [ITALIC] e P | [ITALIC] F1 | CoNLL  [ITALIC] F1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | [BOLD] 71.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "d170dc40-b5d5-403a-8e9e-c3be389759ad",
    "input": "## Claim\nHere is a claim: At the same time, the distributional information embedded into the network appears to have acted as a stabilizing force. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "b3fc9434-7e2f-467b-823c-6e91b5ce5db4",
    "input": "## Claim\nHere is a claim: We suspect that two reasons for the performance drop on booking hotels are 1) the vocabularies of booking hotels are more similar to that of others than of booking flights or restaurants, making it easier to say keywords unintentionally and 2) users tend to vary more in asking details on booking a hotel, Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "4ac59499-db43-4949-aa71-72b2deaffc8a",
    "input": "## Claim\nHere is a claim: The gap has become larger when the threshold becomes smaller, since there is much more noises when the score becomes smaller, our capsule net and word-level attention models are more robust to these noises. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\n-Word-ATT | 0.648 | 0.515 | 0.395 | 0.389\n-Capsule | 0.635 | 0.507 | 0.413 | 0.386\nOur Model | 0.650 | 0.519 | 0.422 | 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e042f4df-12c4-467c-95bd-5043fd5e178e",
    "input": "## Claim\nHere is a claim: The CS-ONLY-DISCRIMINATIVE model is able to prioritize the gold sentence better than all other models, under both conditions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training\nTable caption: Table 5: Accuracy on the dev set and on the test set, according to the type of the gold sentence in the set: code-switched (CS) vs. monolingual (mono).\n[EMPTY] | dev CS | dev mono | test CS | test mono\nCS-only-LM | 45.20 | 65.87 | 43.20 | 62.80\nFine-Tuned-LM | 49.60 | 72.67 | 47.60 | 71.33\nCS-only-disc | [BOLD] 75.60 | 70.40 | 70.80 | 70.53\nFine-Tuned-disc | 70.80 | [BOLD] 74.40 | [BOLD] 75.33 | [BOLD] 75.87\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4180aa48-324e-4fc1-89e9-f95ee5717491",
    "input": "## Claim\nHere is a claim: For example, we take the triple (nele, type, nele). Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 2: Precisions on the Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nRank+ExATT | 0.584 | 0.535 | 0.487 | 0.392\nPCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204\nPCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396\nOur Model | 0.650 | 0.519 | 0.422 | [BOLD] 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6532fb08-f821-4080-912e-391b0e279557",
    "input": "## Claim\nHere is a claim: Increasing the window size to 10 increases the F1 score marginally (A3\u2212A4). Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.\n[EMPTY] | Prec. | Rec. | F1\n(A1) BiLSTM-CNN | 0.473 | 0.606 | 0.531\n(A2) Standard attention | 0.466 | 0.638 | 0.539\n(A3) Window size ( [ITALIC] ws)=5 | 0.507 | 0.652 | [BOLD] 0.571\n(A4) Window size ( [ITALIC] ws)=10 | 0.510 | 0.640 | 0.568\n(A5) Softmax | 0.490 | 0.658 | 0.562\n(A6) Max-pool | 0.492 | 0.600 | 0.541\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ef19ef2f-f972-4231-b2ce-9604c2f0de42",
    "input": "## Claim\nHere is a claim: In contrast, the noise-aware model requires more iterations to converge. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Vector-spaces with Noisy Supervised Lexicons\nTable caption: Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En\u2192De, En\u2192Fi and En\u2192Es improvements are significant at p<0.05 according to ANOVA on the different runs.\nMethod | En\u2192It best | En\u2192It avg | En\u2192It iters | En\u2192De best | En\u2192De avg | En\u2192De iters | En\u2192Fi best | En\u2192Fi avg | En\u2192Fi iters | En\u2192Es best | En\u2192Es avg | En\u2192Es iters\nArtetxe et\u00a0al., 2018b | [BOLD] 48.53 | 48.13 | 573 | 48.47 | 48.19 | 773 | 33.50 | 32.63 | 988 | 37.60 | 37.33 | 808\nNoise-aware Alignment | [BOLD] 48.53 | [BOLD] 48.20 | 471 | [BOLD] 49.67 | [BOLD] 48.89 | 568 | [BOLD] 33.98 | [BOLD] 33.68 | 502 | [BOLD] 38.40 | [BOLD] 37.79 | 551\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "689f8a9c-3097-4448-b010-e9413fcaeabc",
    "input": "## Claim\nHere is a claim: [CONTINUE] For example, using relations generated by TF model using the Europarl corpus, we can understand the MaxDepth as having 789 terms with different values of term frequency, while having 211 that share the same value of term frequency with other terms. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.\nCorpus | Metric | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nEuroparl | TotalTerms: | 957 | 1,000 | 1,000 | 1,000 | 1,000 | 836 | 1,000\nEuroparl | TotalRoots: | 44 | 1 | 1 | 1 | 1 | 43 | 1\nEuroparl | NumberRels: | 1,588 | 1,025 | 1,028 | 1,185 | 1,103 | 1,184 | 999\nEuroparl | MaxDepth: | 21 | 921 | 901 | 788 | 835 | 8 | 15\nEuroparl | MinDepth: | 1 | 921 | 901 | 788 | 835 | 1 | 1\nEuroparl | AvgDepth: | 11.82 | 921 | 901 | 788 | 835 | 3.05 | 8.46\nEuroparl | DepthCohesion: | 1.78 | 1 | 1 | 1 | 1 | 2.62 | 1.77\nEuroparl | MaxWidth: | 20 | 2 | 3 | 4 | 3 | 88 | 41\nEuroparl | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nEuroparl | AvgWidth: | 1.99 | 1.03 | 1.03 | 1.19 | 1.10 | 4.20 | 2.38\nTED Talks | TotalTerms: | 476 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000\nTED Talks | TotalRoots: | 164 | 2 | 1 | 1 | 1 | 1 | 1\nTED Talks | NumberRels: | 521 | 1,029 | 1,331 | 3,025 | 3,438 | 3,802 | 1,009\nTED Talks | MaxDepth: | 16 | 915 | 658 | 454 | 395 | 118 | 12\nTED Talks | MinDepth: | 1 | 913 | 658 | 454 | 395 | 110 | 1\nTED Talks | AvgDepth: | 5.82 | 914 | 658 | 454 | 395 | 112.24 | 5.95\nTED Talks | DepthCohesion: | 2.75 | 1 | 1 | 1 | 1 | 1.05 | 2.02\nTED Talks | MaxWidth: | 25 | 2 | 77 | 13 | 12 | 66 | 98\nTED Talks | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nTED Talks | AvgWidth: | 1.83 | 1.03 | 1.36 | 3.03 | 3.44 | 6.64 | 2.35\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f8dc8a50-467a-4ac0-8ec0-796d2151a87d",
    "input": "## Claim\nHere is a claim: [CONTINUE] For both datasets, our approach substantially outperforms the baselines. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ae9d1040-1c3b-4d4b-8743-4b74fea43a96",
    "input": "## Claim\nHere is a claim: We additionally find that supervised BLEU shows a trade-off with Acc: for a single model type, higher Acc generally corresponds to lower BLEU. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 6: Results on Yelp sentiment transfer, where BLEU is between 1000 transferred sentences and human references, and Acc is restricted to the same 1000 sentences. Our best models (right table) achieve higher BLEU than prior work at similar levels of Acc, but untransferred sentences achieve the highest BLEU. Acc\u2217: the definition of Acc varies by row because of different classifiers in use. Other results from simple-transfer are not included as they are worse.\nModel | BLEU | Acc\u2217\nfu-1 | [EMPTY] | [EMPTY]\nMulti-decoder | 7.6 | 0.792\nStyle embed. | 15.4 | 0.095\nsimple-transfer | simple-transfer | simple-transfer\nTemplate | 18.0 | 0.867\nDelete/Retrieve | 12.6 | 0.909\nyang2018unsupervised | yang2018unsupervised | yang2018unsupervised\nLM | 13.4 | 0.854\nLM + classifier | [BOLD] 22.3 | 0.900\nUntransferred | [BOLD] 31.4 | 0.024\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "dd0fd46f-faae-4f40-bace-d38e01bbf8de",
    "input": "## Claim\nHere is a claim: RoBERTa, due to its optimizations and higher training data, outperforms the other models by a significant margin, indicating the large potential for models trained on much larger data Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See \u00a72 for model details. * indicates our replication experiments.\nModel | Accuracy\nBigramPMI\u00a0Goodwin et al. ( 2012 ) | 63.4\nPMI\u00a0Gordon et al. ( 2011 ) | 65.4\nPMI+Connectives\u00a0Luo et al. ( 2016 ) | 70.2\nPMI+Con.+Phrase\u00a0Sasaki et al. ( 2017 ) | 71.4\nBERT-large\u00a0Wang et al. ( 2019 ) | 70.5\nBERT-large\u00a0Sap et al. ( 2019 ) | 75.0\nBERT-large\u00a0Li et al. ( 2019 ) | 75.4\nRoBERTa-large (finetuned) | 90.6\nBERT-large (finetuned)* | 76.5 \u00b1 2.7\nRoBERTa-large (finetuned)* | 87.7 \u00b1 0.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6c15ac43-fcb9-4598-a50f-607a89c8074f",
    "input": "## Claim\nHere is a claim: Overall results show that ATR achieves the best performance and consumes the least training time. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nModel | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time\nRockt\u00e4schel et\u00a0al. ( 2016 ) | Rockt\u00e4schel et\u00a0al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | -\nThis | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 | [BOLD] 90.49 | 0.696\nThis | GRU | 6.41M | [BOLD] 85.71 | 0.245 | [BOLD] 86.05 | 0.419 | [BOLD] 90.29 | 0.529 | 90.10 | 0.695\nThis | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580\nWork | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555\n[EMPTY] | LRN | 4.25M | 84.88 | [BOLD] 0.209 | 85.06 | [BOLD] 0.223 | 89.98 | [BOLD] 0.488 | 89.93 | [BOLD] 0.506\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4726d56c-a183-45f6-ada9-df0b103a0e3e",
    "input": "## Claim\nHere is a claim: Therefore, our method covers most contexts where \u201cto\u201d is an Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nCue | App. | Prod. | Cov.\nin | 47 | 55.3 | 9.40\nwas | 55 | 61.8 | 11.0\nto | 82 | 40.2 | 16.4\nthe | 85 | 38.8 | 17.0\na | 106 | 57.5 | 21.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "4534055e-da0f-4973-ac32-54f5b588cf48",
    "input": "## Claim\nHere is a claim: GDPL achieves extremely high performance in the task success on account of the substantial improvement in inform F1 and match rate over the baselines. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a7929669-2be1-4529-b9c0-91a29db43a73",
    "input": "## Claim\nHere is a claim: 2018b; Dong et\\xa0al. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "32381fb3-5662-4d51-bf38-51a4d3b1023b",
    "input": "## Claim\nHere is a claim: [CONTINUE] In Librispeech + DEMAND, minimizing DCE (15.8%) and FSEGAN (14.9%) achieves a lower WER than acoustic supervision (15.6%) and multi-task learning (14.4%). Does the following context support or refute the claim?\n\n## Table\nPaper title: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition\nTable caption: TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set\nMethod | WER (%) | DCE\nNo enhancement | 17.3 | 0.828\nWiener filter | 19.5 | 0.722\nMinimizing DCE | 15.8 | [BOLD] 0.269\nFSEGAN | 14.9 | 0.291\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=0) | 15.6 | 0.330\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 14.4 | 0.303\nClean speech | 5.7 | 0.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "bfa7b13c-5cb0-4467-ae28-9a92c6efd558",
    "input": "## Claim\nHere is a claim: For window-based w2 contexts POS disambiguation yields significantly better F scores on lemmatized targets for VN (p \u2264 .005) with borderline significance for WN-N and WN-V (p \u2248 .05). Does the following context support or refute the claim?\n\n## Table\nPaper title: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources\nTable caption: Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.\n[EMPTY] | WN-N P | WN-N R | WN-N F | WN-V P | WN-V R | WN-V F | VN P | VN R | VN F\nContext: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2\ntype | .700 | .654 | .676 | .535 | .474 | .503 | .327 | .309 | .318\nx+POS | .699 | .651 | .674 | .544 | .472 | .505 | .339 | .312 | .325\nlemma | .706 | .660 | .682 | .576 | .520 | .547 | .384 | .360 | .371\nx+POS | <bold>.710</bold> | <bold>.662</bold> | <bold>.685</bold> | <bold>.589</bold> | <bold>.529</bold> | <bold>.557</bold> | <bold>.410</bold> | <bold>.389</bold> | <bold>.399</bold>\nContext: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep\ntype | .712 | .661 | .686 | .545 | .457 | .497 | .324 | .296 | .310\nx+POS | .715 | .659 | .686 | .560 | .464 | .508 | .349 | .320 | .334\nlemma | <bold>.725</bold> | <bold>.668</bold> | <bold>.696</bold> | .591 | .512 | .548 | .408 | .371 | .388\nx+POS | .722 | .666 | .693 | <bold>.609</bold> | <bold>.527</bold> | <bold>.565</bold> | <bold>.412</bold> | <bold>.381</bold> | <bold>.396</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c8ffde58-4d95-40e3-aa21-fbdbe463b9dd",
    "input": "## Claim\nHere is a claim: In Italian, we get an increase of 91.67% of the gap with respect to English. Does the following context support or refute the claim?\n\n## Table\nPaper title: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?\nTable caption: Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. \u201cReduction\u201d stands for gap reduction when removing gender signals from the context.\n[EMPTY] | Italian Original | Italian Debiased | Italian English | Italian Reduction | German Original | German Debiased | German English | German Reduction\nSame Gender | 0.442 | 0.434 | 0.424 | \u2013 | 0.491 | 0.478 | 0.446 | \u2013\nDifferent Gender | 0.385 | 0.421 | 0.415 | \u2013 | 0.415 | 0.435 | 0.403 | \u2013\ndifference | 0.057 | 0.013 | 0.009 | [BOLD] 91.67% | 0.076 | 0.043 | 0.043 | [BOLD] 100%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9846b931-84f9-407f-9a32-b37c96b7c9f1",
    "input": "## Claim\nHere is a claim: [CONTINUE] To validate Acc, human annotators were asked to judge the style of 100 transferred sentences [CONTINUE] We then compute the percentage of machine and human judgments that match. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.\nMetric | Method of validation | Yelp | Lit.\nAcc | % of machine and human judgments that match | 94 | 84\nSim | Spearman\u2019s  [ITALIC] \u03c1 b/w Sim and human ratings of semantic preservation | 0.79 | 0.75\nPP | Spearman\u2019s  [ITALIC] \u03c1 b/w negative PP and human ratings of fluency | 0.81 | 0.67\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "e8f0310f-c946-4b95-88b3-d257d8ea56f7",
    "input": "## Claim\nHere is a claim: Additionally, the ensemble DCGCN models achieve 20.5 and 13.1 BLEU points on the En-De and En-Cs tasks, respectively. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 4: Main results on English-German and English-Czech datasets.\n[BOLD] Model | [BOLD] Type | [BOLD] English-German #P | [BOLD] English-German B | [BOLD] English-German C | [BOLD] English-Czech #P | [BOLD] English-Czech B | [BOLD] English-Czech C\nBoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | -\nCNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | -\nBiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | -\nPB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4\nSeq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8\nGGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3\nDCGCN (ours) | Single | [BOLD]  29.7M | [BOLD] 19.0 | [BOLD] 44.1 | [BOLD]  28.3M | [BOLD] 12.1 | [BOLD] 37.1\nSeq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4\nGGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9\nDCGCN (ours) | Ensemble | [BOLD]  149M | [BOLD] 20.5 | [BOLD] 45.8 | [BOLD]  142M | [BOLD] 13.1 | [BOLD] 37.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b9c0aac4-ab15-4aa4-b7a2-2782a675158e",
    "input": "## Claim\nHere is a claim: HDSA shows the effectiveness of explicitly capturing intent and dialog history. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "a84bc7ed-7512-4465-8dc1-256e680d2065",
    "input": "## Claim\nHere is a claim: However, the greatest performance increase is seen for the last scenario, which underscores the extent to which the semantic features captured by embeddings can be improved with a reasonable selection of the lexical resource from which the concept wordgroups were derived. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VIII: Precision scores for the Semantic Analogy Test\nQuestions Subset | # of Questions Seen | GloVe | Word2Vec | Proposed\nAll | 8783 | 78.94 | 81.03 | 79.96\nAt least one | 1635 | 67.58 | 70.89 | 67.89\nconcept word | 1635 | 67.58 | 70.89 | 67.89\nAll concept words | 110 | 77.27 | 89.09 | 83.64\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "401dfb4f-8750-4c42-bc8d-b0d5942f798f",
    "input": "## Claim\nHere is a claim: Overall results show that LRN achieves competitive performance but consumes the least training time. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nModel | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time\nRockt\u00e4schel et\u00a0al. ( 2016 ) | Rockt\u00e4schel et\u00a0al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | -\nThis | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 | [BOLD] 90.49 | 0.696\nThis | GRU | 6.41M | [BOLD] 85.71 | 0.245 | [BOLD] 86.05 | 0.419 | [BOLD] 90.29 | 0.529 | 90.10 | 0.695\nThis | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580\nWork | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555\n[EMPTY] | LRN | 4.25M | 84.88 | [BOLD] 0.209 | 85.06 | [BOLD] 0.223 | 89.98 | [BOLD] 0.488 | 89.93 | [BOLD] 0.506\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f7a0d205-c70c-45a4-ad95-ee004bb48b14",
    "input": "## Claim\nHere is a claim: Interestingly, G2S-GGNN has better performance among our models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "64535f27-4dd7-4691-827f-a7633eb8c941",
    "input": "## Claim\nHere is a claim: with respect to  the efficiency criteria, in which task the dialog systems take shorter time to reach the successful termination in an average and the total dialog time is shorter when averaged across all dialog sessions, the trend shows that all dialog methods have a strong tendency to increase dialog latency with time. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "4c929b50-4a25-4aa9-a703-75aa75de0325",
    "input": "## Claim\nHere is a claim: The mechanism successfully alleviates the over-fitting issue caused by the imbalanced two tasks\u2019 sizes. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c0e96242-c3ea-48c3-a932-693d83be5c5c",
    "input": "## Claim\nHere is a claim: [CONTINUE] Surprisingly, GDPL even outperforms human in completing the task, and its average dialog turns are close to those of humans, though GDPL is inferior in terms of match rate. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "922444a0-578f-4b72-a5f4-e457f6e26693",
    "input": "## Claim\nHere is a claim: HAN models outperform both LogReg and SVM using the current set of features. Does the following context support or refute the claim?\n\n## Table\nPaper title: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks\nTable caption: Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\n[BOLD] System | [BOLD] ROUGE-1  [BOLD] R (%) | [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%) | [BOLD] ROUGE-2  [BOLD] R (%) | [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%) | [BOLD] Sentence-Level  [BOLD] R (%) | [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)\n[BOLD] ILP | 24.5 | 41.1 | 29.3\u00b10.5 | 7.9 | 15.0 | 9.9\u00b10.5 | 13.6 | 22.6 | 15.6\u00b10.4\n[BOLD] Sum-Basic | 28.4 | 44.4 | 33.1\u00b10.5 | 8.5 | 15.6 | 10.4\u00b10.4 | 14.7 | 22.9 | 16.7\u00b10.5\n[BOLD] KL-Sum | 39.5 | 34.6 | 35.5\u00b10.5 | 13.0 | 12.7 | 12.3\u00b10.5 | 15.2 | 21.1 | 16.3\u00b10.5\n[BOLD] LexRank | 42.1 | 39.5 | 38.7\u00b10.5 | 14.7 | 15.3 | 14.2\u00b10.5 | 14.3 | 21.5 | 16.0\u00b10.5\n[BOLD] MEAD | 45.5 | 36.5 | 38.5\u00b1 0.5 | 17.9 | 14.9 | 15.4\u00b10.5 | 27.8 | 29.2 | 26.8\u00b10.5\n[BOLD] SVM | 19.0 | 48.8 | 24.7\u00b10.8 | 7.5 | 21.1 | 10.0\u00b10.5 | 32.7 | 34.3 | 31.4\u00b10.4\n[BOLD] LogReg | 26.9 | 34.5 | 28.7\u00b10.6 | 6.4 | 9.9 | 7.3\u00b10.4 | 12.2 | 14.9 | 12.7\u00b10.5\n[BOLD] LogReg [ITALIC] r | 28.0 | 34.8 | 29.4\u00b10.6 | 6.9 | 10.4 | 7.8\u00b10.4 | 12.1 | 14.5 | 12.5\u00b10.5\n[BOLD] HAN | 31.0 | 42.8 | 33.7\u00b10.7 | 11.2 | 17.8 | 12.7\u00b10.5 | 26.9 | 34.1 | 32.4\u00b10.5\n[BOLD] HAN+pretrainT | 32.2 | 42.4 | 34.4\u00b10.7 | 11.5 | 17.5 | 12.9\u00b10.5 | 29.6 | 35.8 | 32.2\u00b10.5\n[BOLD] HAN+pretrainU | 32.1 | 42.1 | 33.8\u00b10.7 | 11.6 | 17.6 | 12.9\u00b10.5 | 30.1 | 35.6 | 32.3\u00b10.5\n[BOLD] HAN [ITALIC] r | 38.1 | 40.5 | [BOLD] 37.8\u00b10.5 | 14.0 | 17.1 | [BOLD] 14.7\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainT [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.5 | 16.8 | [BOLD] 14.4\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainU [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.6 | 16.9 | [BOLD] 14.4\u00b10.5 | 33.9 | 33.8 | [BOLD] 33.8\u00b10.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "11b3b856-eb9c-4249-a21b-9c084802ad70",
    "input": "## Claim\nHere is a claim: Table 1: In all language pairs, the best correlation is not achieved by our word mover metrics that use a BERT pretrained on MNLI as the embedding generator and PMeans to aggregate the embeddings from different BERT layers, i.e., WMD-1/2+BERT+MNLI+PMeans. Does the following context support or refute the claim?\n\n## Table\nPaper title: MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance\nTable caption: Table 1: Absolute Pearson correlations with segment-level human judgments in 7 language pairs on WMT17 dataset.\nSetting | Metrics | <bold>Direct Assessment</bold> cs-en | <bold>Direct Assessment</bold> de-en | <bold>Direct Assessment</bold> fi-en | <bold>Direct Assessment</bold> lv-en | <bold>Direct Assessment</bold> ru-en | <bold>Direct Assessment</bold> tr-en | <bold>Direct Assessment</bold> zh-en | <bold>Direct Assessment</bold> Average\nBaselines | METEOR++ | 0.552 | 0.538 | 0.720 | 0.563 | 0.627 | 0.626 | 0.646 | 0.610\nBaselines | RUSE(*) | 0.624 | 0.644 | 0.750 | 0.697 | 0.673 | 0.716 | 0.691 | 0.685\nBaselines | BERTScore-F1 | 0.670 | 0.686 | 0.820 | 0.710 | 0.729 | 0.714 | 0.704 | 0.719\nSent-Mover | Smd + W2V | 0.438 | 0.505 | 0.540 | 0.442 | 0.514 | 0.456 | 0.494 | 0.484\nSent-Mover | Smd + ELMO + PMeans | 0.569 | 0.558 | 0.732 | 0.525 | 0.581 | 0.620 | 0.584 | 0.595\nSent-Mover | Smd + BERT + PMeans | 0.607 | 0.623 | 0.770 | 0.639 | 0.667 | 0.641 | 0.619 | 0.652\nSent-Mover | Smd + BERT + MNLI + PMeans | 0.616 | 0.643 | 0.785 | 0.660 | 0.664 | 0.668 | 0.633 | 0.667\nWord-Mover | Wmd-1 + W2V | 0.392 | 0.463 | 0.558 | 0.463 | 0.456 | 0.485 | 0.481 | 0.471\nWord-Mover | Wmd-1 + ELMO + PMeans | 0.579 | 0.588 | 0.753 | 0.559 | 0.617 | 0.679 | 0.645 | 0.631\nWord-Mover | Wmd-1 + BERT + PMeans | 0.662 | 0.687 | 0.823 | 0.714 | 0.735 | 0.734 | 0.719 | 0.725\nWord-Mover | Wmd-1 + BERT + MNLI + PMeans | 0.670 | 0.708 | <bold>0.835</bold> | <bold>0.746</bold> | <bold>0.738</bold> | 0.762 | <bold>0.744</bold> | <bold>0.743</bold>\nWord-Mover | Wmd-2 + BERT + MNLI + PMeans | <bold>0.679</bold> | <bold>0.710</bold> | 0.832 | 0.745 | 0.736 | <bold>0.763</bold> | 0.740 | <bold>0.743</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "b8a62c5c-3087-4351-a5fd-9c2882dcdd08",
    "input": "## Claim\nHere is a claim: Our proposed method does not outperform GloVe in semantic analogy test set and in overall results, while GloVe performs slightly better in syntactic test set. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "70c37968-9955-4a49-83f6-a44b58ca002c",
    "input": "## Claim\nHere is a claim: Also in cross-document coreference, it achieves the best joint results, except for the CEAF metric. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | <bold>71.2</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6d7e29b8-084e-4ad5-8c41-e8975a7ddf6b",
    "input": "## Claim\nHere is a claim: BI and IS both individually outperform the oracle for all domains, [CONTINUE] With adaptive decoding, we can assume that a uniform ensemble will always perform better than a single model for any potentially unknown domain. Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.\n[BOLD] Decoder configuration | [BOLD] es-en  [BOLD] Health | [BOLD] es-en  [BOLD] Bio | [BOLD] en-de  [BOLD] News | [BOLD] en-de  [BOLD] TED | [BOLD] en-de  [BOLD] IT\nOracle model | 35.9 | 36.1 | 37.8 | 24.1 | 39.6\nUniform | 33.1 | 36.4 | 21.9 | 18.4 | 38.9\nIdentity-BI | 35.0 | 36.6 | 32.7 | 25.3 | 42.6\nBI | 35.9 | 36.5 | 38.0 | 26.1 | [BOLD] 44.7\nIS | [BOLD] 36.0 | 36.8 | 37.5 | 25.6 | 43.3\nBI + IS | [BOLD] 36.0 | [BOLD] 36.9 | [BOLD] 38.4 | [BOLD] 26.4 | [BOLD] 44.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3d29f63f-a0e4-422c-8299-a1e9ebd041b4",
    "input": "## Claim\nHere is a claim: The results show that coverage information does not improve the generalization of both examined models across various NLI datasets. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\n[EMPTY] | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK\nMQAN | 72.30 | 60.91 | 41.82 | 53.95\n+ coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold>\nESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37\n+ coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "bcd7aad2-e4b4-4978-ad9e-8aee69123d1c",
    "input": "## Claim\nHere is a claim: [CONTINUE] Further, contrary to intuition, the sob emoji contributes less than cry, despite representing a stronger emotion. Does the following context support or refute the claim?\n\n## Table\nPaper title: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations\nTable caption: Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.\n[BOLD] Emoji alias | [BOLD] N | [BOLD] emoji # | [BOLD] emoji % | [BOLD] no-emoji # | [BOLD] no-emoji % | [BOLD] \u0394%\nmask | 163 | 154 | 94.48 | 134 | 82.21 | - 12.27\ntwo_hearts | 87 | 81 | 93.10 | 77 | 88.51 | - 4.59\nheart_eyes | 122 | 109 | 89.34 | 103 | 84.43 | - 4.91\nheart | 267 | 237 | 88.76 | 235 | 88.01 | - 0.75\nrage | 92 | 78 | 84.78 | 66 | 71.74 | - 13.04\ncry | 116 | 97 | 83.62 | 83 | 71.55 | - 12.07\nsob | 490 | 363 | 74.08 | 345 | 70.41 | - 3.67\nunamused | 167 | 121 | 72.46 | 116 | 69.46 | - 3.00\nweary | 204 | 140 | 68.63 | 139 | 68.14 | - 0.49\njoy | 978 | 649 | 66.36 | 629 | 64.31 | - 2.05\nsweat_smile | 111 | 73 | 65.77 | 75 | 67.57 | 1.80\nconfused | 77 | 46 | 59.74 | 48 | 62.34 | 2.60\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "eab2c281-14b3-43ac-b1db-b83a227a2122",
    "input": "## Claim\nHere is a claim: Our text classifiers for identifying negation cues and finding the negation scope are built on top of a BERT classifier Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 3: Cue and token distribution in the conversational negation corpus.\nTotal negation cues | 2921\nTrue negation cues | 2674\nFalse negation cues | 247\nAverage scope length | 2.9\nAverage sentence length | 13.6\nAverage tweet length | 22.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "63d505e9-bb71-4a79-a21f-837a2d8402d8",
    "input": "## Claim\nHere is a claim: we see that in most cases fine-tuning on B-COPA does not help the models\u2019 performance, only their robustness. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large | B-COPA | 70.5 (\u00b1 2.5) | 72.6 (\u00b1 2.3) | [BOLD] 69.1 (\u00b1 2.7)\nBERT-large | B-COPA (50%) | 69.9 (\u00b1 1.9) | 71.2 (\u00b1 1.3) | 69.0 (\u00b1 3.5)\nBERT-large | COPA | [BOLD] 71.7 (\u00b1 0.5) | [BOLD] 80.5 (\u00b1 0.4) | 66.3 (\u00b1 0.8)\nRoBERTa-large | B-COPA | [BOLD] 76.7 (\u00b1 0.8) | 73.3 (\u00b1 1.5) | [BOLD] 78.8 (\u00b1 2.0)\nRoBERTa-large | B-COPA (50%) | 72.4 (\u00b1 2.0) | 72.1 (\u00b1 1.7) | 72.6 (\u00b1 2.1)\nRoBERTa-large | COPA | 76.4 (\u00b1 0.7) | [BOLD] 79.6 (\u00b1 1.0) | 74.4 (\u00b1 1.1)\nBERT-base-NSP | None | [BOLD] 66.4 | 66.2 | [BOLD] 66.7\nBERT-large-NSP | None | 65.0 | [BOLD] 66.9 | 62.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "b8dd43d6-532b-40e2-8758-5c8e8fd69a02",
    "input": "## Claim\nHere is a claim: However, CMOW generally outperforms CBOW embeddings. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nMethod | SUBJ | CR | MR | MPQA | MRPC | TREC | SICK-E | SST2 | SST5 | STS-B | SICK-R\nCBOW/784 | 90.0 | [BOLD] 79.2 | [BOLD] 74.0 | 87.1 | 71.6 | 85.6 | 78.9 | 78.5 | 42.1 | 61.0 | [BOLD] 78.1\nCMOW/784 | 87.5 | 73.4 | 70.6 | [BOLD] 87.3 | 69.6 | [BOLD] 88.0 | 77.2 | 74.7 | 37.9 | 56.5 | 76.2\nHybrid | [BOLD] 90.2 | 78.7 | 73.7 | [BOLD] 87.3 | [BOLD] 72.7 | 87.6 | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8\ncmp. CBOW | +0.2% | -0.6% | -0.4% | +0.2% | +1.5% | +2.3% | +0.6% | +1.4% | +2.9% | +3.9% | -0.4%\ncmp. CMOW | +3.1% | +7.2% | +4.4% | +0% | +4.5% | -0.5% | +2.9% | +6.7% | +14.3 | +12.2% | +2.1%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "89a48433-b8ad-489b-a96c-267e8f760ad5",
    "input": "## Claim\nHere is a claim: The results in the table suggest that cleaning the missing slots did not provide more complex training examples. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Original | TGen\u2212 | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94\nOriginal | [BOLD] Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27\nOriginal | [BOLD] Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80\nOriginal | [BOLD] Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen\u2212 | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37\nCleaned missing | [BOLD] Original | TGen\u2212 | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61\nCleaned missing | [BOLD] Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53\nCleaned missing | [BOLD] Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen\u2212 | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e2df8ba1-ef82-4c6d-9407-a8cd9c22a003",
    "input": "## Claim\nHere is a claim: [CONTINUE] Lemmatized targets generally perform better, with the boost being more pronounced on SimVerb. Does the following context support or refute the claim?\n\n## Table\nPaper title: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources\nTable caption: Table 1: Benchmark performance, Spearman\u2019s \u03c1. SGNS results with * taken from [morphfit]. Best results per column (benchmark) annotated for our setup only.\nContext: w2 | Context: w2 SimLex | Context: w2 SimLex | Context: w2 SimLex | Context: w2 SimLex | Context: w2 SimVerb\ntarget | N | V | A | all | V\ntype | .334 | <bold>.336</bold> | <bold>.518</bold> | .348 | .307\nx + POS | .342 | .323 | .513 | .350 | .279\nlemma | <bold>.362</bold> | .333 | .497 | <bold>.351</bold> | .400\nx + POS | .354 | <bold>.336</bold> | .504 | .345 | <bold>.406</bold>\n* type | - | - | - | .339 | .277\n* type MFit-A | - | - | - | .385 | -\n* type MFit-AR | - | - | - | .439 | .381\nContext: dep-W | Context: dep-W | Context: dep-W | Context: dep-W | Context: dep-W | Context: dep-W\ntype | .366 | .365 | .489 | .362 | .314\nx + POS | .364 | .351 | .482 | .359 | .287\nlemma | <bold>.391</bold> | .380 | <bold>.522</bold> | <bold>.379</bold> | .401\nx + POS | .384 | <bold>.388</bold> | .480 | .366 | <bold>.431</bold>\n* type | - | - | - | .376 | .313\n* type MFit-AR | - | - | - | .434 | .418\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ed79ce87-cdaa-4117-94e3-e9003fdbdb66",
    "input": "## Claim\nHere is a claim: When redundancy removal was applied to LogReg, it produces significant improvement. Does the following context support or refute the claim?\n\n## Table\nPaper title: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks\nTable caption: Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\n[BOLD] System | [BOLD] ROUGE-1  [BOLD] R (%) | [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%) | [BOLD] ROUGE-2  [BOLD] R (%) | [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%) | [BOLD] Sentence-Level  [BOLD] R (%) | [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)\n[BOLD] ILP | 24.5 | 41.1 | 29.3\u00b10.5 | 7.9 | 15.0 | 9.9\u00b10.5 | 13.6 | 22.6 | 15.6\u00b10.4\n[BOLD] Sum-Basic | 28.4 | 44.4 | 33.1\u00b10.5 | 8.5 | 15.6 | 10.4\u00b10.4 | 14.7 | 22.9 | 16.7\u00b10.5\n[BOLD] KL-Sum | 39.5 | 34.6 | 35.5\u00b10.5 | 13.0 | 12.7 | 12.3\u00b10.5 | 15.2 | 21.1 | 16.3\u00b10.5\n[BOLD] LexRank | 42.1 | 39.5 | 38.7\u00b10.5 | 14.7 | 15.3 | 14.2\u00b10.5 | 14.3 | 21.5 | 16.0\u00b10.5\n[BOLD] MEAD | 45.5 | 36.5 | 38.5\u00b1 0.5 | 17.9 | 14.9 | 15.4\u00b10.5 | 27.8 | 29.2 | 26.8\u00b10.5\n[BOLD] SVM | 19.0 | 48.8 | 24.7\u00b10.8 | 7.5 | 21.1 | 10.0\u00b10.5 | 32.7 | 34.3 | 31.4\u00b10.4\n[BOLD] LogReg | 26.9 | 34.5 | 28.7\u00b10.6 | 6.4 | 9.9 | 7.3\u00b10.4 | 12.2 | 14.9 | 12.7\u00b10.5\n[BOLD] LogReg [ITALIC] r | 28.0 | 34.8 | 29.4\u00b10.6 | 6.9 | 10.4 | 7.8\u00b10.4 | 12.1 | 14.5 | 12.5\u00b10.5\n[BOLD] HAN | 31.0 | 42.8 | 33.7\u00b10.7 | 11.2 | 17.8 | 12.7\u00b10.5 | 26.9 | 34.1 | 32.4\u00b10.5\n[BOLD] HAN+pretrainT | 32.2 | 42.4 | 34.4\u00b10.7 | 11.5 | 17.5 | 12.9\u00b10.5 | 29.6 | 35.8 | 32.2\u00b10.5\n[BOLD] HAN+pretrainU | 32.1 | 42.1 | 33.8\u00b10.7 | 11.6 | 17.6 | 12.9\u00b10.5 | 30.1 | 35.6 | 32.3\u00b10.5\n[BOLD] HAN [ITALIC] r | 38.1 | 40.5 | [BOLD] 37.8\u00b10.5 | 14.0 | 17.1 | [BOLD] 14.7\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainT [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.5 | 16.8 | [BOLD] 14.4\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainU [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.6 | 16.9 | [BOLD] 14.4\u00b10.5 | 33.9 | 33.8 | [BOLD] 33.8\u00b10.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4e7fe7a0-0c7a-4e48-b4e9-0aaa1d9a2c28",
    "input": "## Claim\nHere is a claim: We found that innovations are not helpful in both early and late fusion frameworks, and late fusion does not perform better on average. Does the following context support or refute the claim?\n\n## Table\nPaper title: Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection\nTable caption: Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. \u201cRaw\u201d indicates the usage of original prosodic features (Section 3.2), while \u201cinnovations\u201d indicate the usage of innovation features (Section 3.3).\n[EMPTY] | [BOLD] Model | [BOLD] dev mean | [BOLD] dev best | [BOLD] test mean | [BOLD] test best | [ITALIC] \u03b1\nsingle | text | 86.54 | 86.80 | 86.47 | 86.96 | \u2013\nsingle | raw | 35.00 | 37.33 | 35.78 | 37.70 | \u2013\nsingle | innovations | 80.86 | 81.51 | 80.28 | 82.15 | \u2013\nearly | text + raw | 86.46 | 86.65 | 86.24 | 86.53 | \u2013\nearly | text + innovations | 86.53 | 86.77 | 86.54 | 87.00 | \u2013\nearly | text + raw + innovations | 86.35 | 86.69 | 86.55 | 86.44 | \u2013\nlate | text + raw | 86.71 | 87.05 | 86.35 | 86.71 | 0.2\nlate | text + innovations | [BOLD] 86.98 | [BOLD] 87.48 | [BOLD] 86.68 | [BOLD] 87.02 | 0.5\nlate | text + raw + innovations | 86.95 | 87.30 | 86.60 | 86.87 | 0.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "8871d76b-c023-4dac-9ec0-866526d5cbed",
    "input": "## Claim\nHere is a claim: Considering the two aggregated categories of syntactic and semantic word analogies respectively and both 3CosAdd and 3CosMul metrics, model cc.el.300 has outperformed all the other models apart from the case of the Syntactic category when we included the out-of-vocabulary (oov) terms [CONTINUE] where the model gr def had the best performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluation of Greek Word Embeddings\nTable caption: Table 3: Summary for 3CosAdd and top-1 nearest vectors.\nCategory Semantic | Category no oov words | gr_def 58.42% | gr_neg10 59.33% | cc.el.300  [BOLD] 68.80% | wiki.el 27.20% | gr_cbow_def 31.76% | gr_d300_nosub 60.79% | gr_w2v_sg_n5 52.70%\n[EMPTY] | with oov words | 52.97% | 55.33% | [BOLD] 64.34% | 25.73% | 28.80% | 55.11% | 47.82%\nSyntactic | no oov words | 65.73% | 61.02% | [BOLD] 69.35% | 40.90% | 64.02% | 53.69% | 52.60%\n[EMPTY] | with oov words | [BOLD] 53.95% | 48.69% | 49.43% | 28.42% | 52.54% | 44.06% | 43.13%\nOverall | no oov words | 63.02% | 59.96% | [BOLD] 68.97% | 36.45% | 52.04% | 56.30% | 52.66%\n[EMPTY] | with oov words | 53.60% | 51.00% | [BOLD] 54.60% | 27.50% | 44.30% | 47.90% | 44.80%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "cdad25c1-2680-41e3-b279-9383fc241c09",
    "input": "## Claim\nHere is a claim: Replacing the attention normalizing function with softmax operation increases the F1 score marginally (A3\u2212A5). Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.\n[EMPTY] | Prec. | Rec. | F1\n(A1) BiLSTM-CNN | 0.473 | 0.606 | 0.531\n(A2) Standard attention | 0.466 | 0.638 | 0.539\n(A3) Window size ( [ITALIC] ws)=5 | 0.507 | 0.652 | [BOLD] 0.571\n(A4) Window size ( [ITALIC] ws)=10 | 0.510 | 0.640 | 0.568\n(A5) Softmax | 0.490 | 0.658 | 0.562\n(A6) Max-pool | 0.492 | 0.600 | 0.541\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3bdf91f0-992d-410b-9583-2f70056ed281",
    "input": "## Claim\nHere is a claim: For each model we report both perplexity and accuracy (except for discriminative training, where perplexity is not valid), where each of them is reported according to the best performing model on that measure (on the dev set). Does the following context support or refute the claim?\n\n## Table\nPaper title: Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training\nTable caption: Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.\n[EMPTY] | dev perp \u2193 | dev acc \u2191 | dev wer \u2193 | test perp \u2193 | test acc \u2191 | test wer \u2193\nSpanish-only-LM | 329.68 | 26.6 | 30.47 | 322.26 | 25.1 | 29.62\nEnglish-only-LM | 320.92 | 29.3 | 32.02 | 314.04 | 30.3 | 32.51\nAll:CS-last-LM | 76.64 | 47.8 | 14.56 | 76.97 | 49.2 | 14.13\nAll:Shuffled-LM | 68.00 | 51.8 | 13.64 | 68.72 | 51.4 | 13.89\nCS-only-LM | 43.20 | 60.7 | 12.60 | 43.42 | 57.9 | 12.18\nCS-only+vocab-LM | 45.61 | 61.0 | 12.56 | 45.79 | 58.8 | 12.49\nFine-Tuned-LM | 39.76 | 66.9 | 10.71 | 40.11 | 65.4 | 10.17\nCS-only-disc | \u2013 | 72.0 | 6.35 | \u2013 | 70.5 | 6.70\nFine-Tuned-disc | \u2013 | [BOLD] 74.2 | [BOLD] 5.85 | \u2013 | [BOLD] 75.5 | [BOLD] 5.59\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "aa179cf6-fac1-48d2-8bcf-e5ad45f72d25",
    "input": "## Claim\nHere is a claim: [CONTINUE] A notable exception is the \"Seanad Abolition\" dataset, where TF-IDF performs relatively better than WMD, Sent2vec and Doc2vec. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 3: ARI and Silhouette coefficient scores.\nMethods | Seanad Abolition ARI | Seanad Abolition  [ITALIC] Sil | Video Games ARI | Video Games  [ITALIC] Sil | Pornography ARI | Pornography  [ITALIC] Sil\nTF-IDF | 0.23 | 0.02 | -0.01 | 0.01 | -0.02 | 0.01\nWMD | 0.09 | 0.01 | 0.01 | 0.01 | -0.02 | 0.01\nSent2vec | -0.01 | -0.01 | 0.11 | 0.06 | 0.01 | 0.02\nDoc2vec | -0.01 | -0.03 | -0.01 | 0.01 | 0.02 | -0.01\nBERT | 0.03 | -0.04 | 0.08 | 0.05 | -0.01 | 0.03\nOD-parse | 0.01 | -0.04 | -0.01 | 0.02 | 0.07 | 0.05\nOD | [BOLD] 0.54 | [BOLD] 0.31 | [BOLD] 0.56 | [BOLD] 0.42 | [BOLD] 0.41 | [BOLD] 0.41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2b367697-ccca-45ce-a3ac-ef1ce5323dac",
    "input": "## Claim\nHere is a claim: Results presented in Table 7 show that the domain adaptation approach does not significantly boost F1 (t-test, p>0.5) and ROC AUC (0.012). Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 7: Complaint prediction results using the original data set and distantly supervised data. All models are based on logistic regression with bag-of-word and Part-of-Speech tag features.\n[BOLD] Model | [BOLD] Acc | [BOLD] F1 | [BOLD] AUC\nMost Frequent Class | 64.2 | 39.1 | 0.500\nLR-All Features \u2013 Original Data | 80.5 | 78.0 | 0.873\nDist. Supervision + Pooling | 77.2 | 75.7 | 0.853\nDist. Supervision + EasyAdapt | [BOLD] 81.2 | [BOLD] 79.0 | [BOLD] 0.885\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "dd2ffda3-81a0-4aea-af41-fd85e010806f",
    "input": "## Claim\nHere is a claim: when the best model for each dataset is deployed in either setting, our program ablation does best and the non-ablated tree does slightly worse but still significantly outperforms the baseline sentence model (t = 12.7, p = 5.0\u00d710-36). Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See \u00a72 for model details. * indicates our replication experiments.\nModel | Accuracy\nBigramPMI\u00a0Goodwin et al. ( 2012 ) | 63.4\nPMI\u00a0Gordon et al. ( 2011 ) | 65.4\nPMI+Connectives\u00a0Luo et al. ( 2016 ) | 70.2\nPMI+Con.+Phrase\u00a0Sasaki et al. ( 2017 ) | 71.4\nBERT-large\u00a0Wang et al. ( 2019 ) | 70.5\nBERT-large\u00a0Sap et al. ( 2019 ) | 75.0\nBERT-large\u00a0Li et al. ( 2019 ) | 75.4\nRoBERTa-large (finetuned) | 90.6\nBERT-large (finetuned)* | 76.5 \u00b1 2.7\nRoBERTa-large (finetuned)* | 87.7 \u00b1 0.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "058d2a40-7061-4dfa-bb41-bf00efd55f2c",
    "input": "## Claim\nHere is a claim: In German, we get a reduction of 100%. Does the following context support or refute the claim?\n\n## Table\nPaper title: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?\nTable caption: Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. \u201cReduction\u201d stands for gap reduction when removing gender signals from the context.\n[EMPTY] | Italian Original | Italian Debiased | Italian English | Italian Reduction | German Original | German Debiased | German English | German Reduction\nSame Gender | 0.442 | 0.434 | 0.424 | \u2013 | 0.491 | 0.478 | 0.446 | \u2013\nDifferent Gender | 0.385 | 0.421 | 0.415 | \u2013 | 0.415 | 0.435 | 0.403 | \u2013\ndifference | 0.057 | 0.013 | 0.009 | [BOLD] 91.67% | 0.076 | 0.043 | 0.043 | [BOLD] 100%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ca96fdb8-2600-46e2-b129-d6c81562945f",
    "input": "## Claim\nHere is a claim: Although these four models have the same number of layers, dense connections allow the model to achieve much better performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\n-{4} dense block | 24.8 | 54.9\n-{3, 4} dense blocks | 23.8 | 54.1\n-{2, 3, 4} dense blocks | 23.2 | 53.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "87b2347d-2e19-4a25-bb66-a742f0e8ebfd",
    "input": "## Claim\nHere is a claim: The relatively low accuracies of BERT-large, RoBERTa-large and BERT-*-NSP show that these pretrained models are not well-equipped to perform this task \"out-of-the-box\". Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large | B-COPA | 70.5 (\u00b1 2.5) | 72.6 (\u00b1 2.3) | [BOLD] 69.1 (\u00b1 2.7)\nBERT-large | B-COPA (50%) | 69.9 (\u00b1 1.9) | 71.2 (\u00b1 1.3) | 69.0 (\u00b1 3.5)\nBERT-large | COPA | [BOLD] 71.7 (\u00b1 0.5) | [BOLD] 80.5 (\u00b1 0.4) | 66.3 (\u00b1 0.8)\nRoBERTa-large | B-COPA | [BOLD] 76.7 (\u00b1 0.8) | 73.3 (\u00b1 1.5) | [BOLD] 78.8 (\u00b1 2.0)\nRoBERTa-large | B-COPA (50%) | 72.4 (\u00b1 2.0) | 72.1 (\u00b1 1.7) | 72.6 (\u00b1 2.1)\nRoBERTa-large | COPA | 76.4 (\u00b1 0.7) | [BOLD] 79.6 (\u00b1 1.0) | 74.4 (\u00b1 1.1)\nBERT-base-NSP | None | [BOLD] 66.4 | 66.2 | [BOLD] 66.7\nBERT-large-NSP | None | 65.0 | [BOLD] 66.9 | 62.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6943a94d-d91a-4fb2-965f-97aa1fa957ce",
    "input": "## Claim\nHere is a claim: Longer sentences pose additional challenges to the models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "e8e28650-51c4-4fbe-b946-b7966b1625a2",
    "input": "## Claim\nHere is a claim: [CONTINUE] As a result, the folding technique performs better than the recursive approach for the training task. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2\nTable caption: Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold\u2019s folding technique, and TensorFlow\u2019s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.\nBatch size | Throughput (instances/s) Inference | Throughput (instances/s) Inference | Throughput (instances/s) Inference | Throughput (instances/s) Training | Throughput (instances/s) Training | Throughput (instances/s) Training\nBatch size | Iter | Recur | Fold | Iter | Recur | Fold\n1 | 19.2 | 81.4 | 16.5 | 2.5 | 4.8 | 9.0\n10 | 49.3 | 217.9 | 52.2 | 4.0 | 4.2 | 37.5\n25 | 72.1 | 269.9 | 61.6 | 5.5 | 3.6 | 54.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "377ecb73-e548-4831-8bbb-d0c2edda5227",
    "input": "## Claim\nHere is a claim: The largest gain is by 4% on the CoordInv task. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "755c48ed-ff95-42cf-9e30-670ae9546e4d",
    "input": "## Claim\nHere is a claim: This is expected as encoding a bigger graph (containing more information) is harder than encoding smaller graphs. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "1cd4c25a-774f-4e75-a511-1dead6a68155",
    "input": "## Claim\nHere is a claim: The inferior score on attention relevance shows that TVMAX is worse at selecting the relevant features and its output is less interpretable. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 2: Human evaluation results on MSCOCO.\n[EMPTY] | caption | attention relevance\nsoftmax | 3.50 | 3.38\nsparsemax | 3.71 | 3.89\nTVmax | [BOLD] 3.87 | [BOLD] 4.10\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2da369f5-44cc-4f6d-8c37-97bfb561547b",
    "input": "## Claim\nHere is a claim: Row (1)-(7) show each model with different representations on the original dataset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "cd3bfb66-26db-4ea5-bbc7-33e9dd881d74",
    "input": "## Claim\nHere is a claim: We observe that the three settings (n=6, m=3), (n=3, m=6) and (n=6, m=6) give significantly different results for both 1 DCGCN block and 2 DCGCN blocks. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.\n[ITALIC] Block | [ITALIC] n | [ITALIC] m | B | C\n1 | 1 | 1 | 17.6 | 48.3\n1 | 1 | 2 | 19.2 | 50.3\n1 | 2 | 1 | 18.4 | 49.1\n1 | 1 | 3 | 19.6 | 49.4\n1 | 3 | 1 | 20.0 | 50.5\n1 | 3 | 3 | 21.4 | 51.0\n1 | 3 | 6 | 21.8 | 51.7\n1 | 6 | 3 | 21.7 | 51.5\n1 | 6 | 6 | 22.0 | 52.1\n2 | 3 | 6 | [BOLD] 23.5 | 53.3\n2 | 6 | 3 | 23.3 | [BOLD] 53.4\n2 | 6 | 6 | 22.0 | 52.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "fd328d46-3d4e-4a19-9b01-87e794fed8b9",
    "input": "## Claim\nHere is a claim: (2017).8 Overall both BERT (76.5%) and RoBERTa (87.7%) do not outperform the best previous model (71.4%) on Hard instances without superficial cues. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1ea08ac9-44c5-4494-894b-728c4468c01b",
    "input": "## Claim\nHere is a claim: In particular, we see that hate speech and harassment are relatively easy to detect. Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 1: Classifier performance\nDataset | Class | Precision | Recall | F1\n[ITALIC] W. & H. | Racism | 0.73 | 0.79 | 0.76\n[EMPTY] | Sexism | 0.69 | 0.73 | 0.71\n[EMPTY] | Neither | 0.88 | 0.85 | 0.86\n[ITALIC] W. | Racism | 0.56 | 0.77 | 0.65\n[EMPTY] | Sexism | 0.62 | 0.73 | 0.67\n[EMPTY] | R. & S. | 0.56 | 0.62 | 0.59\n[EMPTY] | Neither | 0.95 | 0.92 | 0.94\n[ITALIC] D. et al. | Hate | 0.32 | 0.53 | 0.4\n[EMPTY] | Offensive | 0.96 | 0.88 | 0.92\n[EMPTY] | Neither | 0.81 | 0.95 | 0.87\n[ITALIC] G. et al. | Harass. | 0.41 | 0.19 | 0.26\n[EMPTY] | Non. | 0.75 | 0.9 | 0.82\n[ITALIC] F. et al. | Hate | 0.33 | 0.42 | 0.37\n[EMPTY] | Abusive | 0.87 | 0.88 | 0.88\n[EMPTY] | Spam | 0.5 | 0.7 | 0.58\n[EMPTY] | Neither | 0.88 | 0.77 | 0.82\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "207bf674-dc1a-4979-8432-1342a80d538f",
    "input": "## Claim\nHere is a claim: For example, on Yelp, large differences in human judgments of semantic preservation (M2>M0, M7>M0, M7>M2) also show the largest differences in Sim, while M6 and M7 have very similar human judgments but significantly different Sim scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 4: Manual evaluation results (%) using models from Table\u00a02 (i.e., with roughly fixed Acc). > means \u201cbetter than\u201d. \u0394Sim=Sim(A)\u2212Sim(B), and \u0394PP=PP(A)\u2212PP(B) (note that lower PP generally means better fluency). Each row uses at least 120 sentence pairs. A cell is bold if it represents a model win of at least 10%.\nDataset | Models A | Models B | Transfer quality A>B | Transfer quality B>A | Transfer quality Tie | Semantic preservation A>B | Semantic preservation B>A | Semantic preservation Tie | Semantic preservation \u0394Sim | Fluency A>B | Fluency B>A | Fluency Tie | Fluency \u0394PP\n[EMPTY] | M0 | M2 | 9.0 | 6.0 | 85.1 | 1.5 | [BOLD] 25.4 | 73.1 | -0.05 | 10.4 | [BOLD] 23.9 | 65.7 | 0.9\nYelp | M0 | M7 | 9.6 | 14.7 | 75.8 | 2.5 | [BOLD] 54.5 | 42.9 | -0.09 | 4.6 | [BOLD] 39.4 | 56.1 | 8.3\nYelp | M6 | M7 | 13.7 | 11.6 | 74.7 | 16.0 | 16.7 | 67.4 | 0.01 | 10.3 | 20.0 | 69.7 | 14.3\n[EMPTY] | M2 | M7 | 5.8 | 9.3 | 84.9 | 8.1 | [BOLD] 25.6 | 66.3 | -0.04 | 14.0 | [BOLD] 26.7 | 59.3 | 7.4\nLiterature | M2 | M6 | 4.2 | 6.7 | 89.2 | 16.7 | 20.8 | 62.5 | 0.01 | [BOLD] 40.8 | 13.3 | 45.8 | -13.3\nLiterature | M6 | M7 | 15.8 | 13.3 | 70.8 | [BOLD] 25.0 | 9.2 | 65.8 | 0.03 | 14.2 | 20.8 | 65.0 | 14.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4d0e6a67-dd03-4a72-9b81-f8e50e9faae3",
    "input": "## Claim\nHere is a claim: The interpolation weight \u03b1 for the late fusion experiments is high when innovations are used, which further indicates that innovation features are useful in overall prediction. Does the following context support or refute the claim?\n\n## Table\nPaper title: Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection\nTable caption: Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. \u201cRaw\u201d indicates the usage of original prosodic features (Section 3.2), while \u201cinnovations\u201d indicate the usage of innovation features (Section 3.3).\n[EMPTY] | [BOLD] Model | [BOLD] dev mean | [BOLD] dev best | [BOLD] test mean | [BOLD] test best | [ITALIC] \u03b1\nsingle | text | 86.54 | 86.80 | 86.47 | 86.96 | \u2013\nsingle | raw | 35.00 | 37.33 | 35.78 | 37.70 | \u2013\nsingle | innovations | 80.86 | 81.51 | 80.28 | 82.15 | \u2013\nearly | text + raw | 86.46 | 86.65 | 86.24 | 86.53 | \u2013\nearly | text + innovations | 86.53 | 86.77 | 86.54 | 87.00 | \u2013\nearly | text + raw + innovations | 86.35 | 86.69 | 86.55 | 86.44 | \u2013\nlate | text + raw | 86.71 | 87.05 | 86.35 | 86.71 | 0.2\nlate | text + innovations | [BOLD] 86.98 | [BOLD] 87.48 | [BOLD] 86.68 | [BOLD] 87.02 | 0.5\nlate | text + raw + innovations | 86.95 | 87.30 | 86.60 | 86.87 | 0.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "82060521-bd71-4f0d-90fd-6b0e9de930b3",
    "input": "## Claim\nHere is a claim: The performances of all models increase as the diameters of the graphs increase. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "7d1503a6-0b59-4b98-885a-a90ffb1da624",
    "input": "## Claim\nHere is a claim: [CONTINUE] however, GRU yields the best BLEU score of 26.28, outperforming oLRN (+0.45 BLEU). Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.\nModel | #Params | BLEU | Train | Decode\nGNMT | - | 24.61 | - | -\nGRU | 206M | 26.28 | 2.67 | 45.35\nATR | 122M | 25.70 | 1.33 | [BOLD] 34.40\nSRU | 170M | 25.91 | 1.34 | 42.84\nLRN | 143M | 26.26 | [BOLD] 0.99 | 36.50\noLRN | 164M | [BOLD] 26.73 | 1.15 | 40.19\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "92d4db6a-df9b-45a3-bb55-a309229fec18",
    "input": "## Claim\nHere is a claim: It does not come close to VGS on paraphrase retrieval, but it does correlate with the visual modality even better. Does the following context support or refute the claim?\n\n## Table\nPaper title: On the difficulty of a distributional semantics of spoken language\nTable caption: Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.\n[EMPTY] | Recall@10 (%) | Median rank | RSAimage\nVGS | 27 | 6 | 0.4\nSegMatch | [BOLD] 10 | [BOLD] 37 | [BOLD] 0.5\nAudio2vec-U | 5 | 105 | 0.0\nAudio2vec-C | 2 | 647 | 0.0\nMean MFCC | 1 | 1,414 | 0.0\nChance | 0 | 3,955 | 0.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "dfce52f0-6525-4195-a26e-a69faf5d99eb",
    "input": "## Claim\nHere is a claim: [CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe no main effect on SER from cleaning the missed slots, with only slight reductions in insertions and deletions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Cleaned | TGen\u2212 | 36.85 | 5.3782 | 35.14 | 55.01 | 1.6016 | 00.34 | 09.81 | 00.15 | 10.31\nOriginal | [BOLD] Cleaned | TGen | 39.23 | 6.0217 | 36.97 | 55.52 | 1.7623 | 00.40 | 03.59 | 00.07 | 04.05\nOriginal | [BOLD] Cleaned | TGen+ | 40.25 | 6.1448 | 37.50 | 56.19 | 1.8181 | 00.21 | 01.99 | 00.05 | 02.24\nOriginal | [BOLD] Cleaned | SC-LSTM | 23.88 | 3.9310 | 32.11 | 39.90 | 0.5036 | 07.73 | 17.76 | 09.52 | 35.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen\u2212 | 40.19 | 6.0543 | 37.38 | 55.88 | 1.8104 | 00.17 | 01.31 | 00.25 | 01.72\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen | 40.73 | 6.1711 | 37.76 | 56.09 | 1.8518 | 00.07 | 00.72 | 00.08 | 00.87\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen+ | 40.51 | 6.1226 | 37.61 | 55.98 | 1.8286 | 00.02 | 00.63 | 00.06 | 00.70\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | SC-LSTM | 23.66 | 3.9511 | 32.93 | 39.29 | 0.3855 | 07.89 | 15.60 | 08.44 | 31.94\nCleaned missing | [BOLD] Cleaned | TGen\u2212 | 40.48 | 6.0269 | 37.26 | 56.19 | 1.7999 | 00.43 | 02.84 | 00.26 | 03.52\nCleaned missing | [BOLD] Cleaned | TGen | 41.57 | 6.2830 | 37.99 | 56.36 | 1.8849 | 00.37 | 01.40 | 00.09 | 01.86\nCleaned missing | [BOLD] Cleaned | TGen+ | 41.56 | 6.2700 | 37.94 | 56.38 | 1.8827 | 00.21 | 01.04 | 00.07 | 01.31\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen\u2212 | 35.99 | 5.0734 | 34.74 | 54.79 | 1.5259 | 00.02 | 11.58 | 00.02 | 11.62\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen | 40.07 | 6.1243 | 37.45 | 55.81 | 1.8026 | 00.05 | 03.23 | 00.01 | 03.29\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+ | 40.80 | 6.2197 | 37.86 | 56.13 | 1.8422 | 00.01 | 01.87 | 00.01 | 01.88\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ac0a6d46-3c6c-4e6f-8342-d436e43c9c8f",
    "input": "## Claim\nHere is a claim: For both Gigaword and NOW datasets (and the corresponding embeddings), using the cosinebased threshold decreases recall and increases precision (differences are statistically significant with t-test, p < 0.05). Does the following context support or refute the claim?\n\n## Table\nPaper title: One-to-X analogical reasoning on word embeddings: a case for diachronic armed conflict prediction from news texts\nTable caption: Table 3: Average diachronic performance\n[EMPTY] | [BOLD] Algorithm | [BOLD] Precision | [BOLD] Recall | [BOLD] F1\nGiga | Baseline | 0.19 | 0.51 | 0.28\nGiga | Threshold | 0.46 | 0.41 | [BOLD] 0.41\nNOW | Baseline | 0.26 | 0.53 | 0.34\nNOW | Threshold | 0.42 | 0.41 | [BOLD] 0.41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ad0e53cd-7e67-4eb5-bb8d-33949d1f8e6a",
    "input": "## Claim\nHere is a claim: AME outperforms the FME model, confirming the importance of word embeddings adaptation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task\nTable caption: Table 5: Textual similarity scores (asymmetric, Multi30k).\n[EMPTY] | EN \u2192 DE R@1 | EN \u2192 DE R@5 | EN \u2192 DE R@10 | DE \u2192 EN R@1 | DE \u2192 EN R@5 | DE \u2192 EN R@10\nFME | 51.4 | 76.4 | 84.5 | 46.9 | 71.2 | 79.1\nAME | [BOLD] 51.7 | [BOLD] 76.7 | [BOLD] 85.1 | [BOLD] 49.1 | [BOLD] 72.6 | [BOLD] 80.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "8346b898-68f9-44f3-84de-cf4cfeeb808c",
    "input": "## Claim\nHere is a claim: On the other hand, ACER is still subject to trainability limitation due to the lacking of expressivity power in DSTC models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "5323b8d2-37fa-4b1b-a9d9-05db8157d527",
    "input": "## Claim\nHere is a claim: [CONTINUE] We observed no advantage to using a hierachical encoder, [CONTINUE] Finally, we see that a 2 layer LSTM performs similarly to either a 4 layer or a 2 layer SRU with a comparable number of parameters. Does the following context support or refute the claim?\n\n## Table\nPaper title: Building a Production Model for Retrieval-Based Chatbots\nTable caption: Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.\n[BOLD] Model | [BOLD] Parameters | [BOLD] Validation AUC@0.05 | [BOLD] Test AUC@0.05\nBase | 8.0M | [BOLD] 0.871 | 0.816\n4L SRU \u2192 2L LSTM | 7.3M | 0.864 | [BOLD] 0.829\n4L SRU \u2192 2L SRU | 7.8M | 0.856 | [BOLD] 0.829\nFlat \u2192 hierarchical | 12.4M | 0.825 | 0.559\nCross entropy \u2192 hinge loss | 8.0M | 0.765 | 0.693\n6.6M \u2192 1M examples | 8.0M | 0.835 | 0.694\n6.6M \u2192 100K examples | 8.0M | 0.565 | 0.417\n200 \u2192 100 negatives | 8.0M | 0.864 | 0.647\n200 \u2192 10 negatives | 8.0M | 0.720 | 0.412\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "67184fb9-20ca-445e-8366-7d03160cce3a",
    "input": "## Claim\nHere is a claim: The single DCGCN model achieves a BLEU score of 30.4 and a CHRF++ score of 59.6, outperforming the ensemble approach based on combining five DCGCN models initialized with different random seeds. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.\n[BOLD] Model | [BOLD] T | #P | B | C\nSeq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1\nGGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4\nSeq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5\nGGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5\nDCGCN (ours) | S | [BOLD] 19.1M | 27.9 | 57.3\nDCGCN (ours) | E | 92.5M | [BOLD] 30.4 | [BOLD] 59.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2a87f7a2-dfb7-48c8-944d-aab7eb874e0e",
    "input": "## Claim\nHere is a claim: The proposed method does not outperform the original embeddings and performs worse than the SOV. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "292204b8-c7d0-4b68-8a35-f392930d4194",
    "input": "## Claim\nHere is a claim: This superior confirms the effectiveness of our approach. Does the following context support or refute the claim?\n\n## Table\nPaper title: Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation\nTable caption: Table 4: Results of Self-Play Evaluation.\nSystem | TGPC Succ. (%) | TGPC #Turns | CWC Succ. (%) | CWC #Turns\nRetrieval\u00a0 | 7.16 | 4.17 | 0 | -\nRetrieval-Stgy\u00a0 | 47.80 | 6.7 | 44.6 | 7.42\nPMI\u00a0 | 35.36 | 6.38 | 47.4 | 5.29\nNeural\u00a0 | 54.76 | 4.73 | 47.6 | 5.16\nKernel\u00a0 | 62.56 | 4.65 | 53.2 | 4.08\nDKRN (ours) | [BOLD] 89.0 | 5.02 | [BOLD] 84.4 | 4.20\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "026892e8-1d2c-412c-8881-af5a2b4400b2",
    "input": "## Claim\nHere is a claim: Among all the baselines, GDPL does not obtain the most preference against PPO. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "69b2e0e5-215d-4de9-840b-6752ca98311a",
    "input": "## Claim\nHere is a claim: The DCGCN models do not achieve the highest BLEU points on the En-De and En-Cs tasks, respectively. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 4: Main results on English-German and English-Czech datasets.\n[BOLD] Model | [BOLD] Type | [BOLD] English-German #P | [BOLD] English-German B | [BOLD] English-German C | [BOLD] English-Czech #P | [BOLD] English-Czech B | [BOLD] English-Czech C\nBoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | -\nCNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | -\nBiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | -\nPB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4\nSeq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8\nGGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3\nDCGCN (ours) | Single | [BOLD]  29.7M | [BOLD] 19.0 | [BOLD] 44.1 | [BOLD]  28.3M | [BOLD] 12.1 | [BOLD] 37.1\nSeq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4\nGGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9\nDCGCN (ours) | Ensemble | [BOLD]  149M | [BOLD] 20.5 | [BOLD] 45.8 | [BOLD]  142M | [BOLD] 13.1 | [BOLD] 37.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "95306f5e-c196-47cd-99a0-f54bb70d41e7",
    "input": "## Claim\nHere is a claim: Longer sentences do not pose additional challenges to the models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "bdc9f7b9-8ef2-48a3-b7a9-7e6927c6ca87",
    "input": "## Claim\nHere is a claim: our model achieves a slightly better performance in AUC than the baseline models and the proposed model works better. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\n-Word-ATT | 0.648 | 0.515 | 0.395 | 0.389\n-Capsule | 0.635 | 0.507 | 0.413 | 0.386\nOur Model | 0.650 | 0.519 | 0.422 | 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "47460ee9-1544-4470-82f5-665181faad47",
    "input": "## Claim\nHere is a claim: This suggests that our models are not capable of capturing better semantic information from the graph generating outputs semantically related to the reference sentences. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.\n<bold>Model</bold> | REF \u21d2 GEN <bold>ENT</bold> | REF \u21d2 GEN <bold>CON</bold> | REF \u21d2 GEN <bold>NEU</bold>\nS2S | 38.45 | 11.17 | 50.38\nG2S-GIN | 49.78 | 9.80 | 40.42\nG2S-GAT | 49.48 | 8.09 | 42.43\nG2S-GGNN | 51.32 | 8.82 | 39.86\n[EMPTY] | GEN \u21d2 REF | GEN \u21d2 REF | GEN \u21d2 REF\n<bold>Model</bold> | <bold>ENT</bold> | <bold>CON</bold> | <bold>NEU</bold>\nS2S | 73.79 | 12.75 | 13.46\nG2S-GIN | 76.27 | 10.65 | 13.08\nG2S-GAT | 77.54 | 8.54 | 13.92\nG2S-GGNN | 77.64 | 9.64 | 12.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d93f4cc1-346e-4d0f-a0c7-c1cd87778fb1",
    "input": "## Claim\nHere is a claim: Adding either the global node or the linear combination improves the baseline models with only dense connections. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5cd2b818-6cbd-43ff-a379-23d4544992da",
    "input": "## Claim\nHere is a claim: We showed that it is not possible to improve the feature extraction procedure for the VQA task by adding self-attention modules in the different ResNet blocks. Does the following context support or refute the claim?\n\n## Table\nPaper title: Modulated Self-attention Convolutional Network for VQA\nTable caption: Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\n[BOLD] ResNet-34 | [BOLD] Eval set % | [BOLD] #param\nBaseline (No SA)Anderson et al. ( 2018 ) | 55.00 | 0M\nSA (S: 1,2,3 - B: 1) | 55.11 | } 0.107M\nSA (S: 1,2,3 - B: 2) | 55.17 | } 0.107M\n[BOLD] SA (S: 1,2,3 - B: 3) | [BOLD] 55.27 | } 0.107M\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "7bb70ac8-2e77-430a-9c0c-c47235dad046",
    "input": "## Claim\nHere is a claim: The amount of resources is insufficient for executing forward computations, and therefore our framework does not outperform the folding technique for the inference task with up to 4.93x faster throughput. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2\nTable caption: Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold\u2019s folding technique, and TensorFlow\u2019s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.\nBatch size | Throughput (instances/s) Inference | Throughput (instances/s) Inference | Throughput (instances/s) Inference | Throughput (instances/s) Training | Throughput (instances/s) Training | Throughput (instances/s) Training\nBatch size | Iter | Recur | Fold | Iter | Recur | Fold\n1 | 19.2 | 81.4 | 16.5 | 2.5 | 4.8 | 9.0\n10 | 49.3 | 217.9 | 52.2 | 4.0 | 4.2 | 37.5\n25 | 72.1 | 269.9 | 61.6 | 5.5 | 3.6 | 54.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5a52f554-0dbd-442d-af13-23015337b5f0",
    "input": "## Claim\nHere is a claim: We observed an advantage to using a hierachical encoder, [CONTINUE] Finally, we see that a 2 layer LSTM performs worse than either a 4 layer or a 2 layer SRU with a comparable number of parameters. Does the following context support or refute the claim?\n\n## Table\nPaper title: Building a Production Model for Retrieval-Based Chatbots\nTable caption: Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.\n[BOLD] Model | [BOLD] Parameters | [BOLD] Validation AUC@0.05 | [BOLD] Test AUC@0.05\nBase | 8.0M | [BOLD] 0.871 | 0.816\n4L SRU \u2192 2L LSTM | 7.3M | 0.864 | [BOLD] 0.829\n4L SRU \u2192 2L SRU | 7.8M | 0.856 | [BOLD] 0.829\nFlat \u2192 hierarchical | 12.4M | 0.825 | 0.559\nCross entropy \u2192 hinge loss | 8.0M | 0.765 | 0.693\n6.6M \u2192 1M examples | 8.0M | 0.835 | 0.694\n6.6M \u2192 100K examples | 8.0M | 0.565 | 0.417\n200 \u2192 100 negatives | 8.0M | 0.864 | 0.647\n200 \u2192 10 negatives | 8.0M | 0.720 | 0.412\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ef8adc9d-e855-47d4-b3d1-f5099f9892f7",
    "input": "## Claim\nHere is a claim: The second row in Table 3 shows the test accuracy of a system trained without sense priors [CONTINUE] and the third row shows the effect of making the token representations context-insensitive by giving a similar attention score to all related concepts, essentially making them type level representations, but still grounded in WordNet. Does the following context support or refute the claim?\n\n## Table\nPaper title: Ontology-Aware Token Embeddings for Prepositional Phrase Attachment\nTable caption: Table 3: Effect of removing sense priors and context sensitivity (attention) from the model.\n[BOLD] Model | [BOLD] PPA Acc.\nfull | 89.7\n- sense priors | 88.4\n- attention | 87.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "58fa3a44-8ffc-4329-bddb-79ec2c2b9b21",
    "input": "## Claim\nHere is a claim: [CONTINUE] Hashtags also have a [CONTINUE] positive effect on classification performance, however it is less significant. Does the following context support or refute the claim?\n\n## Table\nPaper title: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations\nTable caption: Table 4: Number of tweets on the test set with and without emoji and hashtags. The number between parentheses is the proportion of tweets classified correctly.\n[EMPTY] | [BOLD] Present | [BOLD] Not Present\nEmoji | 4805 (76.6%) | 23952 (68.0%)\nHashtags | 2122 (70.5%) | 26635 (69.4%)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c048f310-3949-4f77-a0fd-f053459012d4",
    "input": "## Claim\nHere is a claim: Each participant evaluates 3 dialog sessions of each model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "284e66c0-e63c-4e04-99a7-e91ee70cdd14",
    "input": "## Claim\nHere is a claim: Our model (OURS) obtains substantial gains in accuracy over the baselines across all three target aspects. Does the following context support or refute the claim?\n\n## Table\nPaper title: Deriving Machine Attention from Human Rationales\nTable caption: Table 3: Accuracy of transferring between aspects. Models with \u2020 use labeled data from source aspects. Models with \u2021 use human rationales on the target aspect.\nSource | Target | Svm | Ra-Svm\u2021 | Ra-Cnn\u2021 | Trans\u2020 | Ra-Trans\u2021\u2020 | Ours\u2021\u2020 | Oracle\u2020\nBeer aroma+palate | Beer look | 74.41 | 74.83 | 74.94 | 72.75 | 76.41 | [BOLD] 79.53 | 80.29\nBeer look+palate | Beer aroma | 68.57 | 69.23 | 67.55 | 69.92 | 76.45 | [BOLD] 77.94 | 78.11\nBeer look+aroma | Beer palate | 63.88 | 67.82 | 65.72 | 74.66 | 73.40 | [BOLD] 75.24 | 75.50\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "9c92823f-0db0-4455-b09c-2636c5e90d5d",
    "input": "## Claim\nHere is a claim: With the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a6442b25-639a-4e9f-acc1-2af93942e266",
    "input": "## Claim\nHere is a claim: Note that the effectiveness of P1 and P2 are not necessarily additive, as combining P1 and P2 does not always perform the best. Does the following context support or refute the claim?\n\n## Table\nPaper title: Two Causal Principles for Improving Visual Dialog\nTable caption: Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table\u00a01. Note that only applying P2 is implemented by the implementations in Section\u00a05 with the history shortcut.\nModel | LF\u00a0 | HCIAE\u00a0 | CoAtt\u00a0 | RvA\u00a0\nbaseline | 57.21 | 56.98 | 56.46 | 56.74\n+P1 | 61.88 | 60.12 | 60.27 | 61.02\n+P2 | 72.65 | 71.50 | 71.41 | 71.44\n+P1+P2 | [BOLD] 73.63 | 71.99 | 71.87 | 72.88\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5b31abdf-f132-46a7-8da5-490adbe8d469",
    "input": "## Claim\nHere is a claim: We observe that for the NYT10 dataset, m = 4 gives the highest F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 3: Performance comparison of our model with different values of m on the two datasets.\n[ITALIC] m | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1\n1 | 0.541 | 0.595 | [BOLD] 0.566 | 0.495 | 0.621 | 0.551\n2 | 0.521 | 0.597 | 0.556 | 0.482 | 0.656 | 0.555\n3 | 0.490 | 0.617 | 0.547 | 0.509 | 0.633 | 0.564\n4 | 0.449 | 0.623 | 0.522 | 0.507 | 0.652 | [BOLD] 0.571\n5 | 0.467 | 0.609 | 0.529 | 0.488 | 0.677 | 0.567\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2096086d-1f21-4a72-992f-724d69319e5d",
    "input": "## Claim\nHere is a claim: The systems trained on the original data or with cleaned added slots clearly perform worse in terms of both semantic accuracy and fluency. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).\n[BOLD] Training data | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] Disfl\nOriginal | 0 | 22 | 0 | 14\nCleaned added | 0 | 23 | 0 | 14\nCleaned missing | 0 | 1 | 0 | 2\nCleaned | 0 | 0 | 0 | 5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "64d3506d-5cb3-472b-8186-a7e06b507407",
    "input": "## Claim\nHere is a claim: The Word2Vec embeddings appear to perform better than our method on the random test, although we suspect that the difference is marginal. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VIII: Precision scores for the Semantic Analogy Test\nQuestions Subset | # of Questions Seen | GloVe | Word2Vec | Proposed\nAll | 8783 | 78.94 | 81.03 | 79.96\nAt least one | 1635 | 67.58 | 70.89 | 67.89\nconcept word | 1635 | 67.58 | 70.89 | 67.89\nAll concept words | 110 | 77.27 | 89.09 | 83.64\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "5025d368-b507-4e9e-850b-ac7661dbc30b",
    "input": "## Claim\nHere is a claim: Although SFN requires a large portion of training data to achieve superior performance, we find that combining large amounts of multi-action parallel data can significantly improve the model\u2019s performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "750beeba-88c2-479e-a030-0576133256a4",
    "input": "## Claim\nHere is a claim: For both Gigaword and NOW datasets (and the corresponding embeddings), using the cosinebased threshold increases recall and decreases precision (differences are statistically significant with t-test, p < 0.05). Does the following context support or refute the claim?\n\n## Table\nPaper title: One-to-X analogical reasoning on word embeddings: a case for diachronic armed conflict prediction from news texts\nTable caption: Table 3: Average diachronic performance\n[EMPTY] | [BOLD] Algorithm | [BOLD] Precision | [BOLD] Recall | [BOLD] F1\nGiga | Baseline | 0.19 | 0.51 | 0.28\nGiga | Threshold | 0.46 | 0.41 | [BOLD] 0.41\nNOW | Baseline | 0.26 | 0.53 | 0.34\nNOW | Threshold | 0.42 | 0.41 | [BOLD] 0.41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e02742aa-05d4-4e91-ab12-6f4aaa5409ee",
    "input": "## Claim\nHere is a claim: Overall, all of the implementations can improve the performances of base models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Two Causal Principles for Improving Visual Dialog\nTable caption: Table 1: Performance (NDCG%) comparison for the experiments of applying our principles on the validation set of VisDial v1.0. LF is the enhanced version as we mentioned. QT, S and D denote question type, answer score sampling, and hidden dictionary learning, respectively. R0, R1, R2, R3 denote regressive loss, weighted softmax loss, binary sigmoid loss ,and generalized ranking loss, respectively.\nModel | baseline | QT | S  [ITALIC] R0 | S  [ITALIC] R1 | S  [ITALIC] R2 | S  [ITALIC] R3 | D\nLF\u00a0 | 57.21 | 58.97 | 67.82 | 71.27 | 72.04 | 72.36 | 72.65\nLF +P1 | 61.88 | 62.87 | 69.47 | 72.16 | 72.85 | 73.42 | [BOLD] 73.63\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7dad5701-2235-4fc8-b66b-306812347530",
    "input": "## Claim\nHere is a claim: In terms of relative numbers, the hybrid model improves upon CBOW in all probing tasks except WC. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "38bfa8f1-7948-49d5-bc60-08c75e385df9",
    "input": "## Claim\nHere is a claim: Note that the effectiveness of P1 and P2 are additive, which means combining P1 and P2 performs the best. Does the following context support or refute the claim?\n\n## Table\nPaper title: Two Causal Principles for Improving Visual Dialog\nTable caption: Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table\u00a01. Note that only applying P2 is implemented by the implementations in Section\u00a05 with the history shortcut.\nModel | LF\u00a0 | HCIAE\u00a0 | CoAtt\u00a0 | RvA\u00a0\nbaseline | 57.21 | 56.98 | 56.46 | 56.74\n+P1 | 61.88 | 60.12 | 60.27 | 61.02\n+P2 | 72.65 | 71.50 | 71.41 | 71.44\n+P1+P2 | [BOLD] 73.63 | 71.99 | 71.87 | 72.88\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "47e27211-3006-49e1-a5d6-34a1993698c1",
    "input": "## Claim\nHere is a claim: Despite achieving high performance in the task success, GDPL does not show substantial improvement in inform F1 and match rate over the baselines. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6077db2c-6ffe-4629-9f6c-42197e0ad297",
    "input": "## Claim\nHere is a claim: In most setups our best case is not better than the former best case. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Vector-spaces with Noisy Supervised Lexicons\nTable caption: Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En\u2192De, En\u2192Fi and En\u2192Es improvements are significant at p<0.05 according to ANOVA on the different runs.\nMethod | En\u2192It best | En\u2192It avg | En\u2192It iters | En\u2192De best | En\u2192De avg | En\u2192De iters | En\u2192Fi best | En\u2192Fi avg | En\u2192Fi iters | En\u2192Es best | En\u2192Es avg | En\u2192Es iters\nArtetxe et\u00a0al., 2018b | [BOLD] 48.53 | 48.13 | 573 | 48.47 | 48.19 | 773 | 33.50 | 32.63 | 988 | 37.60 | 37.33 | 808\nNoise-aware Alignment | [BOLD] 48.53 | [BOLD] 48.20 | 471 | [BOLD] 49.67 | [BOLD] 48.89 | 568 | [BOLD] 33.98 | [BOLD] 33.68 | 502 | [BOLD] 38.40 | [BOLD] 37.79 | 551\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "024b4b9e-0169-4451-a529-066860ae0555",
    "input": "## Claim\nHere is a claim: [CONTINUE] As we can observe, it seems that clustering semantically related terms will increase the precision (at least for the top 1,000 terms in the English corpora used in this experiment) as expected. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0abaf60d-6117-4b20-8493-f3678aadd259",
    "input": "## Claim\nHere is a claim: Selective attention mechanisms like sparsemax and especially TVMAX do not reduce repetition, as measured by the REP metric reported in Table 1. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.\n[EMPTY] | MSCOCO spice | MSCOCO cider | MSCOCO rouge [ITALIC] L | MSCOCO bleu4 | MSCOCO meteor | MSCOCO rep\u2193 | Flickr30k spice | Flickr30k cider | Flickr30k rouge [ITALIC] L | Flickr30k bleu4 | Flickr30k meteor | Flickr30k rep\u2193\nsoftmax | 18.4 | 0.967 | 52.9 | 29.9 | 24.9 | 3.76 | 13.5 | 0.443 | 44.2 | 19.9 | 19.1 | 6.09\nsparsemax | [BOLD] 18.9 | [BOLD] 0.990 | [BOLD] 53.5 | [BOLD] 31.5 | [BOLD] 25.3 | 3.69 | [BOLD] 13.7 | [BOLD] 0.444 | [BOLD] 44.3 | [BOLD] 20.7 | [BOLD] 19.3 | 5.84\nTVmax | 18.5 | 0.974 | 53.1 | 29.9 | 25.1 | [BOLD] 3.17 | 13.3 | 0.438 | 44.2 | 20.5 | 19.0 | [BOLD] 3.97\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "df4b4d38-b005-4456-b84e-1780b5fcc389",
    "input": "## Claim\nHere is a claim: From the table, we can see that our JMEE framework does not achieve the best F1 scores for both trigger classification and argument-related subtasks among all the compared methods. Does the following context support or refute the claim?\n\n## Table\nPaper title: Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation\nTable caption: Table 1: Overall performance comparing to the state-of-the-art methods with golden-standard entities.\n[BOLD] Method | [BOLD] Trigger  [BOLD] Identification (%) | [BOLD] Trigger  [BOLD] Identification (%) | [BOLD] Trigger  [BOLD] Identification (%) | [BOLD] Trigger  [BOLD] Classification (%) | [BOLD] Trigger  [BOLD] Classification (%) | [BOLD] Trigger  [BOLD] Classification (%) | [BOLD] Argument  [BOLD] Identification (%) | [BOLD] Argument  [BOLD] Identification (%) | [BOLD] Argument  [BOLD] Identification (%) | [BOLD] Argument  [BOLD] Role (%) | [BOLD] Argument  [BOLD] Role (%) | [BOLD] Argument  [BOLD] Role (%)\n[BOLD] Method | [ITALIC] P | [ITALIC] R | [ITALIC] F1 | [ITALIC] P | [ITALIC] R | [ITALIC] F1 | [ITALIC] P | [ITALIC] R | [ITALIC] F1 | [ITALIC] P | [ITALIC] R | [ITALIC] F1\nCross-Event | [EMPTY] | [EMPTY] | [EMPTY] | 68.7 | 68.9 | 68.8 | 50.9 | 49.7 | 50.3 | 45.1 | 44.1 | 44.6\nJointBeam | 76.9 | 65.0 | 70.4 | 73.7 | 62.3 | 67.5 | 69.8 | 47.9 | 56.8 | 64.7 | 44.4 | 52.7\nDMCNN | [BOLD] 80.4 | 67.7 | 73.5 | 75.6 | 63.6 | 69.1 | 68.8 | 51.9 | 59.1 | 62.2 | 46.9 | 53.5\nPSL | [EMPTY] | [EMPTY] | [EMPTY] | 75.3 | 64.4 | 69.4 | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nJRNN | 68.5 | [BOLD] 75.7 | 71.9 | 66.0 | [BOLD] 73.0 | 69.3 | 61.4 | 64.2 | 62.8 | 54.2 | 56.7 | 55.4\ndbRNN | [EMPTY] | [EMPTY] | [EMPTY] | 74.1 | 69.8 | 71.9 | 71.3 | 64.5 | 67.7 | 66.2 | 52.8 | 58.7\n[BOLD] JMEE | 80.2 | 72.1 | [BOLD] 75.9 | [BOLD] 76.3 | 71.3 | [BOLD] 73.7 | [BOLD] 71.4 | [BOLD] 65.6 | [BOLD] 68.4 | [BOLD] 66.8 | [BOLD] 54.9 | [BOLD] 60.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "c38e08e0-2ef9-4fdc-bf65-6218e5f62a85",
    "input": "## Claim\nHere is a claim: The results in Table 2 (top half) for the original setup confirm that the ranking mechanism for TGen is effective for both WOMs and SER, whereas the SC-LSTM seems to have trouble scaling to the E2E dataset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Original | TGen\u2212 | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94\nOriginal | [BOLD] Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27\nOriginal | [BOLD] Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80\nOriginal | [BOLD] Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen\u2212 | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37\nCleaned missing | [BOLD] Original | TGen\u2212 | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61\nCleaned missing | [BOLD] Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53\nCleaned missing | [BOLD] Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen\u2212 | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a4701b9a-2f9b-4864-8085-d1ec451d1455",
    "input": "## Claim\nHere is a claim: our system also receives the highest rating in 70% of test cases. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "db48fd7d-eac7-402b-986a-1295738e6236",
    "input": "## Claim\nHere is a claim: [CONTINUE] EWC models do not perform as well as uniform ensembling, as evidenced by the fact that in some cases, uniform ensembling outperforms the oracle. Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 6: Test BLEU for 2-model es-en and 3-model en-de model ensembling for models adapted with EWC, compared to oracle model last trained on each domain, chosen if test domain is known. BI+IS outperforms uniform ensembling and in some cases outperforms the oracle.\n[BOLD] Decoder configuration | [BOLD] es-en  [BOLD] Health | [BOLD] es-en  [BOLD] Bio | [BOLD] en-de  [BOLD] News | [BOLD] en-de  [BOLD] TED | [BOLD] en-de  [BOLD] IT\nOracle model | 35.9 | 37.8 | 37.8 | 27.0 | 57.0\nUniform | 36.0 | 36.4 | [BOLD] 38.9 | 26.0 | 43.5\nBI + IS | [BOLD] 36.2 | [BOLD] 38.0 | 38.7 | [BOLD] 26.1 | [BOLD] 56.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e096ffd1-7885-4c82-b7df-e0b27418c2b0",
    "input": "## Claim\nHere is a claim: GDPL is better at booking flights and restaurants than finding hotels, even though its SLU precision is comparable to other agents. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ac84d56e-4583-4f9c-ae2a-0731f62551ba",
    "input": "## Claim\nHere is a claim: Our model outperforms PG-MMR when trained and tested on the Multi-News dataset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\nTable caption: Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.\n[BOLD] Method | [BOLD] R-1 | [BOLD] R-2 | [BOLD] R-SU\nFirst-1 | 26.83 | 7.25 | 6.46\nFirst-2 | 35.99 | 10.17 | 12.06\nFirst-3 | 39.41 | 11.77 | 14.51\nLexRank Erkan and Radev ( 2004 ) | 38.27 | 12.70 | 13.20\nTextRank Mihalcea and Tarau ( 2004 ) | 38.44 | 13.10 | 13.50\nMMR Carbonell and Goldstein ( 1998 ) | 38.77 | 11.98 | 12.91\nPG-Original Lebanoff et\u00a0al. ( 2018 ) | 41.85 | 12.91 | 16.46\nPG-MMR Lebanoff et\u00a0al. ( 2018 ) | 40.55 | 12.36 | 15.87\nPG-BRNN Gehrmann et\u00a0al. ( 2018 ) | 42.80 | 14.19 | 16.75\nCopyTransformer Gehrmann et\u00a0al. ( 2018 ) | [BOLD] 43.57 | 14.03 | 17.37\nHi-MAP (Our Model) | 43.47 | [BOLD] 14.89 | [BOLD] 17.41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "bb3c6b73-b0df-4c55-a45f-44744409c0cf",
    "input": "## Claim\nHere is a claim: However, on the classes like \"clothing\" and \"bodyparts\" our model ZSGNet shows much better performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Zero-Shot Grounding of Objects from Natural Language Queries\nTable caption: Table 3: Category-wise performance with the default split of Flickr30k Entities.\nMethod | Overall | people | clothing | bodyparts | animals | vehicles | instruments | scene | other\nQRC - VGG(det) | 60.21 | 75.08 | 55.9 | 20.27 | 73.36 | 68.95 | 45.68 | 65.27 | 38.8\nCITE - VGG(det) | 61.89 | [BOLD] 75.95 | 58.50 | 30.78 | [BOLD] 77.03 | [BOLD] 79.25 | 48.15 | 58.78 | 43.24\nZSGNet - VGG (cls) | 60.12 | 72.52 | 60.57 | 38.51 | 63.61 | 64.47 | 49.59 | 64.66 | 41.09\nZSGNet - Res50 (cls) | [BOLD] 63.39 | 73.87 | [BOLD] 66.18 | [BOLD] 45.27 | 73.79 | 71.38 | [BOLD] 58.54 | [BOLD] 66.49 | [BOLD] 45.53\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "6eaf2d50-5277-4860-8a5f-c43beb58c9e3",
    "input": "## Claim\nHere is a claim: The coverage mechanism is not effective in our models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "54e7aefb-6033-47d9-9435-ec4661f93470",
    "input": "## Claim\nHere is a claim: Similarly, manual features reduce recall, but help the system to improve accuracy and precision (sometimes considerably). Does the following context support or refute the claim?\n\n## Table\nPaper title: Low-supervision urgency detection and transfer in short crisis messages\nTable caption: TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal\nSystem | Accuracy | Precision | Recall | F-Measure\nLocal | 63.97% | 64.27% | 64.50% | 63.93%\nManual | 64.25% | [BOLD] 70.84%\u2217\u2217 | 48.50% | 57.11%\nWiki | 67.25% | 66.51% | 69.50% | 67.76%\nLocal-Manual | 65.75% | 67.96% | 59.50% | 62.96%\nWiki-Local | 67.40% | 65.54% | 68.50% | 66.80%\nWiki-Manual | 67.75% | 70.38% | 63.00% | 65.79%\n[ITALIC] Our Approach | [BOLD] 69.25%\u2217\u2217\u2217 | 68.76% | [BOLD] 70.50%\u2217\u2217 | [BOLD] 69.44%\u2217\u2217\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "549bac5e-c1c3-4601-9908-b900f7c78abd",
    "input": "## Claim\nHere is a claim: [CONTINUE] However, the slightly increased invalid response percentage [CONTINUE] We also observe our DAMD model outperforms HDSA in both diversity and appropriateness scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.\nModel | Diversity | App | Good% | OK% | Invalid%\nDAMD | 3.12 | 2.50 | 56.5% | [BOLD] 37.4% | 6.1%\nDAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1% | 9.9%\nHDSA (+) | 2.14 | 2.47 | 57.5% | 32.5% | [BOLD] 10.0%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "8f6c2db9-abdd-4e42-9ad8-3898a329957d",
    "input": "## Claim\nHere is a claim: On the three datasets, OD achieves an average weighted F1 score of 0.54, 0.56 and 0.41 respectively compared to the scores of 0.01, -0.01 and 0.07 by OD-parse. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.\n[EMPTY] | Difference Function | Seanad Abolition | Video Games | Pornography\nOD-parse | Absolute | 0.01 | -0.01 | 0.07\nOD-parse | JS div. | 0.01 | -0.01 | -0.01\nOD-parse | EMD | 0.07 | 0.01 | -0.01\nOD | Absolute | [BOLD] 0.54 | [BOLD] 0.56 | [BOLD] 0.41\nOD | JS div. | 0.07 | -0.01 | -0.02\nOD | EMD | 0.26 | -0.01 | 0.01\nOD (no polarity shifters) | Absolute | 0.23 | 0.08 | 0.04\nOD (no polarity shifters) | JS div. | 0.09 | -0.01 | -0.02\nOD (no polarity shifters) | EMD | 0.10 | 0.01 | -0.01\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f248d065-435c-4785-bd05-398870db94b1",
    "input": "## Claim\nHere is a claim: [CONTINUE] We notice small improvements relative to the baseline showing that self-attention alone does improve the VQA task. Does the following context support or refute the claim?\n\n## Table\nPaper title: Modulated Self-attention Convolutional Network for VQA\nTable caption: Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\n[BOLD] ResNet-34 | [BOLD] Eval set % | [BOLD] #param\nBaseline (No SA)Anderson et al. ( 2018 ) | 55.00 | 0M\nSA (S: 1,2,3 - B: 1) | 55.11 | } 0.107M\nSA (S: 1,2,3 - B: 2) | 55.17 | } 0.107M\n[BOLD] SA (S: 1,2,3 - B: 3) | [BOLD] 55.27 | } 0.107M\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ba9a2f4b-93b8-4d76-9216-ce8824a47208",
    "input": "## Claim\nHere is a claim: We don\u2019t evaluate RoBERTa on the 100 instance subset of COPA due to its tendency to pick superficial cues. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large-FT | B-COPA | 74.5 (\u00b1 0.7) | 74.7 (\u00b1 0.4) | [BOLD] 74.4 (\u00b1 0.9)\nBERT-large-FT | B-COPA (50%) | 74.3 (\u00b1 2.2) | 76.8 (\u00b1 1.9) | 72.8 (\u00b1 3.1)\nBERT-large-FT | COPA | [BOLD] 76.5 (\u00b1 2.7) | [BOLD] 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5)\nRoBERTa-large-FT | B-COPA | [BOLD] 89.0 (\u00b1 0.3) | 88.9 (\u00b1 2.1) | [BOLD] 89.0 (\u00b1 0.8)\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (\u00b1 2.2) | 87.4 (\u00b1 1.1) | 85.4 (\u00b1 2.9)\nRoBERTa-large-FT | COPA | 87.7 (\u00b1 0.9) | [BOLD] 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e0cadaaf-cbe4-40d0-82a3-380e6977dab4",
    "input": "## Claim\nHere is a claim: We observe that the results for the UD representation are comparable to the two others. Does the following context support or refute the claim?\n\n## Table\nPaper title: Syntactic Dependency Representations in Neural Relation Classification\nTable caption: Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.\n[BOLD] Representation | [BOLD] Hyper parameters Filter size | [BOLD] Hyper parameters Num. Feature maps | [BOLD] Hyper parameters Activation func. | [BOLD] Hyper parameters L2 Reg. | [BOLD] Hyper parameters Learning rate | [BOLD] Hyper parameters Dropout Prob. | [BOLD] F1.(avg. in 5-fold) with default values | [BOLD] F1.(avg. in 5-fold) with optimal values\nCoNLL08 | 4-5 | 1000 | Softplus | 1.15e+01 | 1.13e-03 | 1 | 73.34 | 74.49\nSB | 4-5 | 806 | Sigmoid | 8.13e-02 | 1.79e-03 | 0.87 | 72.83 | [BOLD] 75.05\nUD v1.3 | 5 | 716 | Softplus | 1.66e+00 | 9.63E-04 | 1 | 68.93 | 69.57\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "cc099e12-c222-4508-992e-c442f61982cf",
    "input": "## Claim\nHere is a claim: Lemmatized targets generally do not perform better, with the boost being more pronounced on SimVerb. Does the following context support or refute the claim?\n\n## Table\nPaper title: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources\nTable caption: Table 1: Benchmark performance, Spearman\u2019s \u03c1. SGNS results with * taken from [morphfit]. Best results per column (benchmark) annotated for our setup only.\nContext: w2 | Context: w2 SimLex | Context: w2 SimLex | Context: w2 SimLex | Context: w2 SimLex | Context: w2 SimVerb\ntarget | N | V | A | all | V\ntype | .334 | <bold>.336</bold> | <bold>.518</bold> | .348 | .307\nx + POS | .342 | .323 | .513 | .350 | .279\nlemma | <bold>.362</bold> | .333 | .497 | <bold>.351</bold> | .400\nx + POS | .354 | <bold>.336</bold> | .504 | .345 | <bold>.406</bold>\n* type | - | - | - | .339 | .277\n* type MFit-A | - | - | - | .385 | -\n* type MFit-AR | - | - | - | .439 | .381\nContext: dep-W | Context: dep-W | Context: dep-W | Context: dep-W | Context: dep-W | Context: dep-W\ntype | .366 | .365 | .489 | .362 | .314\nx + POS | .364 | .351 | .482 | .359 | .287\nlemma | <bold>.391</bold> | .380 | <bold>.522</bold> | <bold>.379</bold> | .401\nx + POS | .384 | <bold>.388</bold> | .480 | .366 | <bold>.431</bold>\n* type | - | - | - | .376 | .313\n* type MFit-AR | - | - | - | .434 | .418\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "af376ecf-fa75-4847-bbcc-3b39da24aa06",
    "input": "## Claim\nHere is a claim: The hybrid model is not able to repair this deficit, increasing the difference to 8%. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 3: Scores on unsupervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nMethod | STS12 | STS13 | STS14 | STS15 | STS16\nCBOW | 43.5 | [BOLD] 50.0 | [BOLD] 57.7 | [BOLD] 63.2 | 61.0\nCMOW | 39.2 | 31.9 | 38.7 | 49.7 | 52.2\nHybrid | [BOLD] 49.6 | 46.0 | 55.1 | 62.4 | [BOLD] 62.1\ncmp. CBOW | +14.6% | -8% | -4.5% | -1.5% | +1.8%\ncmp. CMOW | +26.5% | +44.2% | +42.4 | +25.6% | +19.0%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5d367238-2cda-4729-80eb-d27d1c716d89",
    "input": "## Claim\nHere is a claim: They are 553 true positives, 48 false positives, and 5 false negatives. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 4: Cue classification on the test set.\n[EMPTY] | [BOLD] F-Score  [BOLD] Baseline | [BOLD] F-Score  [BOLD] Proposed | [BOLD] Support\nFalse cues | 0.61 | 0.68 | 47\nActual cues | 0.97 | 0.98 | 557\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f21cc3b7-2533-4f2f-a259-ac3cd13583d2",
    "input": "## Claim\nHere is a claim: However, the main improvement of SER comes from training on cleaned data with up to 94% error reduction without the ranker and 97% with.11 just cleaning the training data has a much less dramatic effect than using a semantic control mechanism, such as the reranker (4.27% vs. 0.97% SER). Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Original | TGen\u2212 | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94\nOriginal | [BOLD] Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27\nOriginal | [BOLD] Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80\nOriginal | [BOLD] Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen\u2212 | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37\nCleaned missing | [BOLD] Original | TGen\u2212 | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61\nCleaned missing | [BOLD] Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53\nCleaned missing | [BOLD] Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen\u2212 | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3a75d020-da89-4447-ab2b-ae91ec897986",
    "input": "## Claim\nHere is a claim: Though the improvement is slim, it is encouraging to continue researching into visual modulation Does the following context support or refute the claim?\n\n## Table\nPaper title: Modulated Self-attention Convolutional Network for VQA\nTable caption: Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\n[BOLD] ResNet-34 | [BOLD] Eval set % | [BOLD] #param\nSA (S: 3 - M: 1) | 55.25 | } 0.082M\n[BOLD] SA (S: 3 - B: 3) | [BOLD] 55.42 | } 0.082M\nSA (S: 3 - B: 4) | 55.33 | } 0.082M\nSA (S: 3 - B: 6) | 55.31 | } 0.082M\nSA (S: 3 - B: 1,3,5) | 55.45 | } 0.245M\n[BOLD] SA (S: 3 - B: 2,4,6) | [BOLD] 55.56 | } 0.245M\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "89bb4f7f-01cd-4bda-a4ad-cc257857127b",
    "input": "## Claim\nHere is a claim: PPO agent obtains the highest ratio of successful turns, but GDPL outperforms other agents on SLU precision. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ad70b00f-de99-4e50-88ae-fb0d5320778d",
    "input": "## Claim\nHere is a claim: Consequently, with an 8% i is substantially more linguistically informed than CBOW. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c3ba1cac-9233-4b8e-9df7-50eb7d95b160",
    "input": "## Claim\nHere is a claim: the Pearson correlation coefficients in Table VI present the above-mentioned results with the cosine similarity scores used to compare two word embeddings. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VI: Correlations for Word Similarity Tests\nDataset (EN-) | GloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\nWS-353-ALL | 0.612 | 0.7156 | 0.634 | 0.622 | 0.173 | 0.690 | 0.657\nSIMLEX-999 | 0.359 | 0.3939 | 0.295 | 0.355 | 0.090 | 0.380 | 0.381\nVERB-143 | 0.326 | 0.4430 | 0.255 | 0.271 | 0.293 | 0.271 | 0.348\nSimVerb-3500 | 0.193 | 0.2856 | 0.184 | 0.197 | 0.035 | 0.234 | 0.245\nWS-353-REL | 0.578 | 0.6457 | 0.595 | 0.578 | 0.134 | 0.695 | 0.619\nRW-STANF. | 0.378 | 0.4858 | 0.316 | 0.373 | 0.122 | 0.390 | 0.382\nYP-130 | 0.524 | 0.5211 | 0.353 | 0.482 | 0.169 | 0.420 | 0.589\nMEN-TR-3k | 0.710 | 0.7528 | 0.684 | 0.696 | 0.298 | 0.769 | 0.725\nRG-65 | 0.768 | 0.8051 | 0.736 | 0.732 | 0.338 | 0.761 | 0.774\nMTurk-771 | 0.650 | 0.6712 | 0.593 | 0.623 | 0.199 | 0.665 | 0.671\nWS-353-SIM | 0.682 | 0.7883 | 0.713 | 0.702 | 0.220 | 0.720 | 0.720\nMC-30 | 0.749 | 0.8112 | 0.799 | 0.726 | 0.330 | 0.735 | 0.776\nMTurk-287 | 0.649 | 0.6645 | 0.591 | 0.631 | 0.295 | 0.674 | 0.634\nAverage | 0.552 | 0.6141 | 0.519 | 0.538 | 0.207 | 0.570 | 0.579\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "82ec1cdd-1c49-4aaa-9764-e45b072fb22d",
    "input": "## Claim\nHere is a claim: [CONTINUE] Sentiment polarity shifters have a low impact on clustering performance of opinion distance: We find that not utilizing the sentiment polarity shifters, especially in case of datasets \"Video games\" and \"Pornography\" does not significantly hurt the Opinion Representation phase, and thereby does not lead to incorrect computation of opinion distance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.\n[EMPTY] | Difference Function | Seanad Abolition | Video Games | Pornography\nOD-parse | Absolute | 0.01 | -0.01 | 0.07\nOD-parse | JS div. | 0.01 | -0.01 | -0.01\nOD-parse | EMD | 0.07 | 0.01 | -0.01\nOD | Absolute | [BOLD] 0.54 | [BOLD] 0.56 | [BOLD] 0.41\nOD | JS div. | 0.07 | -0.01 | -0.02\nOD | EMD | 0.26 | -0.01 | 0.01\nOD (no polarity shifters) | Absolute | 0.23 | 0.08 | 0.04\nOD (no polarity shifters) | JS div. | 0.09 | -0.01 | -0.02\nOD (no polarity shifters) | EMD | 0.10 | 0.01 | -0.01\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "435103bb-73be-4283-91b1-b429a3b988b7",
    "input": "## Claim\nHere is a claim: Using only one attention head, thereby attending to only one context position at once, degrades the performance to less than the performance of 10 heads using the standard finetuning scheme. Does the following context support or refute the claim?\n\n## Table\nPaper title: Localization of Fake News Detection via Multitask Transfer Learning\nTable caption: Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. \u201cEffect\u201d refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.\n# of Heads | Accuracy | Val. Loss | Effect\n1 | 89.44% | 0.2811 | -6.84%\n2 | 91.20% | 0.2692 | -5.08%\n4 | 93.85% | 0.2481 | -2.43%\n8 | 96.02% | 0.2257 | -0.26%\n10 | 96.28% | 0.2197 | [EMPTY]\n16 | 96.32% | 0.2190 | +0.04\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c677c0e0-d305-42a3-b263-5626bd417d4a",
    "input": "## Claim\nHere is a claim: One reason is that when the reference action sequence is long, the probability of all actions being correct decreases. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "2aad1e34-9d55-4236-849e-9826c3e92730",
    "input": "## Claim\nHere is a claim: As for Success metric, some ambiguous start location can cause low score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e3b3b4b9-da35-4fa1-b2ca-bd86c3df2677",
    "input": "## Claim\nHere is a claim: [CONTINUE] Wikipedia-PubMed-PMC embeddings (Moen and Ananiadou, 2013) does not outperform GloVe (Mikolov et al., 2013a) in the extraction of most relation types (Table 1) [CONTINUE] the combination feature of BoC and sentence embeddings does not outperform sentence embeddings alone, and does not exceed the upper boundary of BoC feature, in which again demonstrating the lack of competitiveness of BoC feature. Does the following context support or refute the claim?\n\n## Table\nPaper title: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data\nTable caption: Table 1: Performance of supervised learning models with different features.\nFeature | LR P | LR R | LR F1 | SVM P | SVM R | SVM F1 | ANN P | ANN R | ANN F1\n+BoW | 0.93 | 0.91 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+BoC (Wiki-PubMed-PMC) | 0.94 | 0.92 | [BOLD] 0.93 | 0.94 | 0.92 | [BOLD] 0.93 | 0.91 | 0.91 | [BOLD] 0.91\n+BoC (GloVe) | 0.93 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+ASM | 0.90 | 0.85 | 0.88 | 0.90 | 0.86 | 0.88 | 0.89 | 0.89 | 0.89\n+Sentence Embeddings(SEs) | 0.89 | 0.89 | 0.89 | 0.90 | 0.86 | 0.88 | 0.88 | 0.88 | 0.88\n+BoC(Wiki-PubMed-PMC)+SEs | 0.92 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3de653b7-7800-41e0-8431-4f7ea3574f5d",
    "input": "## Claim\nHere is a claim: When increasing the number of terms to 10,000, the DocSub models using Europarl corpora performed better than when using TED Talks corpora. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150 | 0.0150 | 0.0445 | 0.0326\nP | EN | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.0060 | 0.0092 | 0.0090 | 0.0356 | 0.0162\nP | PT | Europarl | 0.5710 | 0.1948 | 0.3855 | 0.5474 | 0.4485 | [BOLD] 0.8052 | 0.4058\n[EMPTY] | PT | Ted Talks | [BOLD] 0.6304 | 0.1870 | 0.3250 | 0.5312 | 0.4576 | 0.6064 | 0.3698\nR | EN | Europarl | 0.0037 | 0.3278 | 0.5941 | 0.6486 | [BOLD] 0.6490 | 0.0017 | 0.0003\nR | EN | Ted Talks | 0.0002 | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332 | 0.0967 | 0.0003\nR | PT | Europarl | 0.0002 | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932 | 0.0032 | 0.0001\n[EMPTY] | PT | Ted Talks | 2.10-5 | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887 | 0.1390 | 0.0002\nF | EN | Europarl | 0.0073 | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033 | 0.0006\nF | EN | Ted Talks | 0.0004 | 0.0132 | 0.0118 | 0.0181 | 0.0179 | [BOLD] 0.0520 | 0.0005\nF | PT | Europarl | 0.0005 | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109 | 0.0064 | 0.0002\n[EMPTY] | PT | Ted Talks | 4.10-5 | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149 | 0.2261 | 0.0004\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "546f1892-7648-4fd4-a1e2-35ecc35cd23a",
    "input": "## Claim\nHere is a claim: humans do poorly on hard instances, which requires deeper inference rather than surface cues, and neural language models largely overcome this difficulty. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "7e8e2c4f-92ea-4c80-a27c-84cae2005a80",
    "input": "## Claim\nHere is a claim: SegMatch works slightly better than Audio2vec according to both criteria. Does the following context support or refute the claim?\n\n## Table\nPaper title: On the difficulty of a distributional semantics of spoken language\nTable caption: Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.\n[EMPTY] | Recall@10 (%) | Median rank | RSAimage\nVGS | 27 | 6 | 0.4\nSegMatch | [BOLD] 10 | [BOLD] 37 | [BOLD] 0.5\nAudio2vec-U | 5 | 105 | 0.0\nAudio2vec-C | 2 | 647 | 0.0\nMean MFCC | 1 | 1,414 | 0.0\nChance | 0 | 3,955 | 0.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "85a0c565-04f4-4ff4-9b4f-9f8c0e1be995",
    "input": "## Claim\nHere is a claim: The system's official score was 60.9% (micro-F1) [CONTINUE] af [CONTINUE] However, re-scoring our second submission after replacing these 10 files with the ones from our first submission resulted in a lower score of 67.07%. Does the following context support or refute the claim?\n\n## Table\nPaper title: Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents\nTable caption: Table 1: The scores of our three submitted runs for similarity threshold 50%.\nRun ID | Official score | Score with correction\nep_1 | 60.29 | 66.76\nep_2 | [BOLD] 60.90 | [BOLD] 67.35\nep_3 | 60.61 | 67.07\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "37504e87-a51e-4d90-bc49-27f562c1784e",
    "input": "## Claim\nHere is a claim: All G2S models have [CONTINUE] higher entailment compared to S2S. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.\n<bold>Model</bold> | REF \u21d2 GEN <bold>ENT</bold> | REF \u21d2 GEN <bold>CON</bold> | REF \u21d2 GEN <bold>NEU</bold>\nS2S | 38.45 | 11.17 | 50.38\nG2S-GIN | 49.78 | 9.80 | 40.42\nG2S-GAT | 49.48 | 8.09 | 42.43\nG2S-GGNN | 51.32 | 8.82 | 39.86\n[EMPTY] | GEN \u21d2 REF | GEN \u21d2 REF | GEN \u21d2 REF\n<bold>Model</bold> | <bold>ENT</bold> | <bold>CON</bold> | <bold>NEU</bold>\nS2S | 73.79 | 12.75 | 13.46\nG2S-GIN | 76.27 | 10.65 | 13.08\nG2S-GAT | 77.54 | 8.54 | 13.92\nG2S-GGNN | 77.64 | 9.64 | 12.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "6ff4558a-e94b-4bf7-88fc-ab7a7f88c1fa",
    "input": "## Claim\nHere is a claim: Our model does not outperform PG-MMR when trained and tested on the Multi-News dataset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\nTable caption: Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.\n[BOLD] Method | [BOLD] R-1 | [BOLD] R-2 | [BOLD] R-SU\nFirst-1 | 26.83 | 7.25 | 6.46\nFirst-2 | 35.99 | 10.17 | 12.06\nFirst-3 | 39.41 | 11.77 | 14.51\nLexRank Erkan and Radev ( 2004 ) | 38.27 | 12.70 | 13.20\nTextRank Mihalcea and Tarau ( 2004 ) | 38.44 | 13.10 | 13.50\nMMR Carbonell and Goldstein ( 1998 ) | 38.77 | 11.98 | 12.91\nPG-Original Lebanoff et\u00a0al. ( 2018 ) | 41.85 | 12.91 | 16.46\nPG-MMR Lebanoff et\u00a0al. ( 2018 ) | 40.55 | 12.36 | 15.87\nPG-BRNN Gehrmann et\u00a0al. ( 2018 ) | 42.80 | 14.19 | 16.75\nCopyTransformer Gehrmann et\u00a0al. ( 2018 ) | [BOLD] 43.57 | 14.03 | 17.37\nHi-MAP (Our Model) | 43.47 | [BOLD] 14.89 | [BOLD] 17.41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6200a720-4ce2-40ec-a91f-ad7f8ddfd37c",
    "input": "## Claim\nHere is a claim: Compared to CMOW, the hybrid model shows significant differences. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "b3d0a10d-235d-4eb2-bede-f573f3a44812",
    "input": "## Claim\nHere is a claim: BI+IS decoding with single-domain trained models does not achieve gains over both the naive uniform approach and over oracle single-domain models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.\n[BOLD] Language pair | [BOLD] Model type | [BOLD] Oracle model | [BOLD] Decoder configuration  [BOLD] Uniform | [BOLD] Decoder configuration  [BOLD] BI + IS\nes-en | Unadapted | 36.4 | 34.7 | 36.6\nes-en | No-reg | 36.6 | 34.8 | -\nes-en | EWC | 37.0 | 36.3 | [BOLD] 37.2\nen-de | Unadapted | 36.4 | 26.8 | 38.8\nen-de | No-reg | 41.7 | 31.8 | -\nen-de | EWC | 42.1 | 38.6 | [BOLD] 42.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "295d9f73-4ef3-403e-928f-c6924a9ff349",
    "input": "## Claim\nHere is a claim: The models in the upper portion (1-6) use only dialogue history and turn-level user goals, which are assumed to be error-free. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "34c54516-c7a3-4764-9a4f-5877b980437c",
    "input": "## Claim\nHere is a claim: The first subset contains results by our system, second subset contains results by Refresh, and third subset contains results by ExtAbsRL. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "d977923d-c6e5-4ad6-8a6d-b142c7757162",
    "input": "## Claim\nHere is a claim: Support Vector Machines (SVM) were used as baseline and the results of other proposed methods have been compared with them. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 8: Sentiment classification evaluation, using different classifiers on the test set.\nClassifier | Positive Sentiment Precision | Positive Sentiment Recall | Positive Sentiment Fscore\nSVM-w/o neg. | 0.57 | 0.72 | 0.64\nSVM-Punct. neg. | 0.58 | 0.70 | 0.63\nSVM-our-neg. | 0.58 | 0.73 | 0.65\nCNN | 0.63 | 0.83 | 0.72\nCNN-LSTM | 0.71 | 0.72 | 0.72\nCNN-LSTM-Our-neg-Ant | [BOLD] 0.78 | [BOLD] 0.77 | [BOLD] 0.78\n[EMPTY] | Negative Sentiment | Negative Sentiment | Negative Sentiment\n[EMPTY] | Precision | Recall | Fscore\nSVM-w/o neg. | 0.78 | 0.86 | 0.82\nSVM-Punct. neg. | 0.78 | 0.87 | 0.83\nSVM-Our neg. | 0.80 | 0.87 | 0.83\nCNN | 0.88 | 0.72 | 0.79\nCNN-LSTM. | 0.83 | 0.83 | 0.83\nCNN-LSTM-our-neg-Ant | [BOLD] 0.87 | [BOLD] 0.87 | [BOLD] 0.87\n[EMPTY] | Train | [EMPTY] | Test\nPositive tweets | 5121 | [EMPTY] | 1320\nNegative tweets | 9094 | [EMPTY] | 2244\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "a3e27840-583c-4bc7-a60e-1acc2790dea2",
    "input": "## Claim\nHere is a claim: Table 5 shows that uniform ensembling outperforms all oracle models except es-en Bio, especially on general domains. Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.\n[BOLD] Decoder configuration | [BOLD] es-en  [BOLD] Health | [BOLD] es-en  [BOLD] Bio | [BOLD] en-de  [BOLD] News | [BOLD] en-de  [BOLD] TED | [BOLD] en-de  [BOLD] IT\nOracle model | 35.9 | 36.1 | 37.8 | 24.1 | 39.6\nUniform | 33.1 | 36.4 | 21.9 | 18.4 | 38.9\nIdentity-BI | 35.0 | 36.6 | 32.7 | 25.3 | 42.6\nBI | 35.9 | 36.5 | 38.0 | 26.1 | [BOLD] 44.7\nIS | [BOLD] 36.0 | 36.8 | 37.5 | 25.6 | 43.3\nBI + IS | [BOLD] 36.0 | [BOLD] 36.9 | [BOLD] 38.4 | [BOLD] 26.4 | [BOLD] 44.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ec1c8929-3671-4e7f-8738-bf6ed393975e",
    "input": "## Claim\nHere is a claim: The results show that it is better to add knowledge as features when the knowledge quality is high than compile them into constraints. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nDataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb\n[ITALIC] Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 | [BOLD] 76.41\n[ITALIC] WinoCoref | AntePre | 68.37 | 74.32 | \u2014\u2013 | 88.48 | 88.95 | [BOLD] 89.32\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5879912f-a133-4644-b984-422b306d3d34",
    "input": "## Claim\nHere is a claim: In general, we found when the parameter budget is the same, deeper DCGCN models can obtain better results than the shallower ones. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 7: Comparisons of different DCGCN models under almost the same parameter budget.\n[BOLD] Model | D | #P | B | C\nDCGCN(1) | 300 | 10.9M | 20.9 | 52.0\nDCGCN(2) | 180 | 10.9M | [BOLD] 22.2 | [BOLD] 52.3\nDCGCN(2) | 240 | 11.3M | 22.8 | 52.8\nDCGCN(4) | 180 | 11.4M | [BOLD] 23.4 | [BOLD] 53.4\nDCGCN(1) | 420 | 12.6M | 22.2 | 52.4\nDCGCN(2) | 300 | 12.5M | 23.8 | 53.8\nDCGCN(3) | 240 | 12.3M | [BOLD] 23.9 | [BOLD] 54.1\nDCGCN(2) | 360 | 14.0M | 24.2 | [BOLD] 54.4\nDCGCN(3) | 300 | 14.0M | [BOLD] 24.4 | 54.2\nDCGCN(2) | 420 | 15.6M | 24.1 | 53.7\nDCGCN(4) | 300 | 15.6M | [BOLD] 24.6 | [BOLD] 54.8\nDCGCN(3) | 420 | 18.6M | 24.5 | 54.6\nDCGCN(4) | 360 | 18.4M | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "08c3b235-a0d2-41d2-a7f1-046b2f03f269",
    "input": "## Claim\nHere is a claim: [CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss increases PP, sometimes at a slight cost of semantic preservation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 3: Literature results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.\n[EMPTY] | Acc | Sim | PP | GM\nM0: shen-1 | 0.694 | 0.728 | [BOLD] 22.3 | 8.81\nM1: M0 [ITALIC] +para | 0.702 | 0.747 | 23.6 | 11.7\nM2: M0 [ITALIC] +cyc | 0.692 | 0.781 | 49.9 | [BOLD] 12.8\nM3: M0 [ITALIC] +cyc+lang | 0.698 | 0.754 | 39.2 | 12.0\nM4: M0 [ITALIC] +cyc+para | 0.702 | 0.757 | 33.9 | [BOLD] 12.8\nM5: M0 [ITALIC] +cyc+para+lang | 0.688 | 0.753 | 28.6 | 11.8\nM6: M0 [ITALIC] +cyc+2d | 0.704 | [BOLD] 0.794 | 63.2 | [BOLD] 12.8\nM7: M6+ [ITALIC] para+lang | 0.706 | 0.768 | 49.0 | [BOLD] 12.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "dfc949b8-d660-4336-b9bd-4cd308f74cd4",
    "input": "## Claim\nHere is a claim: BI and IS both individually outperform the oracle for all but IS-News, [CONTINUE] With adaptive decoding, we do not need to assume whether a uniform ensemble or a single model might perform better for some potentially unknown domain. Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.\n[BOLD] Decoder configuration | [BOLD] es-en  [BOLD] Health | [BOLD] es-en  [BOLD] Bio | [BOLD] en-de  [BOLD] News | [BOLD] en-de  [BOLD] TED | [BOLD] en-de  [BOLD] IT\nOracle model | 35.9 | 36.1 | 37.8 | 24.1 | 39.6\nUniform | 33.1 | 36.4 | 21.9 | 18.4 | 38.9\nIdentity-BI | 35.0 | 36.6 | 32.7 | 25.3 | 42.6\nBI | 35.9 | 36.5 | 38.0 | 26.1 | [BOLD] 44.7\nIS | [BOLD] 36.0 | 36.8 | 37.5 | 25.6 | 43.3\nBI + IS | [BOLD] 36.0 | [BOLD] 36.9 | [BOLD] 38.4 | [BOLD] 26.4 | [BOLD] 44.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2861b4ef-ab1a-4f84-b5ff-37cfcd01f477",
    "input": "## Claim\nHere is a claim: Despite the models having fewer examples of bigger graphs to learn from, this does not lead to worse performance when handling graphs with higher diameters. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "f355495c-ff8d-491e-9025-38c4710543de",
    "input": "## Claim\nHere is a claim: the main challenge of the sentiment classification task is to extract the information from the context. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "952269e3-91ba-422b-9157-7a84243d785f",
    "input": "## Claim\nHere is a claim: Analyzing Table 3, we can observe that all values of precision using the English corpora have higher scores when compared with the Portuguese corpora. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1cff5f89-184c-479e-a5ff-7c03244c4ab8",
    "input": "## Claim\nHere is a claim: The contribution of the cue is clear: in particular, the relatively low precision of using the parser introduces more out of scope relations than in-scope. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "b9d6270e-87be-44f9-982a-a037c7cfd0bd",
    "input": "## Claim\nHere is a claim: The results furthermore show that the sdps based on the Stanford Basic (SB) representation do not provide the best performance, followed by the CoNLL08 representation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Syntactic Dependency Representations in Neural Relation Classification\nTable caption: Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.\n[BOLD] Representation | [BOLD] Hyper parameters Filter size | [BOLD] Hyper parameters Num. Feature maps | [BOLD] Hyper parameters Activation func. | [BOLD] Hyper parameters L2 Reg. | [BOLD] Hyper parameters Learning rate | [BOLD] Hyper parameters Dropout Prob. | [BOLD] F1.(avg. in 5-fold) with default values | [BOLD] F1.(avg. in 5-fold) with optimal values\nCoNLL08 | 4-5 | 1000 | Softplus | 1.15e+01 | 1.13e-03 | 1 | 73.34 | 74.49\nSB | 4-5 | 806 | Sigmoid | 8.13e-02 | 1.79e-03 | 0.87 | 72.83 | [BOLD] 75.05\nUD v1.3 | 5 | 716 | Softplus | 1.66e+00 | 9.63E-04 | 1 | 68.93 | 69.57\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "48134843-e7f0-4987-98fd-e830f58e1b3a",
    "input": "## Claim\nHere is a claim: [CONTINUE] When removing sweat smile and confused accuracy increased, Does the following context support or refute the claim?\n\n## Table\nPaper title: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations\nTable caption: Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.\n[BOLD] Emoji alias | [BOLD] N | [BOLD] emoji # | [BOLD] emoji % | [BOLD] no-emoji # | [BOLD] no-emoji % | [BOLD] \u0394%\nmask | 163 | 154 | 94.48 | 134 | 82.21 | - 12.27\ntwo_hearts | 87 | 81 | 93.10 | 77 | 88.51 | - 4.59\nheart_eyes | 122 | 109 | 89.34 | 103 | 84.43 | - 4.91\nheart | 267 | 237 | 88.76 | 235 | 88.01 | - 0.75\nrage | 92 | 78 | 84.78 | 66 | 71.74 | - 13.04\ncry | 116 | 97 | 83.62 | 83 | 71.55 | - 12.07\nsob | 490 | 363 | 74.08 | 345 | 70.41 | - 3.67\nunamused | 167 | 121 | 72.46 | 116 | 69.46 | - 3.00\nweary | 204 | 140 | 68.63 | 139 | 68.14 | - 0.49\njoy | 978 | 649 | 66.36 | 629 | 64.31 | - 2.05\nsweat_smile | 111 | 73 | 65.77 | 75 | 67.57 | 1.80\nconfused | 77 | 46 | 59.74 | 48 | 62.34 | 2.60\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "02bcff71-1cf2-4b28-9482-a7c0e3cab06a",
    "input": "## Claim\nHere is a claim: The comparison shows the powerful advantage of LSTM embeddings over the standard word embeddings in capturing word semantics, that is, semantic similarity. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VI: Correlations for Word Similarity Tests\nDataset (EN-) | GloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\nWS-353-ALL | 0.612 | 0.7156 | 0.634 | 0.622 | 0.173 | 0.690 | 0.657\nSIMLEX-999 | 0.359 | 0.3939 | 0.295 | 0.355 | 0.090 | 0.380 | 0.381\nVERB-143 | 0.326 | 0.4430 | 0.255 | 0.271 | 0.293 | 0.271 | 0.348\nSimVerb-3500 | 0.193 | 0.2856 | 0.184 | 0.197 | 0.035 | 0.234 | 0.245\nWS-353-REL | 0.578 | 0.6457 | 0.595 | 0.578 | 0.134 | 0.695 | 0.619\nRW-STANF. | 0.378 | 0.4858 | 0.316 | 0.373 | 0.122 | 0.390 | 0.382\nYP-130 | 0.524 | 0.5211 | 0.353 | 0.482 | 0.169 | 0.420 | 0.589\nMEN-TR-3k | 0.710 | 0.7528 | 0.684 | 0.696 | 0.298 | 0.769 | 0.725\nRG-65 | 0.768 | 0.8051 | 0.736 | 0.732 | 0.338 | 0.761 | 0.774\nMTurk-771 | 0.650 | 0.6712 | 0.593 | 0.623 | 0.199 | 0.665 | 0.671\nWS-353-SIM | 0.682 | 0.7883 | 0.713 | 0.702 | 0.220 | 0.720 | 0.720\nMC-30 | 0.749 | 0.8112 | 0.799 | 0.726 | 0.330 | 0.735 | 0.776\nMTurk-287 | 0.649 | 0.6645 | 0.591 | 0.631 | 0.295 | 0.674 | 0.634\nAverage | 0.552 | 0.6141 | 0.519 | 0.538 | 0.207 | 0.570 | 0.579\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "991b8ed1-5e3e-408a-9da7-915c600a7d0c",
    "input": "## Claim\nHere is a claim: These results demonstrate that NeuralTDabt indeed learns to generate non-extractive summaries and performs better than a regular extractive baseline, which randomly select sentences from the given document. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "dacd4e41-a392-455b-856c-0a23df7ceab8",
    "input": "## Claim\nHere is a claim: by comparing it to extractive baseline NeuralTD, our proposed abstractive model NeuralTDabt exhibits better performance than extractive baseline, improving 0.9 ROUGE-1 points and 0.3 ROUGE-L points. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "84aa63b6-4dad-4cc3-885a-d5d43259dae9",
    "input": "## Claim\nHere is a claim: For Waseem (2016) we see that there is a significant difference in the estimated rates at which tweets are classified as racist across groups, with higher rates for the white group. Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 2: Experiment 1\nDataset | Class | \u02c6 [ITALIC] piblack | \u02c6 [ITALIC] piwhite | [ITALIC] t | [ITALIC] p | \u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite\n[ITALIC] Waseem and Hovy | Racism | 0.001 | 0.003 | -20.818 | *** | 0.505\n[EMPTY] | Sexism | 0.083 | 0.048 | 101.636 | *** | 1.724\n[ITALIC] Waseem | Racism | 0.001 | 0.001 | 0.035 | [EMPTY] | 1.001\n[EMPTY] | Sexism | 0.023 | 0.012 | 64.418 | *** | 1.993\n[EMPTY] | Racism and sexism | 0.002 | 0.001 | 4.047 | *** | 1.120\n[ITALIC] Davidson et al. | Hate | 0.049 | 0.019 | 120.986 | *** | 2.573\n[EMPTY] | Offensive | 0.173 | 0.065 | 243.285 | *** | 2.653\n[ITALIC] Golbeck et al. | Harassment | 0.032 | 0.023 | 39.483 | *** | 1.396\n[ITALIC] Founta et al. | Hate | 0.111 | 0.061 | 122.707 | *** | 1.812\n[EMPTY] | Abusive | 0.178 | 0.080 | 211.319 | *** | 2.239\n[EMPTY] | Spam | 0.028 | 0.015 | 63.131 | *** | 1.854\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "84e4fdca-7ec9-45af-a79e-d7937dbea06f",
    "input": "## Claim\nHere is a claim: This can be attributed to the fact that the proposed approach relies on more than one concept words, while GloVe only uses the representation of the top concept word to classify the image. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VIII: Precision scores for the Semantic Analogy Test\nQuestions Subset | # of Questions Seen | GloVe | Word2Vec | Proposed\nAll | 8783 | 78.94 | 81.03 | 79.96\nAt least one | 1635 | 67.58 | 70.89 | 67.89\nconcept word | 1635 | 67.58 | 70.89 | 67.89\nAll concept words | 110 | 77.27 | 89.09 | 83.64\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "7b320190-e9bb-4603-8a20-b4a8f2c7fdbc",
    "input": "## Claim\nHere is a claim: DCGCN model is not able to achieve a competitive BLEU points (33.2) by using 0.3M external data, while GraphLSTM achieves a higher score of 33.6 by using 2M data and Seq2SeqK achieves an even higher score of 33.8 by using 20M data. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M\n[BOLD] Model | [BOLD] External | B\nSeq2SeqK (Konstas et al.,  2017 ) | - | 22.0\nGraphLSTM (Song et al.,  2018 ) | - | 23.3\nGCNSEQ (Damonte and Cohen,  2019 ) | - | 24.4\nDCGCN(single) | - | 25.9\nDCGCN(ensemble) | - | [BOLD] 28.2\nTSP (Song et al.,  2016 ) | ALL | 22.4\nPBMT (Pourdamghani et al.,  2016 ) | ALL | 26.9\nTree2Str (Flanigan et al.,  2016 ) | ALL | 23.0\nSNRG (Song et al.,  2017 ) | ALL | 25.6\nSeq2SeqK (Konstas et al.,  2017 ) | 0.2M | 27.4\nGraphLSTM (Song et al.,  2018 ) | 0.2M | 28.2\nDCGCN(single) | 0.1M | 29.0\nDCGCN(single) | 0.2M | [BOLD] 31.6\nSeq2SeqK (Konstas et al.,  2017 ) | 2M | 32.3\nGraphLSTM (Song et al.,  2018 ) | 2M | 33.6\nSeq2SeqK (Konstas et al.,  2017 ) | 20M | 33.8\nDCGCN(single) | 0.3M | 33.2\nDCGCN(ensemble) | 0.3M | [BOLD] 35.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4fe8ff2e-6610-4a8b-905b-0d92bebf29e1",
    "input": "## Claim\nHere is a claim: GGP-MBCM performs best in model 1, but is significantly worse than the other policies in models 2 and 3. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.\nGP-MBCM | ACER | PPO | ALDM | GDPL\n1.666 | 0.775 | 0.639 | 1.069 | [BOLD] 0.238\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "423aa89a-9bca-4bf2-a8aa-78154555b63b",
    "input": "## Claim\nHere is a claim: For example, a is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 21.2% of COPA training instances. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nCue | App. | Prod. | Cov.\nin | 47 | 55.3 | 9.40\nwas | 55 | 61.8 | 11.0\nto | 82 | 40.2 | 16.4\nthe | 85 | 38.8 | 17.0\na | 106 | 57.5 | 21.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "855a3b67-9407-44f3-89a7-0e9ca26da983",
    "input": "## Claim\nHere is a claim: However, the KL divergence between human dialog policy and RL agents policy is quite large, which means the training has more space to improve. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.\nGP-MBCM | ACER | PPO | ALDM | GDPL\n1.666 | 0.775 | 0.639 | 1.069 | [BOLD] 0.238\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "88054a7a-5435-4928-9448-8ae40b53e01b",
    "input": "## Claim\nHere is a claim: we use the sum of the similarities between the question and the two sentences in the passage as a memory initialization Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\n[EMPTY] | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK\nMQAN | 72.30 | 60.91 | 41.82 | 53.95\n+ coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold>\nESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37\n+ coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "7f397cbf-19ef-4fd7-a5e0-22e8af5690f9",
    "input": "## Claim\nHere is a claim: For example, a chatbot that generates a confusing or inappropriate response should be assigned a low efficiency score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c7dbd5dd-6bea-46f7-a832-677c3004a5da",
    "input": "## Claim\nHere is a claim: the performance of GloVe and Word2vec remain unchanged if concept words are unseen in the training corpus. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VIII: Precision scores for the Semantic Analogy Test\nQuestions Subset | # of Questions Seen | GloVe | Word2Vec | Proposed\nAll | 8783 | 78.94 | 81.03 | 79.96\nAt least one | 1635 | 67.58 | 70.89 | 67.89\nconcept word | 1635 | 67.58 | 70.89 | 67.89\nAll concept words | 110 | 77.27 | 89.09 | 83.64\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "deefe413-93b4-46a5-915c-e5ff31c48ab8",
    "input": "## Claim\nHere is a claim: Our model does not obtain the best performance on three out of the four datasets. Does the following context support or refute the claim?\n\n## Table\nPaper title: Keyphrase Generation for Scientific Articles using GANs\nTable caption: Table 2: \u03b1-nDCG@5 metrics\nModel | Inspec | Krapivin | NUS | KP20k\nCatseq | 0.87803 | 0.781 | 0.82118 | 0.804\nCatseq-RL | 0.8602 | [BOLD] 0.786 | 0.83 | 0.809\nGAN | [BOLD] 0.891 | 0.771 | [BOLD] 0.853 | [BOLD] 0.85\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6c9eeac3-a181-46e5-9b58-03aece5eb01a",
    "input": "## Claim\nHere is a claim: All other agents outperform our DKRN agent with a large margin. Does the following context support or refute the claim?\n\n## Table\nPaper title: Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation\nTable caption: Table 5: Results of the Human Rating on CWC.\nSystem | Succ. (%) | Smoothness\nRetrieval-Stgy\u00a0 | 54.0 | 2.48\nPMI\u00a0 | 46.0 | 2.56\nNeural\u00a0 | 36.0 | 2.50\nKernel\u00a0 | 58.0 | 2.48\nDKRN (ours) | [BOLD] 88.0 | [BOLD] 3.22\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "57009ccc-b37d-4ba1-a3bf-60fbc2ac3dfc",
    "input": "## Claim\nHere is a claim: However, this alone cannot improve system-level ROUGE to the level that of the ROUGE-based decoder. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e0687e8f-e6c6-43ea-854d-2257d5ec9015",
    "input": "## Claim\nHere is a claim: The system's official score was 60.9% (micro-F1) [CONTINUE] af [CONTINUE] Therefore, we report both the official score (from our second submission) and the result of re-scoring our second submission after replacing these 10 files with the ones from our first submission. Does the following context support or refute the claim?\n\n## Table\nPaper title: Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents\nTable caption: Table 1: The scores of our three submitted runs for similarity threshold 50%.\nRun ID | Official score | Score with correction\nep_1 | 60.29 | 66.76\nep_2 | [BOLD] 60.90 | [BOLD] 67.35\nep_3 | 60.61 | 67.07\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "8e604b9f-d3fa-4a47-bd09-06cd2caf18c1",
    "input": "## Claim\nHere is a claim: For German descriptions, The results are 11.05% better on average compared to (Gella et al., 2017) in symmetric mode. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task\nTable caption: Table 2: Image-caption ranking results for German (Multi30k)\n[EMPTY] | Image to Text R@1 | Image to Text R@5 | Image to Text R@10 | Image to Text Mr | Text to Image R@1 | Text to Image R@5 | Text to Image R@10 | Text to Image Mr | Alignment\n[BOLD] symmetric | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nParallel\u00a0gella:17 | 28.2 | 57.7 | 71.3 | 4 | 20.9 | 46.9 | 59.3 | 6 | -\nMono | 34.2 | 67.5 | 79.6 | 3 | 26.5 | 54.7 | 66.2 | 4 | -\nFME | 36.8 | 69.4 | 80.8 | 2 | 26.6 | 56.2 | 68.5 | 4 | 76.81%\nAME | [BOLD] 39.6 | [BOLD] 72.7 | [BOLD] 82.7 | [BOLD] 2 | [BOLD] 28.9 | [BOLD] 58.0 | [BOLD] 68.7 | [BOLD] 4 | 66.91%\n[BOLD] asymmetric | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nPivot\u00a0gella:17 | 28.2 | 61.9 | 73.4 | 3 | 22.5 | 49.3 | 61.7 | 6 | -\nParallel\u00a0gella:17 | 30.2 | 60.4 | 72.8 | 3 | 21.8 | 50.5 | 62.3 | 5 | -\nMono | [BOLD] 42.0 | 72.5 | 83.0 | 2 | 29.6 | 58.4 | 69.6 | 4 | -\nFME | 40.5 | 73.3 | 83.4 | 2 | 29.6 | 59.2 | [BOLD] 72.1 | 3 | 76.81%\nAME | 40.5 | [BOLD] 74.3 | [BOLD] 83.4 | [BOLD] 2 | [BOLD] 31.0 | [BOLD] 60.5 | 70.6 | [BOLD] 3 | 73.10%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "bd3a799a-c562-4476-ad82-23a43d147582",
    "input": "## Claim\nHere is a claim: The proposed CNN-LSTMOur-neg-Ant does not improve upon the simple CNNLSTM-w/o neg. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 8: Sentiment classification evaluation, using different classifiers on the test set.\nClassifier | Positive Sentiment Precision | Positive Sentiment Recall | Positive Sentiment Fscore\nSVM-w/o neg. | 0.57 | 0.72 | 0.64\nSVM-Punct. neg. | 0.58 | 0.70 | 0.63\nSVM-our-neg. | 0.58 | 0.73 | 0.65\nCNN | 0.63 | 0.83 | 0.72\nCNN-LSTM | 0.71 | 0.72 | 0.72\nCNN-LSTM-Our-neg-Ant | [BOLD] 0.78 | [BOLD] 0.77 | [BOLD] 0.78\n[EMPTY] | Negative Sentiment | Negative Sentiment | Negative Sentiment\n[EMPTY] | Precision | Recall | Fscore\nSVM-w/o neg. | 0.78 | 0.86 | 0.82\nSVM-Punct. neg. | 0.78 | 0.87 | 0.83\nSVM-Our neg. | 0.80 | 0.87 | 0.83\nCNN | 0.88 | 0.72 | 0.79\nCNN-LSTM. | 0.83 | 0.83 | 0.83\nCNN-LSTM-our-neg-Ant | [BOLD] 0.87 | [BOLD] 0.87 | [BOLD] 0.87\n[EMPTY] | Train | [EMPTY] | Test\nPositive tweets | 5121 | [EMPTY] | 1320\nNegative tweets | 9094 | [EMPTY] | 2244\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3085b0e6-eb83-487a-bb86-0533b9fe919b",
    "input": "## Claim\nHere is a claim: When using more natural language text as an additional training resource, the models\u2019 performance is improved dramatically, outperforming the previous state-of-the-art by 10 absolute points. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "22787daa-80c1-4204-aced-20a547c65df4",
    "input": "## Claim\nHere is a claim: [CONTINUE] It can be observed that the learned reward function has good interpretability in that the reward is positive when the dialog gets a full score on each metric, and negative otherwise. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d7e9fa5f-41af-46a2-b8e9-34424334ea6f",
    "input": "## Claim\nHere is a claim: On the other hand, the presence of terms that show positive sentiment or emotions (good, great, win, POSEMO, AFFECT, ASSENT) are among the least distinctive features for a tweet not being labeled as a complaint. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\n[BOLD] Complaints  [BOLD] Feature | [BOLD] Complaints  [ITALIC] r | [BOLD] Not Complaints  [BOLD] Feature | [BOLD] Not Complaints  [ITALIC] r\n[BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams\nnot | .154 | [URL] | .150\nmy | .131 | ! | .082\nworking | .124 | he | .069\nstill | .123 | thank | .067\non | .119 | , | .064\ncan\u2019t | .113 | love | .064\nservice | .112 | lol | .061\ncustomer | .109 | you | .060\nwhy | .108 | great | .058\nwebsite | .107 | win | .058\nno | .104 | \u2019 | .058\n? | .098 | she | .054\nfix | .093 | : | .053\nwon\u2019t | .092 | that | .053\nbeen | .090 | more | .052\nissue | .089 | it | .052\ndays | .088 | would | .051\nerror | .087 | him | .047\nis | .084 | life | .046\ncharged | .083 | good | .046\n[BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)\nVBN | .141 | UH | .104\n$ | .118 | NNP | .098\nVBZ | .114 | PRP | .076\nNN_VBZ | .114 | HT | .076\nPRP$ | .107 | PRP_. | .076\nPRP$_NN | .105 | PRP_RB | .067\nVBG | .093 | NNP_NNP | .062\nCD | .092 | VBP_PRP | .054\nWRB_VBZ | .084 | JJ | .053\nVBZ_VBN | .084 | DT_JJ | .051\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "0a742890-8fda-45d8-b92c-504a7c2153a8",
    "input": "## Claim\nHere is a claim: For all these systems, a three-sentence summarisation is required; so we set T=3 in our experiment. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8b7d76ba-3bdd-45c0-9afe-72d3024ef13d",
    "input": "## Claim\nHere is a claim: Table 4 shows that LRN has the highest EM/F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 4: Exact match/F1-score on SQuad dataset. \u201c#Params\u201d: the parameter number of Base. rnet*: results published by\u00a0Wang et\u00a0al. (2017).\nModel | #Params | Base | +Elmo\nrnet* | - | 71.1/79.5 | -/-\nLSTM | 2.67M | [BOLD] 70.46/78.98 | 75.17/82.79\nGRU | 2.31M | 70.41/ [BOLD] 79.15 | 75.81/83.12\nATR | 1.59M | 69.73/78.70 | 75.06/82.76\nSRU | 2.44M | 69.27/78.41 | 74.56/82.50\nLRN | 2.14M | 70.11/78.83 | [BOLD] 76.14/ [BOLD] 83.83\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2c9c4e5c-d05b-490f-8859-111e19020af5",
    "input": "## Claim\nHere is a claim: Note that GloVe is the pre-trained word vectors in the very basic representation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ac541d7d-74e8-4def-b326-4c2e2c8fc6bd",
    "input": "## Claim\nHere is a claim: Our single model DCGCN(single) achieves 19.0 and 12.1 BLEU points on the En-De and EnCs tasks, respectively, significantly outperforming all the single models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 4: Main results on English-German and English-Czech datasets.\n[BOLD] Model | [BOLD] Type | [BOLD] English-German #P | [BOLD] English-German B | [BOLD] English-German C | [BOLD] English-Czech #P | [BOLD] English-Czech B | [BOLD] English-Czech C\nBoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | -\nCNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | -\nBiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | -\nPB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4\nSeq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8\nGGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3\nDCGCN (ours) | Single | [BOLD]  29.7M | [BOLD] 19.0 | [BOLD] 44.1 | [BOLD]  28.3M | [BOLD] 12.1 | [BOLD] 37.1\nSeq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4\nGGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9\nDCGCN (ours) | Ensemble | [BOLD]  149M | [BOLD] 20.5 | [BOLD] 45.8 | [BOLD]  142M | [BOLD] 13.1 | [BOLD] 37.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "bab35b27-fc3c-4a34-802c-79f633a9de4f",
    "input": "## Claim\nHere is a claim: The model performs significantly worse when trained with hinge loss instead of cross-entropy loss, indicating the importance of the loss function. Does the following context support or refute the claim?\n\n## Table\nPaper title: Building a Production Model for Retrieval-Based Chatbots\nTable caption: Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.\n[BOLD] Model | [BOLD] Parameters | [BOLD] Validation AUC@0.05 | [BOLD] Test AUC@0.05\nBase | 8.0M | [BOLD] 0.871 | 0.816\n4L SRU \u2192 2L LSTM | 7.3M | 0.864 | [BOLD] 0.829\n4L SRU \u2192 2L SRU | 7.8M | 0.856 | [BOLD] 0.829\nFlat \u2192 hierarchical | 12.4M | 0.825 | 0.559\nCross entropy \u2192 hinge loss | 8.0M | 0.765 | 0.693\n6.6M \u2192 1M examples | 8.0M | 0.835 | 0.694\n6.6M \u2192 100K examples | 8.0M | 0.565 | 0.417\n200 \u2192 100 negatives | 8.0M | 0.864 | 0.647\n200 \u2192 10 negatives | 8.0M | 0.720 | 0.412\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "61264049-8c30-45ff-9a15-8903df2d4727",
    "input": "## Claim\nHere is a claim: the results show that InferSent yields the highest correlation between METEOR and human evaluation, in both \u03c1 and r. However, we see that InferSent has the lowest precision on the \u201cgood\u201d summaries and the highest precision on the \u201cbad\u201d summaries Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nMetric | [ITALIC] \u03c1 | [ITALIC] r | G-Pre | G-Rec\nROUGE-1 | .290 | .304 | .392 | .428\nROUGE-2 | .259 | .278 | .408 | .444\nROUGE-L | .274 | .297 | .390 | .426\nROUGE-SU4 | .282 | .279 | .404 | .440\nBLEU-1 | .256 | .281 | .409 | .448\nBLEU-2 | .301 | .312 | .411 | .446\nBLEU-3 | .317 | .312 | .409 | .444\nBLEU-4 | .311 | .307 | .409 | .446\nBLEU-5 | .308 | .303 | .420 | .459\nMETEOR | .305 | .285 | .409 | .444\nInferSent-Cosine | [BOLD] .329 | [BOLD] .339 | .417 | .460\nBERT-Cosine | .312 | .335 | [BOLD] .440 | [BOLD] .484\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "582f8cd2-36bc-478b-a34b-95e07733d714",
    "input": "## Claim\nHere is a claim: We observe that the transfer baseline that directly uses rationale as augmented supervision (RA-TRANS) outperforms ORACLE in all aspects. Does the following context support or refute the claim?\n\n## Table\nPaper title: Deriving Machine Attention from Human Rationales\nTable caption: Table 3: Accuracy of transferring between aspects. Models with \u2020 use labeled data from source aspects. Models with \u2021 use human rationales on the target aspect.\nSource | Target | Svm | Ra-Svm\u2021 | Ra-Cnn\u2021 | Trans\u2020 | Ra-Trans\u2021\u2020 | Ours\u2021\u2020 | Oracle\u2020\nBeer aroma+palate | Beer look | 74.41 | 74.83 | 74.94 | 72.75 | 76.41 | [BOLD] 79.53 | 80.29\nBeer look+palate | Beer aroma | 68.57 | 69.23 | 67.55 | 69.92 | 76.45 | [BOLD] 77.94 | 78.11\nBeer look+aroma | Beer palate | 63.88 | 67.82 | 65.72 | 74.66 | 73.40 | [BOLD] 75.24 | 75.50\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "7a24d9e2-8e4f-4318-83fc-7fdad95942a0",
    "input": "## Claim\nHere is a claim: When using the same amount of 0.2M data, the performance of DCGCN is 4.2 and 3.4 BLEU points higher than Seq2SeqK and GraphLSTM. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M\n[BOLD] Model | [BOLD] External | B\nSeq2SeqK (Konstas et al.,  2017 ) | - | 22.0\nGraphLSTM (Song et al.,  2018 ) | - | 23.3\nGCNSEQ (Damonte and Cohen,  2019 ) | - | 24.4\nDCGCN(single) | - | 25.9\nDCGCN(ensemble) | - | [BOLD] 28.2\nTSP (Song et al.,  2016 ) | ALL | 22.4\nPBMT (Pourdamghani et al.,  2016 ) | ALL | 26.9\nTree2Str (Flanigan et al.,  2016 ) | ALL | 23.0\nSNRG (Song et al.,  2017 ) | ALL | 25.6\nSeq2SeqK (Konstas et al.,  2017 ) | 0.2M | 27.4\nGraphLSTM (Song et al.,  2018 ) | 0.2M | 28.2\nDCGCN(single) | 0.1M | 29.0\nDCGCN(single) | 0.2M | [BOLD] 31.6\nSeq2SeqK (Konstas et al.,  2017 ) | 2M | 32.3\nGraphLSTM (Song et al.,  2018 ) | 2M | 33.6\nSeq2SeqK (Konstas et al.,  2017 ) | 20M | 33.8\nDCGCN(single) | 0.3M | 33.2\nDCGCN(ensemble) | 0.3M | [BOLD] 35.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2bd4f990-c9ea-44f0-a468-c6d4f6ff7ae5",
    "input": "## Claim\nHere is a claim: That is, the agent is informative and successful but forgets to ask what type of food users want to order occasionally. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "51729d04-4ea1-4472-8452-f5cb29bddfdc",
    "input": "## Claim\nHere is a claim: While Glorot achieves slightly better results on BShift and TopConst, CMOW's ability to memorize word content is improved by a wide [CONTINUE] margin by our initialization strategy. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 7: Scores for initialization strategies on probing tasks.\nInitialization | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\nN(0,0.1) | 29.7 | 71.5 | 82.0 | 78.5 | 60.1 | 80.5 | 76.3 | 74.7 | [BOLD] 51.3 | 52.5\nGlorot | 31.3 | [BOLD] 72.3 | 81.8 | 78.7 | 59.4 | 81.3 | 76.6 | [BOLD] 74.6 | 50.4 | 57.0\nOur paper | [BOLD] 35.1 | 70.8 | [BOLD] 82.0 | [BOLD] 80.2 | [BOLD] 61.8 | [BOLD] 82.8 | [BOLD] 79.7 | 74.2 | 50.7 | [BOLD] 72.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ba7d16da-1cc2-4350-aac4-6bc6e31ada78",
    "input": "## Claim\nHere is a claim: The results of CLUSTER+KCP indicate that pre-clustering of documents to topics is not beneficial, performing substantially worse than our joint model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\n[BOLD] Model | R | MUC P | [ITALIC] F1 | R | B3 P | [ITALIC] F1 | R | CEAF- [ITALIC] e P | [ITALIC] F1 | CoNLL  [ITALIC] F1\n[BOLD] Baselines | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nCluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5\nCV Cybulska and Vossen ( 2015a ) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73\nKCP Kenyon-Dean et\u00a0al. ( 2018 ) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69\nCluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6\n[BOLD] Model Variants | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nDisjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5\nJoint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | [BOLD] 79.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4df10533-fe4d-4bca-a7ae-e944285f1f9a",
    "input": "## Claim\nHere is a claim: These experiments show that the number of factors giving the best performance may vary depending on the underlying data distribution. Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 3: Performance comparison of our model with different values of m on the two datasets.\n[ITALIC] m | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1\n1 | 0.541 | 0.595 | [BOLD] 0.566 | 0.495 | 0.621 | 0.551\n2 | 0.521 | 0.597 | 0.556 | 0.482 | 0.656 | 0.555\n3 | 0.490 | 0.617 | 0.547 | 0.509 | 0.633 | 0.564\n4 | 0.449 | 0.623 | 0.522 | 0.507 | 0.652 | [BOLD] 0.571\n5 | 0.467 | 0.609 | 0.529 | 0.488 | 0.677 | 0.567\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "08e785fd-5276-4bfa-89cb-743853a254f3",
    "input": "## Claim\nHere is a claim: The results in Table 3 show that translation quality of LRN is significantly worse than that of GRU (-0.57 BLEU). Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.\nModel | #Params | BLEU | Train | Decode\nGNMT | - | 24.61 | - | -\nGRU | 206M | 26.28 | 2.67 | 45.35\nATR | 122M | 25.70 | 1.33 | [BOLD] 34.40\nSRU | 170M | 25.91 | 1.34 | 42.84\nLRN | 143M | 26.26 | [BOLD] 0.99 | 36.50\noLRN | 164M | [BOLD] 26.73 | 1.15 | 40.19\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "42cd498e-f308-4f89-ad6b-90f76be30594",
    "input": "## Claim\nHere is a claim: Comparing layers 1 through 4, we see that in 3/5 target languages (Ar, Ru, Zh), POS tagging accuracy peaks at layer 4 and does not improve at lower layers, with some drops at layers 1 and 2. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks\nTable caption: Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. \u201cEn\u201d column is an English autoencoder. BLEU scores are given for reference.\n[ITALIC] k | Ar | Es | Fr | Ru | Zh | En\nPOS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy\n0 | 88.0 | 87.9 | 87.9 | 87.8 | 87.7 | 87.4\n1 | 92.4 | 91.9 | 92.1 | 92.1 | 91.5 | 89.4\n2 | 91.9 | 91.8 | 91.8 | 91.8 | 91.3 | 88.3\n3 | 92.0 | 92.3 | 92.1 | 91.6 | 91.2 | 87.9\n4 | 92.1 | 92.4 | 92.5 | 92.0 | 90.5 | 86.9\nSEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy\n0 | 81.9 | 81.9 | 81.8 | 81.8 | 81.8 | 81.2\n1 | 87.9 | 87.7 | 87.8 | 87.9 | 87.7 | 84.5\n2 | 87.4 | 87.5 | 87.4 | 87.3 | 87.2 | 83.2\n3 | 87.8 | 87.9 | 87.9 | 87.3 | 87.3 | 82.9\n4 | 88.3 | 88.6 | 88.4 | 88.1 | 87.7 | 82.1\nBLEU | BLEU | BLEU | BLEU | BLEU | BLEU | BLEU\n[EMPTY] | 32.7 | 49.1 | 38.5 | 34.2 | 32.1 | 96.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "127db6c3-81b7-450c-8784-9b686fdb189b",
    "input": "## Claim\nHere is a claim: As shown in Table 8, G2S approaches outperform the S2S baseline. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 8: Fraction of elements in the output that are not present in the input (ADDED) and the fraction of elements in the input graph that are missing in the generated sentence (MISS), for the test set of LDC2017T10. The token lemmas are used in the comparison. GOLD refers to the reference sentences.\n<bold>Model</bold> | <bold>ADDED</bold> | <bold>MISS</bold>\nS2S | 47.34 | 37.14\nG2S-GIN | 48.67 | 33.64\nG2S-GAT | 48.24 | 33.73\nG2S-GGNN | 48.66 | 34.06\nGOLD | 50.77 | 28.35\n[EMPTY] | [EMPTY] | [EMPTY]\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "1bcfc28e-4aa6-4a9b-a26f-00cb129437cb",
    "input": "## Claim\nHere is a claim: In analogy 2, all relations are different. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "966334f3-986a-4d6b-8dcd-efda97f994b6",
    "input": "## Claim\nHere is a claim: [CONTINUE] Selective attention mechanisms like sparsemax and especially TVMAX reduce repetition, as measured by the REP metric reported in Table 1. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.\n[EMPTY] | MSCOCO spice | MSCOCO cider | MSCOCO rouge [ITALIC] L | MSCOCO bleu4 | MSCOCO meteor | MSCOCO rep\u2193 | Flickr30k spice | Flickr30k cider | Flickr30k rouge [ITALIC] L | Flickr30k bleu4 | Flickr30k meteor | Flickr30k rep\u2193\nsoftmax | 18.4 | 0.967 | 52.9 | 29.9 | 24.9 | 3.76 | 13.5 | 0.443 | 44.2 | 19.9 | 19.1 | 6.09\nsparsemax | [BOLD] 18.9 | [BOLD] 0.990 | [BOLD] 53.5 | [BOLD] 31.5 | [BOLD] 25.3 | 3.69 | [BOLD] 13.7 | [BOLD] 0.444 | [BOLD] 44.3 | [BOLD] 20.7 | [BOLD] 19.3 | 5.84\nTVmax | 18.5 | 0.974 | 53.1 | 29.9 | 25.1 | [BOLD] 3.17 | 13.3 | 0.438 | 44.2 | 20.5 | 19.0 | [BOLD] 3.97\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "845e45a0-fc97-4bc4-afbd-da01039c86c7",
    "input": "## Claim\nHere is a claim: For example, on AMR17, the single DCGCN model is 1 BLEU point higher than the ensemble model of Seq2SeqB. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.\n[BOLD] Model | [BOLD] T | #P | B | C\nSeq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1\nGGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4\nSeq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5\nGGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5\nDCGCN (ours) | S | [BOLD] 19.1M | 27.9 | 57.3\nDCGCN (ours) | E | 92.5M | [BOLD] 30.4 | [BOLD] 59.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3a961802-e664-47df-a85d-d017f7b3250f",
    "input": "## Claim\nHere is a claim: On the same dataset, we have competitive results to Damonte and Cohen (2019). Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "db362f10-8379-467f-820b-0db06904c36e",
    "input": "## Claim\nHere is a claim: The word analogy test was first introduced in [32] to assess the quality of word vectors. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8a0a5630-4ceb-4984-86ac-cfebf28d6f81",
    "input": "## Claim\nHere is a claim: Adding either the global node or the linear combination does not improve the baseline models with only dense connections. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "0caf7d9c-4732-4d3f-9f66-d8b7b5105251",
    "input": "## Claim\nHere is a claim: [CONTINUE] In addition, other words and clusters expressing positive states such as gratitude (thank, great, love) or laughter (lol) are also distinctive for tweets that are not complaints. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\n[BOLD] Complaints  [BOLD] Feature | [BOLD] Complaints  [ITALIC] r | [BOLD] Not Complaints  [BOLD] Feature | [BOLD] Not Complaints  [ITALIC] r\n[BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams\nnot | .154 | [URL] | .150\nmy | .131 | ! | .082\nworking | .124 | he | .069\nstill | .123 | thank | .067\non | .119 | , | .064\ncan\u2019t | .113 | love | .064\nservice | .112 | lol | .061\ncustomer | .109 | you | .060\nwhy | .108 | great | .058\nwebsite | .107 | win | .058\nno | .104 | \u2019 | .058\n? | .098 | she | .054\nfix | .093 | : | .053\nwon\u2019t | .092 | that | .053\nbeen | .090 | more | .052\nissue | .089 | it | .052\ndays | .088 | would | .051\nerror | .087 | him | .047\nis | .084 | life | .046\ncharged | .083 | good | .046\n[BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)\nVBN | .141 | UH | .104\n$ | .118 | NNP | .098\nVBZ | .114 | PRP | .076\nNN_VBZ | .114 | HT | .076\nPRP$ | .107 | PRP_. | .076\nPRP$_NN | .105 | PRP_RB | .067\nVBG | .093 | NNP_NNP | .062\nCD | .092 | VBP_PRP | .054\nWRB_VBZ | .084 | JJ | .053\nVBZ_VBN | .084 | DT_JJ | .051\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "13188ec1-d49c-4625-a0dd-b375315eb448",
    "input": "## Claim\nHere is a claim: The first one models the agenda state space as discrete and predefined, while the other agent encodes a stochastic latent space for agenda representation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "53df7857-7812-405d-909a-cebde5395c17",
    "input": "## Claim\nHere is a claim: We observe that predictive performance is relatively consistent across all domains with two exceptions ('Food & Beverage' consistently shows lower performance, while 'Other' achieves higher performance) when using all the data available from the other domains. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 9: Performance of models trained with tweets from one domain and tested on other domains. All results are reported in ROC AUC. The All line displays results on training on all categories except the category in testing.\n[BOLD] Test | F&B | A | R | Ca | Se | So | T | E | O\n[BOLD] Train | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nFood & Bev. | \u2013 | 58.1 | 52.5 | 66.4 | 59.7 | 58.9 | 54.1 | 61.4 | 53.7\nApparel | 63.9 | \u2013 | 74.4 | 65.1 | 70.8 | 71.2 | 68.5 | 76.9 | 85.6\nRetail | 58.8 | 74.4 | \u2013 | 70.1 | 72.6 | 69.9 | 68.7 | 69.6 | 82.7\nCars | 68.7 | 61.1 | 65.1 | \u2013 | 58.8 | 67. | 59.3 | 62.9 | 68.2\nServices | 65. | 74.2 | 75.8 | 74. | \u2013 | 68.8 | 74.2 | 77.9 | 77.9\nSoftware | 62. | 74.2 | 68. | 67.9 | 72.8 | \u2013 | 72.8 | 72.1 | 80.6\nTransport | 59.3 | 71.7 | 72.4 | 67. | 74.6 | 75. | \u2013 | 72.6 | 81.7\nElectronics | 61.6 | 75.2 | 71. | 68. | 75. | 69.9 | 68.2 | \u2013 | 78.7\nOther | 56.1 | 71.3 | 72.4 | 70.2 | 73.5 | 67.2 | 68.5 | 71. | \u2013\nAll | 70.3 | 77.7 | 79.5 | 82.0 | 79.6 | 80.1 | 76.8 | 81.7 | 88.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7b4d6ea4-ff74-450c-bbb3-ad38ced11725",
    "input": "## Claim\nHere is a claim: Such case is the most difficult task for this model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6b921c63-e9aa-422c-9036-ad53b45fc2fb",
    "input": "## Claim\nHere is a claim: We observe that predictive performance is not consistent across all domains, with 'Food & Beverage' consistently showing lower performance and 'Other' achieving higher performance when using all the data available from the other domains. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 9: Performance of models trained with tweets from one domain and tested on other domains. All results are reported in ROC AUC. The All line displays results on training on all categories except the category in testing.\n[BOLD] Test | F&B | A | R | Ca | Se | So | T | E | O\n[BOLD] Train | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nFood & Bev. | \u2013 | 58.1 | 52.5 | 66.4 | 59.7 | 58.9 | 54.1 | 61.4 | 53.7\nApparel | 63.9 | \u2013 | 74.4 | 65.1 | 70.8 | 71.2 | 68.5 | 76.9 | 85.6\nRetail | 58.8 | 74.4 | \u2013 | 70.1 | 72.6 | 69.9 | 68.7 | 69.6 | 82.7\nCars | 68.7 | 61.1 | 65.1 | \u2013 | 58.8 | 67. | 59.3 | 62.9 | 68.2\nServices | 65. | 74.2 | 75.8 | 74. | \u2013 | 68.8 | 74.2 | 77.9 | 77.9\nSoftware | 62. | 74.2 | 68. | 67.9 | 72.8 | \u2013 | 72.8 | 72.1 | 80.6\nTransport | 59.3 | 71.7 | 72.4 | 67. | 74.6 | 75. | \u2013 | 72.6 | 81.7\nElectronics | 61.6 | 75.2 | 71. | 68. | 75. | 69.9 | 68.2 | \u2013 | 78.7\nOther | 56.1 | 71.3 | 72.4 | 70.2 | 73.5 | 67.2 | 68.5 | 71. | \u2013\nAll | 70.3 | 77.7 | 79.5 | 82.0 | 79.6 | 80.1 | 76.8 | 81.7 | 88.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "851a3937-519e-4d91-8e18-bb809245164e",
    "input": "## Claim\nHere is a claim: On the WinoCoref dataset, it improves by 15%. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nDataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb\n[ITALIC] Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 | [BOLD] 76.41\n[ITALIC] WinoCoref | AntePre | 68.37 | 74.32 | \u2014\u2013 | 88.48 | 88.95 | [BOLD] 89.32\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "44134077-f8ab-42f9-9e85-66bebe5b1a6e",
    "input": "## Claim\nHere is a claim: These results confirm that simultaneously learning the tasks enhances the performance of a DPP model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | <bold>71.2</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "b8b4f008-0499-4c41-98ce-62faa07f200c",
    "input": "## Claim\nHere is a claim: [CONTINUE] Using a greater BiLSTM hidden size did not help the model, [CONTINUE] We found that using 50-dimensional part-ofspeech embeddings slightly improved results, [CONTINUE] Regarding optimization strategies, we also tried using SGD with different learning rates and a stepwise learning rate schedule as described by Conneau et al. Does the following context support or refute the claim?\n\n## Table\nPaper title: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations\nTable caption: Table 2: Ablation study results.\n[BOLD] Variation | [BOLD] Accuracy (%) | [BOLD] \u0394%\nSubmitted | [BOLD] 69.23 | -\nNo emoji | 68.36 | - 0.87\nNo ELMo | 65.52 | - 3.71\nConcat Pooling | 68.47 | - 0.76\nLSTM hidden=4096 | 69.10 | - 0.13\nLSTM hidden=1024 | 68.93 | - 0.30\nLSTM hidden=512 | 68.43 | - 0.80\nPOS emb dim=100 | 68.99 | - 0.24\nPOS emb dim=75 | 68.61 | - 0.62\nPOS emb dim=50 | 69.33 | + 0.10\nPOS emb dim=25 | 69.21 | - 0.02\nSGD optim lr=1 | 64.33 | - 4.90\nSGD optim lr=0.1 | 66.11 | - 3.12\nSGD optim lr=0.01 | 60.72 | - 8.51\nSGD optim lr=0.001 | 30.49 | - 38.74\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "cef0aae6-12a0-42f5-b9ba-e491d86727e7",
    "input": "## Claim\nHere is a claim: Each extractive summaries of a subset is rated by three annotators who are asked to rank the summaries based on the following criteria: structure, meaning preservation, and relevance, on a 1-5 Likert scale Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "4e3c4acc-419f-40ad-99d3-b60a432f2e43",
    "input": "## Claim\nHere is a claim: Its productivity of 57.5% expresses that it appears in incorrect alternatives 7.5% more often than expected by random chance. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nCue | App. | Prod. | Cov.\nin | 47 | 55.3 | 9.40\nwas | 55 | 61.8 | 11.0\nto | 82 | 40.2 | 16.4\nthe | 85 | 38.8 | 17.0\na | 106 | 57.5 | 21.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "43802ed8-5f82-4f5a-b44a-fa832e72a6c7",
    "input": "## Claim\nHere is a claim: In both cases, the original embeddings perform better than the new ones. Does the following context support or refute the claim?\n\n## Table\nPaper title: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?\nTable caption: Table 6: Results on SimLex-999 and WordSim-353, in Italian and German, before and after debiasing.\n[EMPTY] | Italian Orig | Italian Debias | German Orig | German Debias\nSimLex | 0.280 | [BOLD] 0.288 | 0.343 | [BOLD] 0.356\nWordSim | 0.548 | [BOLD] 0.577 | 0.547 | [BOLD] 0.553\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "de0a2326-e4df-4552-9b76-e80436ea40e1",
    "input": "## Claim\nHere is a claim: We can also observe that the combination of learned reward and coverage penalty in our system further boosts the performance of NeuralTD with learned rewards, relative to using normal ROUGE or the learned reward only Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "9a2c00b9-5f14-4637-8bc2-cac94c5b48ee",
    "input": "## Claim\nHere is a claim: Furthermore, our model generates longer sentences whose lengths are comparable with human arguments, both with about 22 words per sentence. Does the following context support or refute the claim?\n\n## Table\nPaper title: Argument Generation with Retrieval, Planning, and Realization\nTable caption: Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.\n[EMPTY] | [ITALIC] w/ System Retrieval  [BOLD] B-2 | [ITALIC] w/ System Retrieval  [BOLD] B-4 | [ITALIC] w/ System Retrieval  [BOLD] R-2 | [ITALIC] w/ System Retrieval  [BOLD] MTR | [ITALIC] w/ System Retrieval  [BOLD] #Word | [ITALIC] w/ System Retrieval  [BOLD] #Sent | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4 | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent\nHuman | - | - | - | - | 66 | 22 | - | - | - | - | 66 | 22\nRetrieval | 7.55 | 1.11 | 8.64 | 14.38 | 123 | 23 | 10.97 | 3.05 | 23.49 | 20.08 | 140 | 21\n[BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [EMPTY] | [EMPTY]\nSeq2seq | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15 | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15\nSeq2seqAug | 8.26 | 2.24 | 13.79 | 15.75 | 78 | 14 | 10.98 | 4.41 | 22.97 | 19.62 | 71 | 14\n[ITALIC] w/o psg | 7.94 | 2.28 | 10.13 | 15.71 | 75 | 12 | 9.89 | 3.34 | 14.20 | 18.40 | 66 | 12\nH&W\u00a0Hua and Wang ( 2018 ) | 3.64 | 0.92 | 8.83 | 11.78 | 51 | 12 | 8.51 | 2.86 | 18.89 | 17.18 | 58 | 12\n[BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [EMPTY] | [EMPTY]\nCANDELA | 12.02\u2217 | [BOLD] 2.99\u2217 | [BOLD] 14.93\u2217 | [BOLD] 16.92\u2217 | 119 | 22 | 15.80\u2217 | [BOLD] 5.00\u2217 | [BOLD] 23.75 | [BOLD] 20.18 | 116 | 22\n[ITALIC] w/o psg | [BOLD] 12.33\u2217 | 2.86\u2217 | 14.53\u2217 | 16.60\u2217 | 123 | 23 | [BOLD] 16.33\u2217 | 4.98\u2217 | 23.65 | 19.94 | 123 | 23\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ff1ab06e-56c2-42af-bae0-f25b765a45dc",
    "input": "## Claim\nHere is a claim: Some of our bidirectional models obtain 92-93% accuracy. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks\nTable caption: Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.\nUni | POS | 0 87.9 | 1 92.0 | 2 91.7 | 3 91.8 | 4 91.9\nUni | SEM | 81.8 | 87.8 | 87.4 | 87.6 | 88.2\nBi | POS | 87.9 | 93.3 | 92.9 | 93.2 | 92.8\nBi | SEM | 81.9 | 91.3 | 90.8 | 91.9 | 91.9\nRes | POS | 87.9 | 92.5 | 91.9 | 92.0 | 92.4\nRes | SEM | 81.9 | 88.2 | 87.5 | 87.6 | 88.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3307f33e-0d6e-4edb-b335-5897109ca94d",
    "input": "## Claim\nHere is a claim: our model achieved the best results in terms of appropriateness and diversity. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.\nModel | Diversity | App | Good% | OK% | Invalid%\nDAMD | 3.12 | 2.50 | 56.5% | [BOLD] 37.4% | 6.1%\nDAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1% | 9.9%\nHDSA (+) | 2.14 | 2.47 | 57.5% | 32.5% | [BOLD] 10.0%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "1515785d-efa6-40d2-af51-95a8c13ee95c",
    "input": "## Claim\nHere is a claim: Table II shows that Nepal and Macedonia are roughly balanced, while Kerala is imbalanced. Does the following context support or refute the claim?\n\n## Table\nPaper title: Low-supervision urgency detection and transfer in short crisis messages\nTable caption: TABLE II: Details on datasets used for experiments.\nDataset | Unlabeled / Labeled Messages | Urgent / Non-urgent Messages | Unique Tokens | Avg. Tokens / Message | Time Range\nNepal | 6,063/400 | 201/199 | 1,641 | 14 | 04/05/2015-05/06/2015\nMacedonia | 0/205 | 92/113 | 129 | 18 | 09/18/2018-09/21/2018\nKerala | 92,046/400 | 125/275 | 19,393 | 15 | 08/17/2018-08/22/2018\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4cd6be97-c5bd-44fa-a79d-e54753f6893d",
    "input": "## Claim\nHere is a claim: Although LSTM and GRU outperform LRN by 0.3\u223c0.9 in terms of accuracy, these recurrent units sacrifice running efficiency (about 7%\u223c48%) depending on whether LN and BERT are applied. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nModel | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time\nRockt\u00e4schel et\u00a0al. ( 2016 ) | Rockt\u00e4schel et\u00a0al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | -\nThis | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 | [BOLD] 90.49 | 0.696\nThis | GRU | 6.41M | [BOLD] 85.71 | 0.245 | [BOLD] 86.05 | 0.419 | [BOLD] 90.29 | 0.529 | 90.10 | 0.695\nThis | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580\nWork | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555\n[EMPTY] | LRN | 4.25M | 84.88 | [BOLD] 0.209 | 85.06 | [BOLD] 0.223 | 89.98 | [BOLD] 0.488 | 89.93 | [BOLD] 0.506\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "862ee0dc-0f85-4a85-a1c3-b15e962ba324",
    "input": "## Claim\nHere is a claim: [CONTINUE] However, it does not improve significantly over \"ranking\". Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers\nTable caption: Table 1: Impact of linguistic features on deep-coref models on the CoNLL development set.\n[EMPTY] | MUC | <italic>B</italic>3 | CEAF<italic>e</italic> | CoNLL | LEA\nranking | 74.31 | 64.23 | 59.73 | 66.09 | 60.47\n+linguistic | 74.35 | 63.96 | 60.19 | 66.17 | 60.20\ntop-pairs | 73.95 | 63.98 | 59.52 | 65.82 | 60.07\n+linguistic | 74.32 | 64.45 | 60.19 | 66.32 | 60.62\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "fd89ce86-c88f-4273-882a-21c474839874",
    "input": "## Claim\nHere is a claim: RANDOM is the best performing baseline here, and other baselines are far from gender-parity. Does the following context support or refute the claim?\n\n## Table\nPaper title: Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns\nTable caption: Table 7: Performance of our baselines on the development set in the gold-two-mention task (access to the two candidate name spans). Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.\n[EMPTY] | M | F | B | O\nRandom | 47.5 | 50.5 | [ITALIC] 1.06 | 49.0\nToken Distance | 50.6 | 47.5 | [ITALIC] 0.94 | 49.1\nTopical Entity | 50.2 | 47.3 | [ITALIC] 0.94 | 48.8\nSyntactic Distance | 66.7 | 66.7 | [ITALIC]  [BOLD] 1.00 | 66.7\nParallelism | [BOLD] 69.3 | [BOLD] 69.2 | [ITALIC]  [BOLD] 1.00 | [BOLD] 69.2\nParallelism+URL | [BOLD] 74.2 | [BOLD] 71.6 | [ITALIC]  [BOLD] 0.96 | [BOLD] 72.9\nTransformer-Single | 59.6 | 56.6 | [ITALIC] 0.95 | 58.1\nTransformer-Multi | 62.9 | 61.7 | [ITALIC] 0.98 | 62.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "039561ae-0905-4fd8-85e4-f75db58ff616",
    "input": "## Claim\nHere is a claim: Tweets in the black-aligned corpus are classified as containing sexism almost twice as frequently and 1.1 times as frequently classified as containing racism and sexism compared to those in the white-aligned corpus, but the difference is not statistically significant. Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 2: Experiment 1\nDataset | Class | \u02c6 [ITALIC] piblack | \u02c6 [ITALIC] piwhite | [ITALIC] t | [ITALIC] p | \u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite\n[ITALIC] Waseem and Hovy | Racism | 0.001 | 0.003 | -20.818 | *** | 0.505\n[EMPTY] | Sexism | 0.083 | 0.048 | 101.636 | *** | 1.724\n[ITALIC] Waseem | Racism | 0.001 | 0.001 | 0.035 | [EMPTY] | 1.001\n[EMPTY] | Sexism | 0.023 | 0.012 | 64.418 | *** | 1.993\n[EMPTY] | Racism and sexism | 0.002 | 0.001 | 4.047 | *** | 1.120\n[ITALIC] Davidson et al. | Hate | 0.049 | 0.019 | 120.986 | *** | 2.573\n[EMPTY] | Offensive | 0.173 | 0.065 | 243.285 | *** | 2.653\n[ITALIC] Golbeck et al. | Harassment | 0.032 | 0.023 | 39.483 | *** | 1.396\n[ITALIC] Founta et al. | Hate | 0.111 | 0.061 | 122.707 | *** | 1.812\n[EMPTY] | Abusive | 0.178 | 0.080 | 211.319 | *** | 2.239\n[EMPTY] | Spam | 0.028 | 0.015 | 63.131 | *** | 1.854\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "20733675-9d80-4d43-9f7d-92d3d2a434bf",
    "input": "## Claim\nHere is a claim: [CONTINUE] The results of CLUSTER+KCP again indicate that pre-clustering of documents to topics is beneficial, improving upon the KCP performance by 4.6 points, though still performing substantially worse than our joint model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\n[BOLD] Model | R | MUC P | [ITALIC] F1 | R | B3 P | [ITALIC] F1 | R | CEAF- [ITALIC] e P | [ITALIC] F1 | CoNLL  [ITALIC] F1\n[BOLD] Baselines | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nCluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5\nCV Cybulska and Vossen ( 2015a ) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73\nKCP Kenyon-Dean et\u00a0al. ( 2018 ) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69\nCluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6\n[BOLD] Model Variants | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nDisjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5\nJoint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | [BOLD] 79.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2b0d604e-1c16-41b1-9485-1f37db56aebb",
    "input": "## Claim\nHere is a claim: This means that the cleaned dataset is less complex overall, with more references per MR and fewer diverse MRs. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 1: Data statistics comparison for the original E2E data and our cleaned version (number of distinct MRs, total number of textual references, SER as measured by our slot matching script, see Section\u00a03).\n[BOLD] Dataset | [BOLD] Part | [BOLD] MRs | [BOLD] Refs | [BOLD] SER(%)\nOriginal | Train | 4,862 | 42,061 | 17.69\nOriginal | Dev | 547 | 4,672 | 11.42\nOriginal | Test | 630 | 4,693 | 11.49\n[0.5pt/2pt] Cleaned | Train | 8,362 | 33,525 | (0.00)\n[0.5pt/2pt] Cleaned | Dev | 1,132 | 4,299 | (0.00)\n[0.5pt/2pt] Cleaned | Test | 1,358 | 4,693 | (0.00)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1722ed89-381a-467b-a50f-39d73e119b85",
    "input": "## Claim\nHere is a claim: For both datasets, our approach does not substantially outperform the baselines. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5142bc85-da69-4450-bd65-28cd5ce2831e",
    "input": "## Claim\nHere is a claim: [CONTINUE] As expected, in both languages, the difference between the average of the two sets with the debiased embeddings is much lower. Does the following context support or refute the claim?\n\n## Table\nPaper title: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?\nTable caption: Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. \u201cReduction\u201d stands for gap reduction when removing gender signals from the context.\n[EMPTY] | Italian Original | Italian Debiased | Italian English | Italian Reduction | German Original | German Debiased | German English | German Reduction\nSame Gender | 0.442 | 0.434 | 0.424 | \u2013 | 0.491 | 0.478 | 0.446 | \u2013\nDifferent Gender | 0.385 | 0.421 | 0.415 | \u2013 | 0.415 | 0.435 | 0.403 | \u2013\ndifference | 0.057 | 0.013 | 0.009 | [BOLD] 91.67% | 0.076 | 0.043 | 0.043 | [BOLD] 100%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "78540156-ca72-40ea-bf46-e0aae6172d16",
    "input": "## Claim\nHere is a claim: As can be seen in the results presented in Table 3 the models using TVMAX in the output attention layer outperform the models using softmax and sparsemax. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.\n[EMPTY] | Att. to image | Att. to bounding boxes | Test-Dev Yes/No | Test-Dev Number | Test-Dev Other | Test-Dev Overall | Test-Standard Yes/No | Test-Standard Number | Test-Standard Other | Test-Standard Overall\nsoftmax | \u2713 | [EMPTY] | 83.08 | 42.65 | 55.74 | 65.52 | 83.55 | 42.68 | 56.01 | 65.97\nsparsemax | \u2713 | [EMPTY] | 83.08 | 43.19 | 55.79 | 65.60 | 83.33 | 42.99 | 56.06 | 65.94\nsoft-TVmax | \u2713 | [EMPTY] | 83.13 | 43.53 | 56.01 | 65.76 | 83.63 | 43.24 | 56.10 | 66.11\nsparse-TVmax | \u2713 | [EMPTY] | 83.10 | 43.30 | 56.14 | 65.79 | 83.66 | 43.18 | 56.21 | 66.17\nsoftmax | [EMPTY] | \u2713 | 85.14 | 49.59 | 58.72 | 68.57 | 85.56 | 49.54 | 59.11 | 69.04\nsparsemax | [EMPTY] | \u2713 | [BOLD] 85.40 | [BOLD] 50.87 | 58.67 | 68.79 | [BOLD] 85.80 | 50.18 | 59.08 | 69.19\nsoftmax | \u2713 | \u2713 | 85.33 | 50.49 | 58.88 | 68.82 | 85.58 | 50.42 | 59.18 | 69.17\nsparse-TVmax | \u2713 | \u2713 | 85.35 | 50.52 | [BOLD] 59.15 | [BOLD] 68.96 | 85.72 | [BOLD] 50.66 | [BOLD] 59.22 | [BOLD] 69.28\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a22077ab-b54d-465a-ab9a-67f73866caa7",
    "input": "## Claim\nHere is a claim: \u201cCoverage\u201d represents how much text a system extracts for a document (higher is better); \u201cOverlap\u201d represents the percentage of words that are in the extractive summarization (higher is better) \u201cAvg. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "9064b2de-b304-408b-9aa9-81c8f1be3e65",
    "input": "## Claim\nHere is a claim: For Marian amun, the effect of adding domain labels is significant as we can see in Table 3. Does the following context support or refute the claim?\n\n## Table\nPaper title: The MeMAD Submission to the WMT18 Multimodal Translation Task\nTable caption: Table 3: Using automatically translated image captions and domain labels (BLEU% scores). A is short for Amun, T for Transformer.\n[EMPTY] | en-fr | flickr16 | flickr17 | mscoco17\nA | subs1M [ITALIC]  [ITALIC] H+MS-COCO | 66.3 | 60.5 | 52.1\nA | +domain-tuned | 66.8 | 60.6 | 52.0\nA | +labels | [BOLD] 67.2 | 60.4 | 51.7\nT | subs1M [ITALIC]  [ITALIC] LM+MS-COCO | 66.9 | 60.3 | [BOLD] 52.8\nT | +labels | [BOLD] 67.2 | [BOLD] 60.9 | 52.7\n[EMPTY] | en-de | flickr16 | flickr17 | mscoco17\nA | subs1M [ITALIC]  [ITALIC] H+MS-COCO | 43.1 | 39.0 | 35.1\nA | +domain-tuned | 43.9 | 39.4 | 35.8\nA | +labels | 43.2 | 39.3 | 34.3\nT | subs1M [ITALIC]  [ITALIC] LM+MS-COCO | [BOLD] 44.4 | 39.4 | 35.0\nT | +labels | 44.1 | [BOLD] 39.8 | [BOLD] 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d8b4c1e7-8cf5-4916-87af-b646b8ef4b6b",
    "input": "## Claim\nHere is a claim: Tweets containing emoji seem to be harder for the model to classify than those without. Does the following context support or refute the claim?\n\n## Table\nPaper title: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations\nTable caption: Table 4: Number of tweets on the test set with and without emoji and hashtags. The number between parentheses is the proportion of tweets classified correctly.\n[EMPTY] | [BOLD] Present | [BOLD] Not Present\nEmoji | 4805 (76.6%) | 23952 (68.0%)\nHashtags | 2122 (70.5%) | 26635 (69.4%)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "55e35248-14b5-4372-91ab-184449934829",
    "input": "## Claim\nHere is a claim: We observe that the results for the UD representation are quite a bit lower than the two others. Does the following context support or refute the claim?\n\n## Table\nPaper title: Syntactic Dependency Representations in Neural Relation Classification\nTable caption: Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.\n[BOLD] Representation | [BOLD] Hyper parameters Filter size | [BOLD] Hyper parameters Num. Feature maps | [BOLD] Hyper parameters Activation func. | [BOLD] Hyper parameters L2 Reg. | [BOLD] Hyper parameters Learning rate | [BOLD] Hyper parameters Dropout Prob. | [BOLD] F1.(avg. in 5-fold) with default values | [BOLD] F1.(avg. in 5-fold) with optimal values\nCoNLL08 | 4-5 | 1000 | Softplus | 1.15e+01 | 1.13e-03 | 1 | 73.34 | 74.49\nSB | 4-5 | 806 | Sigmoid | 8.13e-02 | 1.79e-03 | 0.87 | 72.83 | [BOLD] 75.05\nUD v1.3 | 5 | 716 | Softplus | 1.66e+00 | 9.63E-04 | 1 | 68.93 | 69.57\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "adadfacf-0744-4462-951f-d4678d921ee0",
    "input": "## Claim\nHere is a claim: [CONTINUE] EWC models perform well over multiple domains, so the improvement over uniform ensembling is less striking than for unadapted models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 6: Test BLEU for 2-model es-en and 3-model en-de model ensembling for models adapted with EWC, compared to oracle model last trained on each domain, chosen if test domain is known. BI+IS outperforms uniform ensembling and in some cases outperforms the oracle.\n[BOLD] Decoder configuration | [BOLD] es-en  [BOLD] Health | [BOLD] es-en  [BOLD] Bio | [BOLD] en-de  [BOLD] News | [BOLD] en-de  [BOLD] TED | [BOLD] en-de  [BOLD] IT\nOracle model | 35.9 | 37.8 | 37.8 | 27.0 | 57.0\nUniform | 36.0 | 36.4 | [BOLD] 38.9 | 26.0 | 43.5\nBI + IS | [BOLD] 36.2 | [BOLD] 38.0 | 38.7 | [BOLD] 26.1 | [BOLD] 56.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f31c781e-b429-446a-9493-6900c16e04ef",
    "input": "## Claim\nHere is a claim: In future work, we are also looking into a systematic way of identifying the markers as well as introducing negation for the LSTM which may be able to capture the negation aspects better. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 8: Sentiment classification evaluation, using different classifiers on the test set.\nClassifier | Positive Sentiment Precision | Positive Sentiment Recall | Positive Sentiment Fscore\nSVM-w/o neg. | 0.57 | 0.72 | 0.64\nSVM-Punct. neg. | 0.58 | 0.70 | 0.63\nSVM-our-neg. | 0.58 | 0.73 | 0.65\nCNN | 0.63 | 0.83 | 0.72\nCNN-LSTM | 0.71 | 0.72 | 0.72\nCNN-LSTM-Our-neg-Ant | [BOLD] 0.78 | [BOLD] 0.77 | [BOLD] 0.78\n[EMPTY] | Negative Sentiment | Negative Sentiment | Negative Sentiment\n[EMPTY] | Precision | Recall | Fscore\nSVM-w/o neg. | 0.78 | 0.86 | 0.82\nSVM-Punct. neg. | 0.78 | 0.87 | 0.83\nSVM-Our neg. | 0.80 | 0.87 | 0.83\nCNN | 0.88 | 0.72 | 0.79\nCNN-LSTM. | 0.83 | 0.83 | 0.83\nCNN-LSTM-our-neg-Ant | [BOLD] 0.87 | [BOLD] 0.87 | [BOLD] 0.87\n[EMPTY] | Train | [EMPTY] | Test\nPositive tweets | 5121 | [EMPTY] | 1320\nNegative tweets | 9094 | [EMPTY] | 2244\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "2b3c80a8-30f1-48d4-9751-a7353290f19e",
    "input": "## Claim\nHere is a claim: So, the score of analogy 2 will be 0. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "711764b0-0eb4-4498-8325-afce0b6667b5",
    "input": "## Claim\nHere is a claim: WN-N shows high coverage containing many high-frequency members. Does the following context support or refute the claim?\n\n## Table\nPaper title: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources\nTable caption: Table 4: Lexicon member coverage (%)\ntarget | VN | WN-V | WN-N\ntype | 81 | 66 | 47\nx+POS | 54 | 39 | 43\nlemma | 88 | 76 | 53\nx+POS | 79 | 63 | 50\nshared | 54 | 39 | 41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "84408bed-7687-4049-9b6b-35bed42eda8f",
    "input": "## Claim\nHere is a claim: In both cases the classifiers trained upon their data are still more likely to flag white-aligned tweets as sexism. Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 4: Experiment 2, t= \u201cb*tch\u201d\nDataset | Class | \u02c6 [ITALIC] piblack | \u02c6 [ITALIC] piwhite | [ITALIC] t | [ITALIC] p | \u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite\n[ITALIC] Waseem and Hovy | Racism | 0.010 | 0.010 | -0.632 | [EMPTY] | 0.978\n[EMPTY] | Sexism | 0.963 | 0.944 | 20.064 | *** | 1.020\n[ITALIC] Waseem | Racism | 0.011 | 0.011 | -1.254 | [EMPTY] | 0.955\n[EMPTY] | Sexism | 0.349 | 0.290 | 28.803 | *** | 1.203\n[EMPTY] | Racism and sexism | 0.012 | 0.012 | -0.162 | [EMPTY] | 0.995\n[ITALIC] Davidson et al. | Hate | 0.017 | 0.015 | 4.698 | *** | 1.152\n[EMPTY] | Offensive | 0.988 | 0.991 | -6.289 | *** | 0.997\n[ITALIC] Golbeck et al. | Harassment | 0.099 | 0.091 | 6.273 | *** | 1.091\n[ITALIC] Founta et al. | Hate | 0.074 | 0.027 | 46.054 | *** | 2.728\n[EMPTY] | Abusive | 0.925 | 0.968 | -41.396 | *** | 0.956\n[EMPTY] | Spam | 0.010 | 0.010 | 0.000 | [EMPTY] | 1.000\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3a6e389b-f580-4a8a-a2be-ac2fe511e577",
    "input": "## Claim\nHere is a claim: However, the model using TVMAX in the final attention layer does not necessarily achieve the highest accuracy, showing that features obtained using the TVMAX transformation are not necessarily a better complement to bounding box features. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.\n[EMPTY] | Att. to image | Att. to bounding boxes | Test-Dev Yes/No | Test-Dev Number | Test-Dev Other | Test-Dev Overall | Test-Standard Yes/No | Test-Standard Number | Test-Standard Other | Test-Standard Overall\nsoftmax | \u2713 | [EMPTY] | 83.08 | 42.65 | 55.74 | 65.52 | 83.55 | 42.68 | 56.01 | 65.97\nsparsemax | \u2713 | [EMPTY] | 83.08 | 43.19 | 55.79 | 65.60 | 83.33 | 42.99 | 56.06 | 65.94\nsoft-TVmax | \u2713 | [EMPTY] | 83.13 | 43.53 | 56.01 | 65.76 | 83.63 | 43.24 | 56.10 | 66.11\nsparse-TVmax | \u2713 | [EMPTY] | 83.10 | 43.30 | 56.14 | 65.79 | 83.66 | 43.18 | 56.21 | 66.17\nsoftmax | [EMPTY] | \u2713 | 85.14 | 49.59 | 58.72 | 68.57 | 85.56 | 49.54 | 59.11 | 69.04\nsparsemax | [EMPTY] | \u2713 | [BOLD] 85.40 | [BOLD] 50.87 | 58.67 | 68.79 | [BOLD] 85.80 | 50.18 | 59.08 | 69.19\nsoftmax | \u2713 | \u2713 | 85.33 | 50.49 | 58.88 | 68.82 | 85.58 | 50.42 | 59.18 | 69.17\nsparse-TVmax | \u2713 | \u2713 | 85.35 | 50.52 | [BOLD] 59.15 | [BOLD] 68.96 | 85.72 | [BOLD] 50.66 | [BOLD] 59.22 | [BOLD] 69.28\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "bca20741-c631-4e3e-9086-cb068cfcb160",
    "input": "## Claim\nHere is a claim: The key advantage of this method is that one does not need a large human-annotated corpus for RL training but can use a simulated corpus for supervised and RL training Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ac292ba0-cc9c-4235-8e92-4901c60e2903",
    "input": "## Claim\nHere is a claim: It closely matches the performance of ORACLE with only 0.40% absolute difference. Does the following context support or refute the claim?\n\n## Table\nPaper title: Deriving Machine Attention from Human Rationales\nTable caption: Table 3: Accuracy of transferring between aspects. Models with \u2020 use labeled data from source aspects. Models with \u2021 use human rationales on the target aspect.\nSource | Target | Svm | Ra-Svm\u2021 | Ra-Cnn\u2021 | Trans\u2020 | Ra-Trans\u2021\u2020 | Ours\u2021\u2020 | Oracle\u2020\nBeer aroma+palate | Beer look | 74.41 | 74.83 | 74.94 | 72.75 | 76.41 | [BOLD] 79.53 | 80.29\nBeer look+palate | Beer aroma | 68.57 | 69.23 | 67.55 | 69.92 | 76.45 | [BOLD] 77.94 | 78.11\nBeer look+aroma | Beer palate | 63.88 | 67.82 | 65.72 | 74.66 | 73.40 | [BOLD] 75.24 | 75.50\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "6e133404-6df1-467e-b5af-f0a895d24779",
    "input": "## Claim\nHere is a claim: In Table 2, we can see a noticeable margin brought by our capsule-based approach over the strong baselines on EUR-Lex, and competitive results on RCV1. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Scalable and Reliable Capsule Networksfor Challenging NLP Applications\nTable caption: Table 2: Comparisons of our NLP-Cap approach and baselines on two text classification benchmarks, where \u2019-\u2019 denotes methods that failed to scale due to memory issues.\n<bold>Datasets</bold> | <bold>Metrics</bold> | <bold>FastXML</bold> | <bold>PD-Sparse</bold> | <bold>FastText</bold> | <bold>Bow-CNN</bold> | <bold>CNN-Kim</bold> | <bold>XML-CNN</bold> | <bold>Cap-Zhao</bold> | <bold>NLP-Cap</bold> | <bold>Impv</bold>\nRCV1 | PREC@1 | 94.62 | 95.16 | 95.40 | 96.40 | 93.54 | 96.86 | 96.63 | <bold>97.05</bold> | +0.20%\nRCV1 | PREC@3 | 78.40 | 79.46 | 79.96 | 81.17 | 76.15 | 81.11 | 81.02 | <bold>81.27</bold> | +0.20%\nRCV1 | PREC@5 | 54.82 | 55.61 | 55.64 | <bold>56.74</bold> | 52.94 | 56.07 | 56.12 | 56.33 | -0.72%\n[EMPTY] | NDCG@1 | 94.62 | 95.16 | 95.40 | 96.40 | 93.54 | 96.88 | 96.63 | <bold>97.05</bold> | +0.20%\n[EMPTY] | NDCG@3 | 89.21 | 90.29 | 90.95 | 92.04 | 87.26 | 92.22 | 92.31 | <bold>92.47</bold> | +0.17%\n[EMPTY] | NDCG@5 | 90.27 | 91.29 | 91.68 | 92.89 | 88.20 | 92.63 | 92.75 | <bold>93.11</bold> | +0.52%\nEUR-Lex | PREC@1 | 68.12 | 72.10 | 71.51 | 64.99 | 68.35 | 75.65 | - | <bold>80.20</bold> | +6.01%\nEUR-Lex | PREC@3 | 57.93 | 57.74 | 60.37 | 51.68 | 54.45 | 61.81 | - | <bold>65.48</bold> | +5.93%\nEUR-Lex | PREC@5 | 48.97 | 47.48 | 50.41 | 42.32 | 44.07 | 50.90 | - | <bold>52.83</bold> | +3.79%\n[EMPTY] | NDCG@1 | 68.12 | 72.10 | 71.51 | 64.99 | 68.35 | 75.65 | - | <bold>80.20</bold> | +6.01%\n[EMPTY] | NDCG@3 | 60.66 | 61.33 | 63.32 | 55.03 | 59.81 | 66.71 | - | <bold>71.11</bold> | +6.59%\n[EMPTY] | NDCG@5 | 56.42 | 55.93 | 58.56 | 49.92 | 57.99 | 64.45 | - | <bold>68.80</bold> | +6.75%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f4879cd6-63b6-4f55-9bca-036a3a0a0b90",
    "input": "## Claim\nHere is a claim: Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain low degree nodes. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "b5999ce8-28d9-47e9-a847-e7d72dcfde52",
    "input": "## Claim\nHere is a claim: Still, both LRN and oLRN translate sentences faster than SRU (+15%/+6%). Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.\nModel | #Params | BLEU | Train | Decode\nGNMT | - | 24.61 | - | -\nGRU | 206M | 26.28 | 2.67 | 45.35\nATR | 122M | 25.70 | 1.33 | [BOLD] 34.40\nSRU | 170M | 25.91 | 1.34 | 42.84\nLRN | 143M | 26.26 | [BOLD] 0.99 | 36.50\noLRN | 164M | [BOLD] 26.73 | 1.15 | 40.19\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c1eb6469-562e-4b25-9c8e-0d11fb645a96",
    "input": "## Claim\nHere is a claim: Compared to CMOW, the hybrid model shows rather small differences. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d69416ad-80be-4be2-bc76-6806f2a74b90",
    "input": "## Claim\nHere is a claim: MLP with BERT as encoder does not have the best overall performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "dbbed763-8147-4853-9749-d8a28167584b",
    "input": "## Claim\nHere is a claim: MIL-ND does not significantly outperform MIL: the 95% confidence intervals for them overlap. Does the following context support or refute the claim?\n\n## Table\nPaper title: Distant Learning for Entity Linking with Automatic Noise Detection\nTable caption: Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.\nSystem | All P | All R | All F1 | In  [ITALIC] E+ P | In  [ITALIC] E+ R | In  [ITALIC] E+ F1\nName matching | 15.03 | 15.03 | 15.03 | 29.13 | 29.13 | 29.13\nMIL (model 1) | 35.87 | 35.87 | 35.87 \u00b10.72 | 69.38 | 69.38 | 69.38 \u00b11.29\nMIL-ND (model 2) | 37.42 | [BOLD] 37.42 | 37.42 \u00b10.35 | 72.50 | [BOLD] 72.50 | [BOLD] 72.50 \u00b10.68\n[ITALIC] \u03c4MIL-ND (model 2) | [BOLD] 38.91 | 36.73 | [BOLD] 37.78 \u00b10.26 | [BOLD] 73.19 | 71.15 | 72.16 \u00b10.48\nSupervised learning | 42.90 | 42.90 | 42.90 \u00b10.59 | 83.12 | 83.12 | 83.12 \u00b11.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "76927b0e-70f9-4c0a-b09f-c2af489a85dd",
    "input": "## Claim\nHere is a claim: in general, 5.2% of tokens are negation cues, 26.1% of tokens are negated, and 11.2% of tokens are negated but are not cues. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 3: Cue and token distribution in the conversational negation corpus.\nTotal negation cues | 2921\nTrue negation cues | 2674\nFalse negation cues | 247\nAverage scope length | 2.9\nAverage sentence length | 13.6\nAverage tweet length | 22.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "75819ab9-8d94-432d-bb30-590df24c67b7",
    "input": "## Claim\nHere is a claim: However, when gold PP attachment are used, we note only a small improvement of 10.46 points in PP attachment accuracies (between the PPA accuracy for RBG and RBG + Oracle PP), which suggests that adding PP predictions as features is not an effective approach. Does the following context support or refute the claim?\n\n## Table\nPaper title: Ontology-Aware Token Embeddings for Prepositional Phrase Attachment\nTable caption: Table 2: Results from RBG dependency parser with features coming from various PP attachment predictors and oracle attachments.\n[BOLD] System | [BOLD] Full UAS | [BOLD] PPA Acc.\nRBG | 94.17 | 88.51\nRBG + HPCD (full) | 94.19 | 89.59\nRBG + LSTM-PP | 94.14 | 86.35\nRBG + OntoLSTM-PP | 94.30 | 90.11\nRBG + Oracle PP | 94.60 | 98.97\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a86eec51-fb50-4409-9bde-27aa6dd58d44",
    "input": "## Claim\nHere is a claim: BERT achieved a final accuracy of 87.47%, lower than ULMFiT's full performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Localization of Fake News Detection via Multitask Transfer Learning\nTable caption: Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.\nModel | Val. Accuracy | Loss | Val. Loss | Pretraining Time | Finetuning Time\nSiamese Networks | 77.42% | 0.5601 | 0.5329 | [EMPTY] | 4m per epoch\nBERT | 87.47% | 0.4655 | 0.4419 | 66 hours | 2m per epoch\nGPT-2 | 90.99% | 0.2172 | 0.1826 | 78 hours | 4m per epoch\nULMFiT | 91.59% | 0.3750 | 0.1972 | 11 hours | 2m per epoch\nULMFiT (no LM Finetuning) | 78.11% | 0.5512 | 0.5409 | 11 hours | 2m per epoch\nBERT + Multitasking | 91.20% | 0.3155 | 0.3023 | 66 hours | 4m per epoch\nGPT-2 + Multitasking | 96.28% | 0.2609 | 0.2197 | 78 hours | 5m per epoch\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9b0e1193-f48e-4334-b899-f5e92f4df3da",
    "input": "## Claim\nHere is a claim: The results reported in Table 7 show that precision on BDI indeed increases as a result of the reduced effect of grammatical gender on the embeddings for German and Italian, i.e. Does the following context support or refute the claim?\n\n## Table\nPaper title: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?\nTable caption: Table 7: Cross-lingual embedding alignment in Italian and in German, before and after debiasing.\n[EMPTY] | Italian \u2192 En | Italian En \u2192 | German \u2192 En | German En \u2192\nOrig | 58.73 | 59.68 | 47.58 | 50.48\nDebias | [BOLD] 60.03 | [BOLD] 60.96 | [BOLD] 47.89 | [BOLD] 51.76\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "9ba61c9d-3bb7-4573-b8ca-8e1240271ace",
    "input": "## Claim\nHere is a claim: All metrics have good correlations and become more informative when BERT embeddings are used Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nMetric | [ITALIC] \u03c1 | [ITALIC] r | G-Pre | G-Rec\nROUGE-1 | .290 | .304 | .392 | .428\nROUGE-2 | .259 | .278 | .408 | .444\nROUGE-L | .274 | .297 | .390 | .426\nROUGE-SU4 | .282 | .279 | .404 | .440\nBLEU-1 | .256 | .281 | .409 | .448\nBLEU-2 | .301 | .312 | .411 | .446\nBLEU-3 | .317 | .312 | .409 | .444\nBLEU-4 | .311 | .307 | .409 | .446\nBLEU-5 | .308 | .303 | .420 | .459\nMETEOR | .305 | .285 | .409 | .444\nInferSent-Cosine | [BOLD] .329 | [BOLD] .339 | .417 | .460\nBERT-Cosine | .312 | .335 | [BOLD] .440 | [BOLD] .484\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6a0f3f3f-8f25-43cb-938d-3808d00199ac",
    "input": "## Claim\nHere is a claim: Replacing the attention normalizing function with softmax operation also reduces the F1 score marginally (A3\u2212A5). Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.\n[EMPTY] | Prec. | Rec. | F1\n(A1) BiLSTM-CNN | 0.473 | 0.606 | 0.531\n(A2) Standard attention | 0.466 | 0.638 | 0.539\n(A3) Window size ( [ITALIC] ws)=5 | 0.507 | 0.652 | [BOLD] 0.571\n(A4) Window size ( [ITALIC] ws)=10 | 0.510 | 0.640 | 0.568\n(A5) Softmax | 0.490 | 0.658 | 0.562\n(A6) Max-pool | 0.492 | 0.600 | 0.541\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "6f84236f-e476-4ea2-9bba-f83a1b157df1",
    "input": "## Claim\nHere is a claim: [CONTINUE] A distinctive part-of-speech pattern common in complaints is possessive pronouns followed by nouns (PRP$ NN) which refer to items of services possessed by the complainer (e.g., my account, my order). Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\n[BOLD] Complaints  [BOLD] Feature | [BOLD] Complaints  [ITALIC] r | [BOLD] Not Complaints  [BOLD] Feature | [BOLD] Not Complaints  [ITALIC] r\n[BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams\nnot | .154 | [URL] | .150\nmy | .131 | ! | .082\nworking | .124 | he | .069\nstill | .123 | thank | .067\non | .119 | , | .064\ncan\u2019t | .113 | love | .064\nservice | .112 | lol | .061\ncustomer | .109 | you | .060\nwhy | .108 | great | .058\nwebsite | .107 | win | .058\nno | .104 | \u2019 | .058\n? | .098 | she | .054\nfix | .093 | : | .053\nwon\u2019t | .092 | that | .053\nbeen | .090 | more | .052\nissue | .089 | it | .052\ndays | .088 | would | .051\nerror | .087 | him | .047\nis | .084 | life | .046\ncharged | .083 | good | .046\n[BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)\nVBN | .141 | UH | .104\n$ | .118 | NNP | .098\nVBZ | .114 | PRP | .076\nNN_VBZ | .114 | HT | .076\nPRP$ | .107 | PRP_. | .076\nPRP$_NN | .105 | PRP_RB | .067\nVBG | .093 | NNP_NNP | .062\nCD | .092 | VBP_PRP | .054\nWRB_VBZ | .084 | JJ | .053\nVBZ_VBN | .084 | DT_JJ | .051\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "e58cd347-4775-4698-9703-16d155e90bc7",
    "input": "## Claim\nHere is a claim: The HAN models do not outperform MEAD in terms of sentence prediction. Does the following context support or refute the claim?\n\n## Table\nPaper title: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks\nTable caption: Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\n[BOLD] System | [BOLD] ROUGE-1  [BOLD] R (%) | [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%) | [BOLD] ROUGE-2  [BOLD] R (%) | [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%) | [BOLD] Sentence-Level  [BOLD] R (%) | [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)\n[BOLD] ILP | 24.5 | 41.1 | 29.3\u00b10.5 | 7.9 | 15.0 | 9.9\u00b10.5 | 13.6 | 22.6 | 15.6\u00b10.4\n[BOLD] Sum-Basic | 28.4 | 44.4 | 33.1\u00b10.5 | 8.5 | 15.6 | 10.4\u00b10.4 | 14.7 | 22.9 | 16.7\u00b10.5\n[BOLD] KL-Sum | 39.5 | 34.6 | 35.5\u00b10.5 | 13.0 | 12.7 | 12.3\u00b10.5 | 15.2 | 21.1 | 16.3\u00b10.5\n[BOLD] LexRank | 42.1 | 39.5 | 38.7\u00b10.5 | 14.7 | 15.3 | 14.2\u00b10.5 | 14.3 | 21.5 | 16.0\u00b10.5\n[BOLD] MEAD | 45.5 | 36.5 | 38.5\u00b1 0.5 | 17.9 | 14.9 | 15.4\u00b10.5 | 27.8 | 29.2 | 26.8\u00b10.5\n[BOLD] SVM | 19.0 | 48.8 | 24.7\u00b10.8 | 7.5 | 21.1 | 10.0\u00b10.5 | 32.7 | 34.3 | 31.4\u00b10.4\n[BOLD] LogReg | 26.9 | 34.5 | 28.7\u00b10.6 | 6.4 | 9.9 | 7.3\u00b10.4 | 12.2 | 14.9 | 12.7\u00b10.5\n[BOLD] LogReg [ITALIC] r | 28.0 | 34.8 | 29.4\u00b10.6 | 6.9 | 10.4 | 7.8\u00b10.4 | 12.1 | 14.5 | 12.5\u00b10.5\n[BOLD] HAN | 31.0 | 42.8 | 33.7\u00b10.7 | 11.2 | 17.8 | 12.7\u00b10.5 | 26.9 | 34.1 | 32.4\u00b10.5\n[BOLD] HAN+pretrainT | 32.2 | 42.4 | 34.4\u00b10.7 | 11.5 | 17.5 | 12.9\u00b10.5 | 29.6 | 35.8 | 32.2\u00b10.5\n[BOLD] HAN+pretrainU | 32.1 | 42.1 | 33.8\u00b10.7 | 11.6 | 17.6 | 12.9\u00b10.5 | 30.1 | 35.6 | 32.3\u00b10.5\n[BOLD] HAN [ITALIC] r | 38.1 | 40.5 | [BOLD] 37.8\u00b10.5 | 14.0 | 17.1 | [BOLD] 14.7\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainT [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.5 | 16.8 | [BOLD] 14.4\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainU [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.6 | 16.9 | [BOLD] 14.4\u00b10.5 | 33.9 | 33.8 | [BOLD] 33.8\u00b10.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "718a9e4f-712c-4c8b-bb79-52a817600a8e",
    "input": "## Claim\nHere is a claim: Interestingly, G2S-GIN has better performance among our models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "f5a1f7ce-a335-4908-a0e1-baea0d34c863",
    "input": "## Claim\nHere is a claim: [CONTINUE] For LOC, it turns out that candidate selection is not a bottleneck: when candidate selection was flawless, the models made only about 55% errors, down from about 96%. Does the following context support or refute the claim?\n\n## Table\nPaper title: Distant Learning for Entity Linking with Automatic Noise Detection\nTable caption: Table 3: % errors on the development set for different named entity types under two settings. (Smaller is better.)\nSystem | All LOC | All ORG | All PER | All MISC | In  [ITALIC] E+ LOC | In  [ITALIC] E+ ORG | In  [ITALIC] E+ PER | In  [ITALIC] E+ MISC\nName matching | 96.26 | 89.48 | 57.38 | 96.60 | 92.32 | 76.87 | 47.40 | 76.29\nMIL | 57.09 | [BOLD] 76.30 | 41.35 | 93.35 | 11.90 | [BOLD] 47.90 | 27.60 | 53.61\nMIL-ND | 57.15 | 77.15 | 35.95 | 92.47 | 12.02 | 49.77 | 20.94 | 47.42\n[ITALIC] \u03c4MIL-ND | [BOLD] 55.15 | 76.56 | [BOLD] 34.03 | [BOLD] 92.15 | [BOLD] 11.14 | 51.18 | [BOLD] 20.59 | [BOLD] 40.00\nSupervised learning | 55.58 | 61.32 | 24.98 | 89.96 | 8.80 | 14.95 | 7.40 | 29.90\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ebd1548f-ff05-41b1-917c-9a5e04da6635",
    "input": "## Claim\nHere is a claim: TF has the best values of recall and f-measure for all corpora but the English version of TED Talks which has in DF the best value of recall and in DocSub the best value of f-measure. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5ecc4b82-ccbe-480e-af2d-c5e567617179",
    "input": "## Claim\nHere is a claim: Table 4 shows the BLEU scores of our Dual2seq model taking gold or automatic AMRs as inputs. Does the following context support or refute the claim?\n\n## Table\nPaper title: Semantic Neural Machine Translation using AMR\nTable caption: Table 4: BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.\nAMR Anno. | BLEU\nAutomatic | 16.8\nGold | [BOLD] *17.5*\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "8dac475d-fd22-4945-a778-12d64244ffb8",
    "input": "## Claim\nHere is a claim: Our model does not improve the precision scores on both datasets with good recall scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. \u2020 denotes the previous best state-of-the-art model.\nModel | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1\nCNN zeng2014relation | 0.413 | 0.591 | 0.486 | 0.444 | 0.625 | 0.519\nPCNN zeng2015distant | 0.380 | [BOLD] 0.642 | 0.477 | 0.446 | 0.679 | 0.538\u2020\nEA huang2016attention | 0.443 | 0.638 | 0.523\u2020 | 0.419 | 0.677 | 0.517\nBGWA jat2018attention | 0.364 | 0.632 | 0.462 | 0.417 | [BOLD] 0.692 | 0.521\nBiLSTM-CNN | 0.490 | 0.507 | 0.498 | 0.473 | 0.606 | 0.531\nOur model | [BOLD] 0.541 | 0.595 | [BOLD] 0.566* | [BOLD] 0.507 | 0.652 | [BOLD] 0.571*\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4c9a4b70-f8f6-4aac-b271-616dbbee6ad4",
    "input": "## Claim\nHere is a claim: The topical features such as the LIWC dictionaries (which combine syntactic and semantic information) and Word2Vec topics perform in the same range as the part of speech tags. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.\n[BOLD] Model | [BOLD] Acc | [BOLD] F1 | [BOLD] AUC\nMost Frequent Class | 64.2 | 39.1 | 0.500\nLogistic Regression | [EMPTY] | [EMPTY] | [EMPTY]\nSentiment \u2013 MPQA | 64.2 | 39.1 | 0.499\nSentiment \u2013 NRC | 63.9 | 42.2 | 0.599\nSentiment \u2013 V&B | 68.9 | 60.0 | 0.696\nSentiment \u2013 VADER | 66.0 | 54.2 | 0.654\nSentiment \u2013 Stanford | 68.0 | 55.6 | 0.696\nComplaint Specific (all) | 65.7 | 55.2 | 0.634\nRequest | 64.2 | 39.1 | 0.583\nIntensifiers | 64.5 | 47.3 | 0.639\nDowngraders | 65.4 | 49.8 | 0.615\nTemporal References | 64.2 | 43.7 | 0.535\nPronoun Types | 64.1 | 39.1 | 0.545\nPOS Bigrams | 72.2 | 66.8 | 0.756\nLIWC | 71.6 | 65.8 | 0.784\nWord2Vec Clusters | 67.7 | 58.3 | 0.738\nBag-of-Words | 79.8 | 77.5 | 0.866\nAll Features | [BOLD] 80.5 | [BOLD] 78.0 | [BOLD] 0.873\nNeural Networks | [EMPTY] | [EMPTY] | [EMPTY]\nMLP | 78.3 | 76.2 | 0.845\nLSTM | 80.2 | 77.0 | 0.864\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ed36c8eb-dda1-48b8-8fa3-7a9456fdfd05",
    "input": "## Claim\nHere is a claim: [CONTINUE] However, the highest accuracy was achieved by using Binary Cross Entropy, with a score of 55.20. Does the following context support or refute the claim?\n\n## Table\nPaper title: Zero-Shot Grounding of Objects from Natural Language Queries\nTable caption: Table 6: Ablation study: BM=Base Model, softmax means we classify only one candidate box as foreground, BCE = Binary Cross Entropy means we classify each candidate box as the foreground or background, FL = Focal Loss, Img-Resize: use images of dimension 600\u00d7600\nModel | Accuracy on RefClef\nBM + Softmax | 48.54\nBM + BCE | 55.20\nBM + FL | 57.13\nBM + FL + Img-Resize | [BOLD] 61.75\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "50fff713-1425-4f87-9a4c-dbcde001032f",
    "input": "## Claim\nHere is a claim: despite their sensitivity to these semantic clues, BERT models trained with their own distributions alone make better decisions when we combine their outputs. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 7: Sensitivity of BERT-large to superficial cues identified in \u00a72 (unit: 10\u22122). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.\nCue | [ITALIC] SCOPA | [ITALIC] SB_COPA | Diff. | Prod.\nwoman | 7.98 | 4.84 | -3.14 | 0.25\nmother | 5.16 | 3.95 | -1.21 | 0.75\nwent | 6.00 | 5.15 | -0.85 | 0.73\ndown | 5.52 | 4.93 | -0.58 | 0.71\ninto | 4.07 | 3.51 | -0.56 | 0.40\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "a56519fa-2e7c-416d-aed9-07c1bd180793",
    "input": "## Claim\nHere is a claim: for example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9). Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See \u00a72 for model details. * indicates our replication experiments.\nModel | Accuracy\nBigramPMI\u00a0Goodwin et al. ( 2012 ) | 63.4\nPMI\u00a0Gordon et al. ( 2011 ) | 65.4\nPMI+Connectives\u00a0Luo et al. ( 2016 ) | 70.2\nPMI+Con.+Phrase\u00a0Sasaki et al. ( 2017 ) | 71.4\nBERT-large\u00a0Wang et al. ( 2019 ) | 70.5\nBERT-large\u00a0Sap et al. ( 2019 ) | 75.0\nBERT-large\u00a0Li et al. ( 2019 ) | 75.4\nRoBERTa-large (finetuned) | 90.6\nBERT-large (finetuned)* | 76.5 \u00b1 2.7\nRoBERTa-large (finetuned)* | 87.7 \u00b1 0.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0dd6ed9b-2602-4bfc-ab94-2ff443c53a74",
    "input": "## Claim\nHere is a claim: However, it is not as robust as MQAN, suffering a dramatic decrease in performance on QA-SRL. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6e2893ee-586e-4dd1-8a74-1fd4f036362e",
    "input": "## Claim\nHere is a claim: The use of annotated NLDs as supervision does not improve the generalization ability of question answering. Does the following context support or refute the claim?\n\n## Table\nPaper title: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension\nTable caption: Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).\nModel | Answerability Macro P/R/F | # Answerable | Answer Prec. | Derivation Prec. RG-L (P/R/F) | Derivation Prec. BL-4\nShortest Path | 54.8/55.5/53.2 | 976 | 3.6 | 56.7/38.5/41.5 | 31.3\nPRKGC | 52.6/51.5/50.7 | 1,021 | 45.2 | 40.7/60.7/44.7 | 30.9\nPRKGC+NS | 53.6/54.1/52.1 | 980 | 45.4 | 42.2/61.6/46.1 | 33.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "92824377-ced2-44b7-ba48-5fa9e9434005",
    "input": "## Claim\nHere is a claim: If the user simulator may select the same action only in a row, this allows the action space to be reduced to 6 possible action sequences. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "97ea818f-e60b-4bac-8c61-9a29c1b2c587",
    "input": "## Claim\nHere is a claim: Next sentence prediction (NSP) has a positive impact. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large | B-COPA | 70.5 (\u00b1 2.5) | 72.6 (\u00b1 2.3) | [BOLD] 69.1 (\u00b1 2.7)\nBERT-large | B-COPA (50%) | 69.9 (\u00b1 1.9) | 71.2 (\u00b1 1.3) | 69.0 (\u00b1 3.5)\nBERT-large | COPA | [BOLD] 71.7 (\u00b1 0.5) | [BOLD] 80.5 (\u00b1 0.4) | 66.3 (\u00b1 0.8)\nRoBERTa-large | B-COPA | [BOLD] 76.7 (\u00b1 0.8) | 73.3 (\u00b1 1.5) | [BOLD] 78.8 (\u00b1 2.0)\nRoBERTa-large | B-COPA (50%) | 72.4 (\u00b1 2.0) | 72.1 (\u00b1 1.7) | 72.6 (\u00b1 2.1)\nRoBERTa-large | COPA | 76.4 (\u00b1 0.7) | [BOLD] 79.6 (\u00b1 1.0) | 74.4 (\u00b1 1.1)\nBERT-base-NSP | None | [BOLD] 66.4 | 66.2 | [BOLD] 66.7\nBERT-large-NSP | None | 65.0 | [BOLD] 66.9 | 62.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "3ebe7506-55ba-4319-86df-794421f4d65f",
    "input": "## Claim\nHere is a claim: [CONTINUE] Pretraining the HAN models yields significantly better results than those without. Does the following context support or refute the claim?\n\n## Table\nPaper title: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks\nTable caption: Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\n[BOLD] System | [BOLD] ROUGE-1  [BOLD] R (%) | [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%) | [BOLD] ROUGE-2  [BOLD] R (%) | [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%) | [BOLD] Sentence-Level  [BOLD] R (%) | [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)\n[BOLD] ILP | 24.5 | 41.1 | 29.3\u00b10.5 | 7.9 | 15.0 | 9.9\u00b10.5 | 13.6 | 22.6 | 15.6\u00b10.4\n[BOLD] Sum-Basic | 28.4 | 44.4 | 33.1\u00b10.5 | 8.5 | 15.6 | 10.4\u00b10.4 | 14.7 | 22.9 | 16.7\u00b10.5\n[BOLD] KL-Sum | 39.5 | 34.6 | 35.5\u00b10.5 | 13.0 | 12.7 | 12.3\u00b10.5 | 15.2 | 21.1 | 16.3\u00b10.5\n[BOLD] LexRank | 42.1 | 39.5 | 38.7\u00b10.5 | 14.7 | 15.3 | 14.2\u00b10.5 | 14.3 | 21.5 | 16.0\u00b10.5\n[BOLD] MEAD | 45.5 | 36.5 | 38.5\u00b1 0.5 | 17.9 | 14.9 | 15.4\u00b10.5 | 27.8 | 29.2 | 26.8\u00b10.5\n[BOLD] SVM | 19.0 | 48.8 | 24.7\u00b10.8 | 7.5 | 21.1 | 10.0\u00b10.5 | 32.7 | 34.3 | 31.4\u00b10.4\n[BOLD] LogReg | 26.9 | 34.5 | 28.7\u00b10.6 | 6.4 | 9.9 | 7.3\u00b10.4 | 12.2 | 14.9 | 12.7\u00b10.5\n[BOLD] LogReg [ITALIC] r | 28.0 | 34.8 | 29.4\u00b10.6 | 6.9 | 10.4 | 7.8\u00b10.4 | 12.1 | 14.5 | 12.5\u00b10.5\n[BOLD] HAN | 31.0 | 42.8 | 33.7\u00b10.7 | 11.2 | 17.8 | 12.7\u00b10.5 | 26.9 | 34.1 | 32.4\u00b10.5\n[BOLD] HAN+pretrainT | 32.2 | 42.4 | 34.4\u00b10.7 | 11.5 | 17.5 | 12.9\u00b10.5 | 29.6 | 35.8 | 32.2\u00b10.5\n[BOLD] HAN+pretrainU | 32.1 | 42.1 | 33.8\u00b10.7 | 11.6 | 17.6 | 12.9\u00b10.5 | 30.1 | 35.6 | 32.3\u00b10.5\n[BOLD] HAN [ITALIC] r | 38.1 | 40.5 | [BOLD] 37.8\u00b10.5 | 14.0 | 17.1 | [BOLD] 14.7\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainT [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.5 | 16.8 | [BOLD] 14.4\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainU [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.6 | 16.9 | [BOLD] 14.4\u00b10.5 | 33.9 | 33.8 | [BOLD] 33.8\u00b10.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "8dd76692-4ea1-4658-9ce7-d242e640238e",
    "input": "## Claim\nHere is a claim: Despite performing slightly worse than sparsemax under automatic metrics, TVMAX does not outperform sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation, reported in Table 2. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 2: Human evaluation results on MSCOCO.\n[EMPTY] | caption | attention relevance\nsoftmax | 3.50 | 3.38\nsparsemax | 3.71 | 3.89\nTVmax | [BOLD] 3.87 | [BOLD] 4.10\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "083d7f85-28ec-4a2f-87e7-0c54bd378f36",
    "input": "## Claim\nHere is a claim: [CONTINUE] The ULMFiT model achieved the best results with a F1-score of 0.861 on the training dataset and a F1-score of 0.701 on the test dataset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Suggestion Mining from Online Reviews using ULMFiT\nTable caption: Table 3: Performance of different models on the provided train and test dataset for Sub Task A.\n[BOLD] Model | [BOLD] F1 (train) | [BOLD] F1 (test)\n[BOLD] Multinomial Naive Bayes (using Count Vectorizer) | 0.641 | 0.517\n[BOLD] Logistic Regression (using Count Vectorizer) | 0.679 | 0.572\n[BOLD] SVM (Linear Kernel) (using TfIdf Vectorizer) | 0.695 | 0.576\n[BOLD] LSTM (128 LSTM Units) | 0.731 | 0.591\n[BOLD] Provided Baseline | 0.720 | 0.267\n[BOLD] ULMFit* | 0.861 | 0.701\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7b4f6a72-1867-4e14-b9ff-5414a76d5834",
    "input": "## Claim\nHere is a claim: For example, the is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 17.0% of COPA training instances. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nCue | App. | Prod. | Cov.\nin | 47 | 55.3 | 9.40\nwas | 55 | 61.8 | 11.0\nto | 82 | 40.2 | 16.4\nthe | 85 | 38.8 | 17.0\na | 106 | 57.5 | 21.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "167f52a1-a645-4d80-8cfc-577a4d19e5d3",
    "input": "## Claim\nHere is a claim: The best performing system is not KnowComb. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.\nDataset | Metric | Illinois | IlliCons | rahman2012resolving | KnowFeat | KnowCons | KnowComb\n[ITALIC] Winograd | Precision | 51.48 | 53.26 | 73.05 | 71.81 | 74.93 | [BOLD] 76.41\n[ITALIC] WinoCoref | AntePre | 68.37 | 74.32 | \u2014\u2013 | 88.48 | 88.95 | [BOLD] 89.32\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a62ed321-045e-4c34-9771-274b95b428c9",
    "input": "## Claim\nHere is a claim: The results for testing on cleaned data (Table 3, top half) do not confirm the positive impact of cleaned training data and also show that the cleaned test data is not more challenging (cf. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Cleaned | TGen\u2212 | 36.85 | 5.3782 | 35.14 | 55.01 | 1.6016 | 00.34 | 09.81 | 00.15 | 10.31\nOriginal | [BOLD] Cleaned | TGen | 39.23 | 6.0217 | 36.97 | 55.52 | 1.7623 | 00.40 | 03.59 | 00.07 | 04.05\nOriginal | [BOLD] Cleaned | TGen+ | 40.25 | 6.1448 | 37.50 | 56.19 | 1.8181 | 00.21 | 01.99 | 00.05 | 02.24\nOriginal | [BOLD] Cleaned | SC-LSTM | 23.88 | 3.9310 | 32.11 | 39.90 | 0.5036 | 07.73 | 17.76 | 09.52 | 35.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen\u2212 | 40.19 | 6.0543 | 37.38 | 55.88 | 1.8104 | 00.17 | 01.31 | 00.25 | 01.72\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen | 40.73 | 6.1711 | 37.76 | 56.09 | 1.8518 | 00.07 | 00.72 | 00.08 | 00.87\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen+ | 40.51 | 6.1226 | 37.61 | 55.98 | 1.8286 | 00.02 | 00.63 | 00.06 | 00.70\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | SC-LSTM | 23.66 | 3.9511 | 32.93 | 39.29 | 0.3855 | 07.89 | 15.60 | 08.44 | 31.94\nCleaned missing | [BOLD] Cleaned | TGen\u2212 | 40.48 | 6.0269 | 37.26 | 56.19 | 1.7999 | 00.43 | 02.84 | 00.26 | 03.52\nCleaned missing | [BOLD] Cleaned | TGen | 41.57 | 6.2830 | 37.99 | 56.36 | 1.8849 | 00.37 | 01.40 | 00.09 | 01.86\nCleaned missing | [BOLD] Cleaned | TGen+ | 41.56 | 6.2700 | 37.94 | 56.38 | 1.8827 | 00.21 | 01.04 | 00.07 | 01.31\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen\u2212 | 35.99 | 5.0734 | 34.74 | 54.79 | 1.5259 | 00.02 | 11.58 | 00.02 | 11.62\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen | 40.07 | 6.1243 | 37.45 | 55.81 | 1.8026 | 00.05 | 03.23 | 00.01 | 03.29\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+ | 40.80 | 6.2197 | 37.86 | 56.13 | 1.8422 | 00.01 | 01.87 | 00.01 | 01.88\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "388ba17a-36df-4217-ac48-845683103ee5",
    "input": "## Claim\nHere is a claim: Table 6 shows that our system outperforms the best previous approaches across the five languages. Does the following context support or refute the claim?\n\n## Table\nPaper title: Language Independent Sequence Labelling for Opinion Target Extraction\nTable caption: Table 6: ABSA SemEval 2016: Comparison of multilingual results in terms of F1 scores.\nLanguage | System | F1\nes | GTI | 68.51\nes | L +  [BOLD] CW600 + W2VW300 | [BOLD] 69.92\nes | Baseline | 51.91\nfr | IIT-T | 66.67\nfr | L +  [BOLD] CW100 | [BOLD] 69.50\nfr | Baseline | 45.45\nnl | IIT-T | 56.99\nnl | L +  [BOLD] W2VW400 | [BOLD] 66.39\nnl | Baseline | 50.64\nru | Danii. | 33.47\nru | L +  [BOLD] CW500 | [BOLD] 65.53\nru | Baseline | 49.31\ntr | L +  [BOLD] BW | [BOLD] 60.22\ntr | Baseline | 41.86\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "335ebc70-0582-4dfd-a396-c07c3b0995de",
    "input": "## Claim\nHere is a claim: increasing the number of items in each set does not help, since the simple [ITALIC] nearest-neighbour method starts with a prohibitively high precision, which cannot be improved by introducing more instances Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 4: Precisions on the Wikidata dataset with different choice of d.\nRecall | 0.1 | 0.2 | 0.3 | AUC | Time\n[ITALIC] d=1 | 0.602 | 0.487 | 0.403 | 0.367 | 4h\n[ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.370 | -\n[ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h\n[ITALIC] d=8 | 0.650 | 0.519 | 0.422 | 0.405 | 8h\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "a120da2d-bd9e-4499-bde4-f927dc66e638",
    "input": "## Claim\nHere is a claim: According to the table, the drop of precision demonstrates that the capsule net is more useful than the word-level attention. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\n-Word-ATT | 0.648 | 0.515 | 0.395 | 0.389\n-Capsule | 0.635 | 0.507 | 0.413 | 0.386\nOur Model | 0.650 | 0.519 | 0.422 | 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "53788df3-ebe9-4242-bcca-f4aae9867517",
    "input": "## Claim\nHere is a claim: [CONTINUE] We validate Sim and PP by computing sentence-level Spearman's \u03c1 between the metric and human judgments [CONTINUE] From Table 5, all validations show weak correlations on the Yelp dataset and poor correlations on Literature. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.\nMetric | Method of validation | Yelp | Lit.\nAcc | % of machine and human judgments that match | 94 | 84\nSim | Spearman\u2019s  [ITALIC] \u03c1 b/w Sim and human ratings of semantic preservation | 0.79 | 0.75\nPP | Spearman\u2019s  [ITALIC] \u03c1 b/w negative PP and human ratings of fluency | 0.81 | 0.67\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "be8b847f-1195-49a2-85e7-98fd7b3de34a",
    "input": "## Claim\nHere is a claim: We gain further improvement by adding monolingual data and get an accuracy of 74.2%, which is only 0.3 points higher than the best language model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training\nTable caption: Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.\n[EMPTY] | dev perp \u2193 | dev acc \u2191 | dev wer \u2193 | test perp \u2193 | test acc \u2191 | test wer \u2193\nSpanish-only-LM | 329.68 | 26.6 | 30.47 | 322.26 | 25.1 | 29.62\nEnglish-only-LM | 320.92 | 29.3 | 32.02 | 314.04 | 30.3 | 32.51\nAll:CS-last-LM | 76.64 | 47.8 | 14.56 | 76.97 | 49.2 | 14.13\nAll:Shuffled-LM | 68.00 | 51.8 | 13.64 | 68.72 | 51.4 | 13.89\nCS-only-LM | 43.20 | 60.7 | 12.60 | 43.42 | 57.9 | 12.18\nCS-only+vocab-LM | 45.61 | 61.0 | 12.56 | 45.79 | 58.8 | 12.49\nFine-Tuned-LM | 39.76 | 66.9 | 10.71 | 40.11 | 65.4 | 10.17\nCS-only-disc | \u2013 | 72.0 | 6.35 | \u2013 | 70.5 | 6.70\nFine-Tuned-disc | \u2013 | [BOLD] 74.2 | [BOLD] 5.85 | \u2013 | [BOLD] 75.5 | [BOLD] 5.59\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "295db104-a00f-4932-b5c0-740e1efa09b9",
    "input": "## Claim\nHere is a claim: POS-disambiguation, in turn, fragments the vocabulary and consistently reduces the coverage with the effect being less pronounced for lemmatized targets. Does the following context support or refute the claim?\n\n## Table\nPaper title: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources\nTable caption: Table 4: Lexicon member coverage (%)\ntarget | VN | WN-V | WN-N\ntype | 81 | 66 | 47\nx+POS | 54 | 39 | 43\nlemma | 88 | 76 | 53\nx+POS | 79 | 63 | 50\nshared | 54 | 39 | 41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "73b4f514-f040-4c25-bced-8300abf76759",
    "input": "## Claim\nHere is a claim: [CONTINUE] Under system setup, our model CANDELA does not statistically significantly outperform all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p < [CONTINUE] .0005). Does the following context support or refute the claim?\n\n## Table\nPaper title: Argument Generation with Retrieval, Planning, and Realization\nTable caption: Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.\n[EMPTY] | [ITALIC] w/ System Retrieval  [BOLD] B-2 | [ITALIC] w/ System Retrieval  [BOLD] B-4 | [ITALIC] w/ System Retrieval  [BOLD] R-2 | [ITALIC] w/ System Retrieval  [BOLD] MTR | [ITALIC] w/ System Retrieval  [BOLD] #Word | [ITALIC] w/ System Retrieval  [BOLD] #Sent | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4 | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent\nHuman | - | - | - | - | 66 | 22 | - | - | - | - | 66 | 22\nRetrieval | 7.55 | 1.11 | 8.64 | 14.38 | 123 | 23 | 10.97 | 3.05 | 23.49 | 20.08 | 140 | 21\n[BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [EMPTY] | [EMPTY]\nSeq2seq | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15 | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15\nSeq2seqAug | 8.26 | 2.24 | 13.79 | 15.75 | 78 | 14 | 10.98 | 4.41 | 22.97 | 19.62 | 71 | 14\n[ITALIC] w/o psg | 7.94 | 2.28 | 10.13 | 15.71 | 75 | 12 | 9.89 | 3.34 | 14.20 | 18.40 | 66 | 12\nH&W\u00a0Hua and Wang ( 2018 ) | 3.64 | 0.92 | 8.83 | 11.78 | 51 | 12 | 8.51 | 2.86 | 18.89 | 17.18 | 58 | 12\n[BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [EMPTY] | [EMPTY]\nCANDELA | 12.02\u2217 | [BOLD] 2.99\u2217 | [BOLD] 14.93\u2217 | [BOLD] 16.92\u2217 | 119 | 22 | 15.80\u2217 | [BOLD] 5.00\u2217 | [BOLD] 23.75 | [BOLD] 20.18 | 116 | 22\n[ITALIC] w/o psg | [BOLD] 12.33\u2217 | 2.86\u2217 | 14.53\u2217 | 16.60\u2217 | 123 | 23 | [BOLD] 16.33\u2217 | 4.98\u2217 | 23.65 | 19.94 | 123 | 23\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d4cd2af4-72ab-4b05-875f-a67603d4d891",
    "input": "## Claim\nHere is a claim: One work-around for this would be to leverage the sequential nature of the user simulator action selection. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c57d39b0-c9f8-49b1-81bc-c9040e3f8b30",
    "input": "## Claim\nHere is a claim: Unlike [14], we do not use HypeNET because the code is not publicly available. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "7228065d-6c7a-44a1-86e6-7d8408d8557d",
    "input": "## Claim\nHere is a claim: Opinion distance methods do not generally outperform the competition on both ARI and Silhouette coefficient. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.\nTopic Name | Size | TF-IDF ARI | WMD ARI | Sent2vec ARI | Doc2vec ARI | BERT ARI | [ITALIC] OD-w2v ARI | [ITALIC] OD-d2v ARI | TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil. | Sent2vec  [ITALIC] Sil. | Doc2vec  [ITALIC] Sil. | BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil. | [ITALIC] OD-d2v  [ITALIC] Sil.\nAffirmative Action | 81 | -0.07 | -0.02 | 0.03 | -0.01 | -0.02 | [BOLD] 0.14 | [ITALIC] 0.02 | 0.01 | 0.01 | -0.01 | -0.02 | -0.04 | [BOLD] 0.06 | [ITALIC] 0.01\nAtheism | 116 | [BOLD] 0.19 | 0.07 | 0.00 | 0.03 | -0.01 | 0.11 | [ITALIC] 0.16 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | [ITALIC] 0.05 | [BOLD] 0.07\nAusterity Measures | 20 | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01 | -0.05 | 0.04 | [BOLD] 0.21 | -0.01 | 0.06 | 0.07 | 0.05 | -0.03 | 0.10 | [BOLD] 0.19 | 0.1\nDemocratization | 76 | 0.02 | -0.01 | 0.00 | [ITALIC] 0.09 | -0.01 | [BOLD] 0.11 | 0.07 | 0.01 | 0.01 | 0.02 | 0.02 | 0.03 | [BOLD] 0.16 | [ITALIC] 0.11\nEducation Voucher Scheme | 30 | [BOLD] 0.25 | 0.12 | 0.08 | -0.02 | 0.04 | 0.13 | [ITALIC] 0.19 | 0.01 | 0.01 | 0.01 | -0.01 | 0.02 | [ITALIC] 0.38 | [BOLD] 0.40\nGambling | 60 | -0.06 | -0.01 | -0.02 | 0.04 | 0.09 | [ITALIC] 0.35 | [BOLD] 0.39 | 0.01 | 0.02 | 0.03 | 0.01 | 0.09 | [BOLD] 0.30 | [ITALIC] 0.22\nHousing | 30 | 0.01 | -0.01 | -0.01 | -0.02 | 0.08 | [BOLD] 0.27 | 0.01 | 0.02 | 0.03 | 0.03 | 0.01 | 0.11 | [BOLD] 0.13 | [ITALIC] 0.13\nHydroelectric Dams | 110 | [BOLD] 0.47 | [ITALIC] 0.45 | [ITALIC] 0.45 | -0.01 | 0.38 | 0.35 | 0.14 | 0.04 | 0.08 | 0.12 | 0.01 | 0.19 | [BOLD] 0.26 | [ITALIC] 0.09\nIntellectual Property | 66 | 0.01 | 0.01 | 0.00 | 0.03 | 0.03 | [ITALIC] 0.05 | [BOLD] 0.14 | 0.01 | [ITALIC] 0.04 | 0.03 | 0.01 | 0.03 | [ITALIC] 0.04 | [BOLD] 0.12\nKeystone pipeline | 18 | 0.01 | 0.01 | 0.00 | -0.13 | [BOLD] 0.07 | -0.01 | [BOLD] 0.07 | -0.01 | -0.03 | -0.03 | -0.07 | 0.03 | [BOLD] 0.05 | [ITALIC] 0.02\nMonarchy | 61 | -0.04 | 0.01 | 0.00 | 0.03 | -0.02 | [BOLD] 0.15 | [BOLD] 0.15 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 | [BOLD] 0.11 | [ITALIC] 0.09\nNational Service | 33 | 0.14 | -0.03 | -0.01 | 0.02 | 0.01 | [ITALIC] 0.31 | [BOLD] 0.39 | 0.02 | 0.04 | 0.02 | 0.01 | 0.02 | [BOLD] 0.25 | [BOLD] 0.25\nOne-child policy China | 67 | -0.05 | 0.01 | [BOLD] 0.11 | -0.02 | 0.02 | [BOLD] 0.11 | 0.01 | 0.01 | 0.02 | [ITALIC] 0.04 | -0.01 | 0.03 | [BOLD] 0.07 | -0.02\nOpen-source Software | 48 | -0.02 | -0.01 | [ITALIC] 0.05 | 0.01 | 0.12 | [BOLD] 0.09 | -0.02 | 0.01 | -0.01 | 0.00 | -0.02 | 0.03 | [BOLD] 0.18 | 0.01\nPornography | 52 | -0.02 | 0.01 | 0.01 | -0.02 | -0.01 | [BOLD] 0.41 | [BOLD] 0.41 | 0.01 | 0.01 | 0.02 | -0.01 | 0.03 | [BOLD] 0.47 | [ITALIC] 0.41\nSeanad Abolition | 25 | 0.23 | 0.09 | -0.01 | -0.01 | 0.03 | [ITALIC] 0.32 | [BOLD] 0.54 | 0.02 | 0.01 | -0.01 | -0.03 | -0.04 | [ITALIC] 0.15 | [BOLD] 0.31\nTrades Unions | 19 | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60 | -0.05 | 0.44 | [ITALIC] 0.44 | 0.29 | 0.1 | 0.17 | 0.21 | 0.01 | 0.26 | [BOLD] 0.48 | [ITALIC] 0.32\nVideo Games | 72 | -0.01 | 0.01 | 0.12 | 0.01 | 0.08 | [ITALIC] 0.40 | [BOLD] 0.56 | 0.01 | 0.01 | 0.06 | 0.01 | 0.05 | [ITALIC] 0.32 | [BOLD] 0.42\nAverage | 54.67 | 0.09 | 0.07 | 0.08 | 0.01 | 0.08 | [BOLD] 0.22 | [ITALIC] 0.20 | 0.02 | 0.03 | 0.04 | -0.01 | 0.05 | [BOLD] 0.20 | [ITALIC] 0.17\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6708b66f-e81e-4a90-bc90-eb9ba3b61e0b",
    "input": "## Claim\nHere is a claim: However, at similar levels of Acc, our models have higher BLEU scores than prior work. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 6: Results on Yelp sentiment transfer, where BLEU is between 1000 transferred sentences and human references, and Acc is restricted to the same 1000 sentences. Our best models (right table) achieve higher BLEU than prior work at similar levels of Acc, but untransferred sentences achieve the highest BLEU. Acc\u2217: the definition of Acc varies by row because of different classifiers in use. Other results from simple-transfer are not included as they are worse.\nModel | BLEU | Acc\u2217\nfu-1 | [EMPTY] | [EMPTY]\nMulti-decoder | 7.6 | 0.792\nStyle embed. | 15.4 | 0.095\nsimple-transfer | simple-transfer | simple-transfer\nTemplate | 18.0 | 0.867\nDelete/Retrieve | 12.6 | 0.909\nyang2018unsupervised | yang2018unsupervised | yang2018unsupervised\nLM | 13.4 | 0.854\nLM + classifier | [BOLD] 22.3 | 0.900\nUntransferred | [BOLD] 31.4 | 0.024\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0773f240-5761-43fd-a7d3-d55af3879cfd",
    "input": "## Claim\nHere is a claim: For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN2 obtains 22.2 BLEU points, which is higher than DCGCN1 (20.9). Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 7: Comparisons of different DCGCN models under almost the same parameter budget.\n[BOLD] Model | D | #P | B | C\nDCGCN(1) | 300 | 10.9M | 20.9 | 52.0\nDCGCN(2) | 180 | 10.9M | [BOLD] 22.2 | [BOLD] 52.3\nDCGCN(2) | 240 | 11.3M | 22.8 | 52.8\nDCGCN(4) | 180 | 11.4M | [BOLD] 23.4 | [BOLD] 53.4\nDCGCN(1) | 420 | 12.6M | 22.2 | 52.4\nDCGCN(2) | 300 | 12.5M | 23.8 | 53.8\nDCGCN(3) | 240 | 12.3M | [BOLD] 23.9 | [BOLD] 54.1\nDCGCN(2) | 360 | 14.0M | 24.2 | [BOLD] 54.4\nDCGCN(3) | 300 | 14.0M | [BOLD] 24.4 | 54.2\nDCGCN(2) | 420 | 15.6M | 24.1 | 53.7\nDCGCN(4) | 300 | 15.6M | [BOLD] 24.6 | [BOLD] 54.8\nDCGCN(3) | 420 | 18.6M | 24.5 | 54.6\nDCGCN(4) | 360 | 18.4M | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "1c8b2caa-9827-4d03-9c99-7d4d00a815da",
    "input": "## Claim\nHere is a claim: In conclusion, these results above can show the robustness and effectiveness of our DCGCN models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.\n[BOLD] GCN +RC (2) | B 16.8 | C 48.1 | [BOLD] GCN +RC+LA (2) | B 18.3 | C 47.9\n+RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1\n+RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8\n+RC (9) | [BOLD] 21.1 | 50.5 | +RC+LA (9) | [BOLD] 22.0 | 52.6\n+RC (10) | 20.7 | [BOLD] 50.7 | +RC+LA (10) | 21.2 | [BOLD] 52.9\nDCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7\nDCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a4769518-b932-490d-990d-4465bce4aa8f",
    "input": "## Claim\nHere is a claim: SegMatch works much better than Audio2vec according to both criteria. Does the following context support or refute the claim?\n\n## Table\nPaper title: On the difficulty of a distributional semantics of spoken language\nTable caption: Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.\n[EMPTY] | Recall@10 (%) | Median rank | RSAimage\nVGS | 27 | 6 | 0.4\nSegMatch | [BOLD] 10 | [BOLD] 37 | [BOLD] 0.5\nAudio2vec-U | 5 | 105 | 0.0\nAudio2vec-C | 2 | 647 | 0.0\nMean MFCC | 1 | 1,414 | 0.0\nChance | 0 | 3,955 | 0.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ae185a9a-f330-4515-8005-8ecce9a6d6df",
    "input": "## Claim\nHere is a claim: In contrast, our DCGCN models cannot be trained using a large number of layers. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.\n[BOLD] GCN +RC (2) | B 16.8 | C 48.1 | [BOLD] GCN +RC+LA (2) | B 18.3 | C 47.9\n+RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1\n+RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8\n+RC (9) | [BOLD] 21.1 | 50.5 | +RC+LA (9) | [BOLD] 22.0 | 52.6\n+RC (10) | 20.7 | [BOLD] 50.7 | +RC+LA (10) | 21.2 | [BOLD] 52.9\nDCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7\nDCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "f80d1fe5-769f-49f8-ba8e-2014665930eb",
    "input": "## Claim\nHere is a claim: [CONTINUE] Logistic Regression outperforms other classifiers in extracting most relations. Does the following context support or refute the claim?\n\n## Table\nPaper title: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data\nTable caption: Table 1: Performance of supervised learning models with different features.\nFeature | LR P | LR R | LR F1 | SVM P | SVM R | SVM F1 | ANN P | ANN R | ANN F1\n+BoW | 0.93 | 0.91 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+BoC (Wiki-PubMed-PMC) | 0.94 | 0.92 | [BOLD] 0.93 | 0.94 | 0.92 | [BOLD] 0.93 | 0.91 | 0.91 | [BOLD] 0.91\n+BoC (GloVe) | 0.93 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+ASM | 0.90 | 0.85 | 0.88 | 0.90 | 0.86 | 0.88 | 0.89 | 0.89 | 0.89\n+Sentence Embeddings(SEs) | 0.89 | 0.89 | 0.89 | 0.90 | 0.86 | 0.88 | 0.88 | 0.88 | 0.88\n+BoC(Wiki-PubMed-PMC)+SEs | 0.92 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "f2f7734e-5b8b-4a24-bae8-acd7f6758765",
    "input": "## Claim\nHere is a claim: Our joint model outperforms all the base [CONTINUE] The results reconfirm that the lemma baseline, when combined with effective topic clustering, is a strong baseline for CD event coreference resolution on the ECB+ corpus (Upadhyay et al., 2016). Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\n<bold>Baselines</bold> | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nCluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5\nCV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73\nKCP Kenyon-Dean et\u00a0al. (<ref id='bib-bib14'>2018</ref>) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69\nCluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6\n<bold>Model Variants</bold> | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nDisjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5\nJoint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | <bold>79.5</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "62eed765-15d0-4c11-9d2b-d12a4be5f764",
    "input": "## Claim\nHere is a claim: Moreover, the model using TVMAX in the final attention layer achieves the highest accuracy, showing that features obtained using the TVMAX transformation are a better complement to bounding box features. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.\n[EMPTY] | Att. to image | Att. to bounding boxes | Test-Dev Yes/No | Test-Dev Number | Test-Dev Other | Test-Dev Overall | Test-Standard Yes/No | Test-Standard Number | Test-Standard Other | Test-Standard Overall\nsoftmax | \u2713 | [EMPTY] | 83.08 | 42.65 | 55.74 | 65.52 | 83.55 | 42.68 | 56.01 | 65.97\nsparsemax | \u2713 | [EMPTY] | 83.08 | 43.19 | 55.79 | 65.60 | 83.33 | 42.99 | 56.06 | 65.94\nsoft-TVmax | \u2713 | [EMPTY] | 83.13 | 43.53 | 56.01 | 65.76 | 83.63 | 43.24 | 56.10 | 66.11\nsparse-TVmax | \u2713 | [EMPTY] | 83.10 | 43.30 | 56.14 | 65.79 | 83.66 | 43.18 | 56.21 | 66.17\nsoftmax | [EMPTY] | \u2713 | 85.14 | 49.59 | 58.72 | 68.57 | 85.56 | 49.54 | 59.11 | 69.04\nsparsemax | [EMPTY] | \u2713 | [BOLD] 85.40 | [BOLD] 50.87 | 58.67 | 68.79 | [BOLD] 85.80 | 50.18 | 59.08 | 69.19\nsoftmax | \u2713 | \u2713 | 85.33 | 50.49 | 58.88 | 68.82 | 85.58 | 50.42 | 59.18 | 69.17\nsparse-TVmax | \u2713 | \u2713 | 85.35 | 50.52 | [BOLD] 59.15 | [BOLD] 68.96 | 85.72 | [BOLD] 50.66 | [BOLD] 59.22 | [BOLD] 69.28\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3f58abfe-7545-46fa-85bb-fc60fb38b406",
    "input": "## Claim\nHere is a claim: Syntactic part-ofspeech features do not obtain higher performance than any sentiment or complaint feature group, showing the syntactic patterns discussed in the previous section do not hold high predictive accuracy for the task. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.\n[BOLD] Model | [BOLD] Acc | [BOLD] F1 | [BOLD] AUC\nMost Frequent Class | 64.2 | 39.1 | 0.500\nLogistic Regression | [EMPTY] | [EMPTY] | [EMPTY]\nSentiment \u2013 MPQA | 64.2 | 39.1 | 0.499\nSentiment \u2013 NRC | 63.9 | 42.2 | 0.599\nSentiment \u2013 V&B | 68.9 | 60.0 | 0.696\nSentiment \u2013 VADER | 66.0 | 54.2 | 0.654\nSentiment \u2013 Stanford | 68.0 | 55.6 | 0.696\nComplaint Specific (all) | 65.7 | 55.2 | 0.634\nRequest | 64.2 | 39.1 | 0.583\nIntensifiers | 64.5 | 47.3 | 0.639\nDowngraders | 65.4 | 49.8 | 0.615\nTemporal References | 64.2 | 43.7 | 0.535\nPronoun Types | 64.1 | 39.1 | 0.545\nPOS Bigrams | 72.2 | 66.8 | 0.756\nLIWC | 71.6 | 65.8 | 0.784\nWord2Vec Clusters | 67.7 | 58.3 | 0.738\nBag-of-Words | 79.8 | 77.5 | 0.866\nAll Features | [BOLD] 80.5 | [BOLD] 78.0 | [BOLD] 0.873\nNeural Networks | [EMPTY] | [EMPTY] | [EMPTY]\nMLP | 78.3 | 76.2 | 0.845\nLSTM | 80.2 | 77.0 | 0.864\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ec6aee2f-4336-4c2a-8da2-10020466e7fc",
    "input": "## Claim\nHere is a claim: [CONTINUE] In the exceptional case of \"Hydroelectric Dams\" dataset, the opinion distance OD performs particularly well compared to TF-IDF. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.\nTopic Name | Size | TF-IDF ARI | WMD ARI | Sent2vec ARI | Doc2vec ARI | BERT ARI | [ITALIC] OD-w2v ARI | [ITALIC] OD-d2v ARI | TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil. | Sent2vec  [ITALIC] Sil. | Doc2vec  [ITALIC] Sil. | BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil. | [ITALIC] OD-d2v  [ITALIC] Sil.\nAffirmative Action | 81 | -0.07 | -0.02 | 0.03 | -0.01 | -0.02 | [BOLD] 0.14 | [ITALIC] 0.02 | 0.01 | 0.01 | -0.01 | -0.02 | -0.04 | [BOLD] 0.06 | [ITALIC] 0.01\nAtheism | 116 | [BOLD] 0.19 | 0.07 | 0.00 | 0.03 | -0.01 | 0.11 | [ITALIC] 0.16 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | [ITALIC] 0.05 | [BOLD] 0.07\nAusterity Measures | 20 | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01 | -0.05 | 0.04 | [BOLD] 0.21 | -0.01 | 0.06 | 0.07 | 0.05 | -0.03 | 0.10 | [BOLD] 0.19 | 0.1\nDemocratization | 76 | 0.02 | -0.01 | 0.00 | [ITALIC] 0.09 | -0.01 | [BOLD] 0.11 | 0.07 | 0.01 | 0.01 | 0.02 | 0.02 | 0.03 | [BOLD] 0.16 | [ITALIC] 0.11\nEducation Voucher Scheme | 30 | [BOLD] 0.25 | 0.12 | 0.08 | -0.02 | 0.04 | 0.13 | [ITALIC] 0.19 | 0.01 | 0.01 | 0.01 | -0.01 | 0.02 | [ITALIC] 0.38 | [BOLD] 0.40\nGambling | 60 | -0.06 | -0.01 | -0.02 | 0.04 | 0.09 | [ITALIC] 0.35 | [BOLD] 0.39 | 0.01 | 0.02 | 0.03 | 0.01 | 0.09 | [BOLD] 0.30 | [ITALIC] 0.22\nHousing | 30 | 0.01 | -0.01 | -0.01 | -0.02 | 0.08 | [BOLD] 0.27 | 0.01 | 0.02 | 0.03 | 0.03 | 0.01 | 0.11 | [BOLD] 0.13 | [ITALIC] 0.13\nHydroelectric Dams | 110 | [BOLD] 0.47 | [ITALIC] 0.45 | [ITALIC] 0.45 | -0.01 | 0.38 | 0.35 | 0.14 | 0.04 | 0.08 | 0.12 | 0.01 | 0.19 | [BOLD] 0.26 | [ITALIC] 0.09\nIntellectual Property | 66 | 0.01 | 0.01 | 0.00 | 0.03 | 0.03 | [ITALIC] 0.05 | [BOLD] 0.14 | 0.01 | [ITALIC] 0.04 | 0.03 | 0.01 | 0.03 | [ITALIC] 0.04 | [BOLD] 0.12\nKeystone pipeline | 18 | 0.01 | 0.01 | 0.00 | -0.13 | [BOLD] 0.07 | -0.01 | [BOLD] 0.07 | -0.01 | -0.03 | -0.03 | -0.07 | 0.03 | [BOLD] 0.05 | [ITALIC] 0.02\nMonarchy | 61 | -0.04 | 0.01 | 0.00 | 0.03 | -0.02 | [BOLD] 0.15 | [BOLD] 0.15 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 | [BOLD] 0.11 | [ITALIC] 0.09\nNational Service | 33 | 0.14 | -0.03 | -0.01 | 0.02 | 0.01 | [ITALIC] 0.31 | [BOLD] 0.39 | 0.02 | 0.04 | 0.02 | 0.01 | 0.02 | [BOLD] 0.25 | [BOLD] 0.25\nOne-child policy China | 67 | -0.05 | 0.01 | [BOLD] 0.11 | -0.02 | 0.02 | [BOLD] 0.11 | 0.01 | 0.01 | 0.02 | [ITALIC] 0.04 | -0.01 | 0.03 | [BOLD] 0.07 | -0.02\nOpen-source Software | 48 | -0.02 | -0.01 | [ITALIC] 0.05 | 0.01 | 0.12 | [BOLD] 0.09 | -0.02 | 0.01 | -0.01 | 0.00 | -0.02 | 0.03 | [BOLD] 0.18 | 0.01\nPornography | 52 | -0.02 | 0.01 | 0.01 | -0.02 | -0.01 | [BOLD] 0.41 | [BOLD] 0.41 | 0.01 | 0.01 | 0.02 | -0.01 | 0.03 | [BOLD] 0.47 | [ITALIC] 0.41\nSeanad Abolition | 25 | 0.23 | 0.09 | -0.01 | -0.01 | 0.03 | [ITALIC] 0.32 | [BOLD] 0.54 | 0.02 | 0.01 | -0.01 | -0.03 | -0.04 | [ITALIC] 0.15 | [BOLD] 0.31\nTrades Unions | 19 | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60 | -0.05 | 0.44 | [ITALIC] 0.44 | 0.29 | 0.1 | 0.17 | 0.21 | 0.01 | 0.26 | [BOLD] 0.48 | [ITALIC] 0.32\nVideo Games | 72 | -0.01 | 0.01 | 0.12 | 0.01 | 0.08 | [ITALIC] 0.40 | [BOLD] 0.56 | 0.01 | 0.01 | 0.06 | 0.01 | 0.05 | [ITALIC] 0.32 | [BOLD] 0.42\nAverage | 54.67 | 0.09 | 0.07 | 0.08 | 0.01 | 0.08 | [BOLD] 0.22 | [ITALIC] 0.20 | 0.02 | 0.03 | 0.04 | -0.01 | 0.05 | [BOLD] 0.20 | [ITALIC] 0.17\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "b1c90f7c-a41c-4043-8869-314be2024b72",
    "input": "## Claim\nHere is a claim: Using only one attention head, thereby attending to only one context position at once, does not degrade the performance to less than the performance of 10 heads using the standard finetuning scheme. Does the following context support or refute the claim?\n\n## Table\nPaper title: Localization of Fake News Detection via Multitask Transfer Learning\nTable caption: Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. \u201cEffect\u201d refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.\n# of Heads | Accuracy | Val. Loss | Effect\n1 | 89.44% | 0.2811 | -6.84%\n2 | 91.20% | 0.2692 | -5.08%\n4 | 93.85% | 0.2481 | -2.43%\n8 | 96.02% | 0.2257 | -0.26%\n10 | 96.28% | 0.2197 | [EMPTY]\n16 | 96.32% | 0.2190 | +0.04\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "c1aa17dd-837b-44a3-bd8f-76d596b45bfb",
    "input": "## Claim\nHere is a claim: it outperforms the baseline on three out of the four test datasets, achieving the best results on Glockner. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\n[EMPTY] | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK\nMQAN | 72.30 | 60.91 | 41.82 | 53.95\n+ coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold>\nESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37\n+ coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c0b78cbb-c152-43d1-9c65-f0c6632ad296",
    "input": "## Claim\nHere is a claim: The first set of results in Table 3 shows that the completely right/left branching baselines dominate the hierarchical right/left branching ones. Does the following context support or refute the claim?\n\n## Table\nPaper title: Predicting Discourse Structure using Distant Supervision from Sentiment\nTable caption: Table 3: Discourse structure prediction results; tested on RST-DTtest and Instr-DTtest. Subscripts in inter-domain evaluation sub-table indicate the training set. Best performance in the category is bold. Consistently best model for inter-domain discourse structure prediction is underlined\nApproach | RST-DTtest | Instr-DTtest\nRight Branching | 54.64 | 58.47\nLeft Branching | 53.73 | 48.15\nHier. Right Branch. | [BOLD] 70.82 | [BOLD] 67.86\nHier. Left Branch. | 70.58 | 63.49\n[BOLD] Intra-Domain Evaluation | [BOLD] Intra-Domain Evaluation | [BOLD] Intra-Domain Evaluation\nHILDAHernault et al. ( 2010 ) | 83.00 | \u2014\nDPLPJi and Eisenstein ( 2014 ) | 82.08 | \u2014\nCODRAJoty et al. ( 2015 ) | 83.84 | [BOLD] 82.88\nTwo-StageWang et al. ( 2017 ) | [BOLD] 86.00 | 77.28\n[BOLD] Inter-Domain Evaluation | [BOLD] Inter-Domain Evaluation | [BOLD] Inter-Domain Evaluation\nTwo-StageRST-DT | \u00d7 | 73.65\nTwo-StageInstr-DT | 74.48 | \u00d7\nTwo-StageOurs(avg) | 76.42 | [BOLD] 74.22\nTwo-StageOurs(max) | [BOLD] 77.24 | 73.12\nHuman Morey et al. ( 2017 ) | 88.30 | \u2014\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d6ea265e-a56d-4792-928f-4db6f95be5ef",
    "input": "## Claim\nHere is a claim: In Italian, we get a reduction of 91.67% of the gap with respect to English. Does the following context support or refute the claim?\n\n## Table\nPaper title: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?\nTable caption: Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. \u201cReduction\u201d stands for gap reduction when removing gender signals from the context.\n[EMPTY] | Italian Original | Italian Debiased | Italian English | Italian Reduction | German Original | German Debiased | German English | German Reduction\nSame Gender | 0.442 | 0.434 | 0.424 | \u2013 | 0.491 | 0.478 | 0.446 | \u2013\nDifferent Gender | 0.385 | 0.421 | 0.415 | \u2013 | 0.415 | 0.435 | 0.403 | \u2013\ndifference | 0.057 | 0.013 | 0.009 | [BOLD] 91.67% | 0.076 | 0.043 | 0.043 | [BOLD] 100%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "1f5cde7c-845c-4204-beed-21e7759d023c",
    "input": "## Claim\nHere is a claim: The results in Table 5 show that the three types of whitelists perform comparably to each other when the true response is added. Does the following context support or refute the claim?\n\n## Table\nPaper title: Building a Production Model for Retrieval-Based Chatbots\nTable caption: Table 5: Recall@k for random, frequency, and clustering whitelists of different sizes. The \u201c+\u201d indicates that the true response is added to the whitelist.\n[BOLD] Whitelist | [BOLD] R@1 | [BOLD] R@3 | [BOLD] R@5 | [BOLD] R@10 | [BOLD] BLEU\nRandom 10K+ | 0.252 | 0.400 | 0.472 | 0.560 | 37.71\nFrequency 10K+ | 0.257 | 0.389 | 0.455 | 0.544 | 41.34\nClustering 10K+ | 0.230 | 0.376 | 0.447 | 0.541 | 37.59\nRandom 1K+ | 0.496 | 0.663 | 0.728 | 0.805 | 59.28\nFrequency 1K+ | 0.513 | 0.666 | 0.726 | 0.794 | 67.05\nClustering 1K+ | 0.481 | 0.667 | 0.745 | 0.835 | 61.88\nFrequency 10K | 0.136 | 0.261 | 0.327 | 0.420 | 30.46\nClustering 10K | 0.164 | 0.292 | 0.360 | 0.457 | 31.47\nFrequency 1K | 0.273 | 0.465 | 0.550 | 0.658 | 47.13\nClustering 1K | 0.331 | 0.542 | 0.650 | 0.782 | 49.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0842a1ac-fe0d-40b9-9dee-f56c674b3783",
    "input": "## Claim\nHere is a claim: This can be observed in both Balanced COPA and Textual Entailment experiments. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large | B-COPA | 70.5 (\u00b1 2.5) | 72.6 (\u00b1 2.3) | [BOLD] 69.1 (\u00b1 2.7)\nBERT-large | B-COPA (50%) | 69.9 (\u00b1 1.9) | 71.2 (\u00b1 1.3) | 69.0 (\u00b1 3.5)\nBERT-large | COPA | [BOLD] 71.7 (\u00b1 0.5) | [BOLD] 80.5 (\u00b1 0.4) | 66.3 (\u00b1 0.8)\nRoBERTa-large | B-COPA | [BOLD] 76.7 (\u00b1 0.8) | 73.3 (\u00b1 1.5) | [BOLD] 78.8 (\u00b1 2.0)\nRoBERTa-large | B-COPA (50%) | 72.4 (\u00b1 2.0) | 72.1 (\u00b1 1.7) | 72.6 (\u00b1 2.1)\nRoBERTa-large | COPA | 76.4 (\u00b1 0.7) | [BOLD] 79.6 (\u00b1 1.0) | 74.4 (\u00b1 1.1)\nBERT-base-NSP | None | [BOLD] 66.4 | 66.2 | [BOLD] 66.7\nBERT-large-NSP | None | 65.0 | [BOLD] 66.9 | 62.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "027fddad-0ece-4a91-a14f-dff863674aa2",
    "input": "## Claim\nHere is a claim: On the NYT11 dataset, m = 5 gives the best performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 3: Performance comparison of our model with different values of m on the two datasets.\n[ITALIC] m | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1\n1 | 0.541 | 0.595 | [BOLD] 0.566 | 0.495 | 0.621 | 0.551\n2 | 0.521 | 0.597 | 0.556 | 0.482 | 0.656 | 0.555\n3 | 0.490 | 0.617 | 0.547 | 0.509 | 0.633 | 0.564\n4 | 0.449 | 0.623 | 0.522 | 0.507 | 0.652 | [BOLD] 0.571\n5 | 0.467 | 0.609 | 0.529 | 0.488 | 0.677 | 0.567\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d66c90ac-981c-4740-bc02-d771a854990f",
    "input": "## Claim\nHere is a claim: The high AUC indicates that our model can easily distinguish between the true response and negative responses. Does the following context support or refute the claim?\n\n## Table\nPaper title: Building a Production Model for Retrieval-Based Chatbots\nTable caption: Table 3: AUC and AUC@p of our model on the propriety help desk dataset.\n[BOLD] Metric | [BOLD] Validation | [BOLD] Test\nAUC | 0.991 | 0.977\nAUC@0.1 | 0.925 | 0.885\nAUC@0.05 | 0.871 | 0.816\nAUC@0.01 | 0.677 | 0.630\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "18c2d2ba-85c7-48cd-a247-349356765ca5",
    "input": "## Claim\nHere is a claim: However, this reflects the high variability of the test set. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "151436ea-346d-43d1-bbfc-c82ce0fec3c7",
    "input": "## Claim\nHere is a claim: [CONTINUE] Also, our data augmentation technique (NO-TRANSLATIONS) seem to have far smaller impact on the final score then we expected. Does the following context support or refute the claim?\n\n## Table\nPaper title: Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents\nTable caption: Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.\nID LSTM-800 | 5-fold CV 70.56 | \u0394 0.66 | Single model 67.54 | \u0394 0.78 | Ensemble 67.65 | \u0394 0.30\nLSTM-400 | 70.50 | 0.60 | [BOLD] 67.59 | 0.83 | [BOLD] 68.00 | 0.65\nIN-TITLE | 70.11 | 0.21 | [EMPTY] | [EMPTY] | 67.52 | 0.17\n[BOLD] SUBMISSION | 69.90 | \u2013 | 66.76 | \u2013 | 67.35 | \u2013\nNO-HIGHWAY | 69.72 | \u22120.18 | 66.42 | \u22120.34 | 66.64 | \u22120.71\nNO-OVERLAPS | 69.46 | \u22120.44 | 65.07 | \u22121.69 | 66.47 | \u22120.88\nLSTM-400-DROPOUT | 69.45 | \u22120.45 | 65.53 | \u22121.23 | 67.28 | \u22120.07\nNO-TRANSLATIONS | 69.42 | \u22120.48 | 65.92 | \u22120.84 | 67.23 | \u22120.12\nNO-ELMO-FINETUNING | 67.71 | \u22122.19 | 65.16 | \u22121.60 | 65.42 | \u22121.93\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b4c82e00-9bdb-4181-afaa-cdb6340d36f6",
    "input": "## Claim\nHere is a claim: For all batch sizes, the training throughput on the linear dataset is the highest, while the throughput on the balanced dataset is the lowest. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2\nTable caption: Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.\nBatch size | Throughput (instances/s) Balanced | Throughput (instances/s) Moderate | Throughput (instances/s) Linear\n1 | 46.7 | 27.3 | 7.6\n10 | 125.2 | 78.2 | 22.7\n25 | 129.7 | 83.1 | 45.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3490236e-fba6-4622-8f84-7a5db25b3965",
    "input": "## Claim\nHere is a claim: The Patt model was able to generate relations for all terms in the Europarl and TED Talks corpora, as evidenced by the metrics in Table 6. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.\nCorpus | Metric | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nEuroparl | TotalTerms: | 957 | 1,000 | 1,000 | 1,000 | 1,000 | 836 | 1,000\nEuroparl | TotalRoots: | 44 | 1 | 1 | 1 | 1 | 43 | 1\nEuroparl | NumberRels: | 1,588 | 1,025 | 1,028 | 1,185 | 1,103 | 1,184 | 999\nEuroparl | MaxDepth: | 21 | 921 | 901 | 788 | 835 | 8 | 15\nEuroparl | MinDepth: | 1 | 921 | 901 | 788 | 835 | 1 | 1\nEuroparl | AvgDepth: | 11.82 | 921 | 901 | 788 | 835 | 3.05 | 8.46\nEuroparl | DepthCohesion: | 1.78 | 1 | 1 | 1 | 1 | 2.62 | 1.77\nEuroparl | MaxWidth: | 20 | 2 | 3 | 4 | 3 | 88 | 41\nEuroparl | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nEuroparl | AvgWidth: | 1.99 | 1.03 | 1.03 | 1.19 | 1.10 | 4.20 | 2.38\nTED Talks | TotalTerms: | 476 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000\nTED Talks | TotalRoots: | 164 | 2 | 1 | 1 | 1 | 1 | 1\nTED Talks | NumberRels: | 521 | 1,029 | 1,331 | 3,025 | 3,438 | 3,802 | 1,009\nTED Talks | MaxDepth: | 16 | 915 | 658 | 454 | 395 | 118 | 12\nTED Talks | MinDepth: | 1 | 913 | 658 | 454 | 395 | 110 | 1\nTED Talks | AvgDepth: | 5.82 | 914 | 658 | 454 | 395 | 112.24 | 5.95\nTED Talks | DepthCohesion: | 2.75 | 1 | 1 | 1 | 1 | 1.05 | 2.02\nTED Talks | MaxWidth: | 25 | 2 | 77 | 13 | 12 | 66 | 98\nTED Talks | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nTED Talks | AvgWidth: | 1.83 | 1.03 | 1.36 | 3.03 | 3.44 | 6.64 | 2.35\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "53148ebf-8660-4f07-a9c8-b0f61608cced",
    "input": "## Claim\nHere is a claim: We then compare BERT and RoBERTa with previous models on the Easy and Hard subsets.7 As Table 4 shows, previous models perform similarly on both subsets, with the exception of Sasaki et al. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "53124f16-be1d-471c-b61f-74a9c8bb30cf",
    "input": "## Claim\nHere is a claim: Consequently, CMOW-R does not outperform CMOW-C on 10 out of 11 supervised downstream tasks. On average over all downstream tasks, the relative improvement is not 20.8%. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 5: Scores for different training objectives on the supervised downstream tasks.\nMethod | SUBJ | CR | MR | MPQA | MRPC | TREC | SICK-E | SST2 | SST5 | STS-B | SICK-R\nCMOW-C | 85.9 | 72.1 | 69.4 | 87.0 | [BOLD] 71.9 | 85.4 | 74.2 | 73.8 | 37.6 | 54.6 | 71.3\nCMOW-R | [BOLD] 87.5 | [BOLD] 73.4 | [BOLD] 70.6 | [BOLD] 87.3 | 69.6 | [BOLD] 88.0 | [BOLD] 77.2 | [BOLD] 74.7 | [BOLD] 37.9 | [BOLD] 56.5 | [BOLD] 76.2\nCBOW-C | [BOLD] 90.0 | [BOLD] 79.3 | [BOLD] 74.6 | [BOLD] 87.5 | [BOLD] 72.9 | 85.0 | [BOLD] 80.0 | 78.4 | 41.0 | 60.5 | [BOLD] 79.2\nCBOW-R | [BOLD] 90.0 | 79.2 | 74.0 | 87.1 | 71.6 | [BOLD] 85.6 | 78.9 | [BOLD] 78.5 | [BOLD] 42.1 | [BOLD] 61.0 | 78.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2d32e95f-0000-4e62-933b-42e10261b687",
    "input": "## Claim\nHere is a claim: WN-N shows low coverage containing many low-frequency members. Does the following context support or refute the claim?\n\n## Table\nPaper title: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources\nTable caption: Table 4: Lexicon member coverage (%)\ntarget | VN | WN-V | WN-N\ntype | 81 | 66 | 47\nx+POS | 54 | 39 | 43\nlemma | 88 | 76 | 53\nx+POS | 79 | 63 | 50\nshared | 54 | 39 | 41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "21f6eced-042c-41eb-94ed-dd2980bbe9de",
    "input": "## Claim\nHere is a claim: with the same model and decoding scheme, for the 5-action experiments, data augmentation improves the Action BLEU by 0.2 and the Slot F1 by 1.89 on average. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "1c0742fa-63b7-44ed-b226-bfa550dabe1c",
    "input": "## Claim\nHere is a claim: acoustic supervision (27.7%) and multi-task learning (26.1%) show lower WER than minimizing DCE (31.1%) and FSEGAN (29.1%)). Does the following context support or refute the claim?\n\n## Table\nPaper title: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition\nTable caption: TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set\nMethod | WER (%) | DCE\nNo enhancement | 38.4 | 0.958\nWiener filter | 41.0 | 0.775\nMinimizing DCE | 31.1 | [BOLD] 0.392\nFSEGAN | 29.1 | 0.421\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=0) | 27.7 | 0.476\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 26.1 | 0.462\nClean speech | 9.3 | 0.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5655d55f-686a-4173-ae58-87964c81a390",
    "input": "## Claim\nHere is a claim: The difference is particularly striking on the essay level where the parsers often completely fail to learn, that is, their performance scores are close to 0%. Does the following context support or refute the claim?\n\n## Table\nPaper title: Neural End-to-End Learning for Computational Argumentation Mining\nTable caption: Table 4: C-F1 (100%) in % for the two indicated systems; essay vs.\u00a0paragraph level. Note that the mean performances are lower than the majority performances over the runs given in Table 2.\n[EMPTY] | STagBLCC | LSTM-Parser\nEssay | 60.62\u00b13.54 | 9.40\u00b113.57\nParagraph | 64.74\u00b11.97 | 56.24\u00b12.87\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "201a8927-705e-4154-837e-2527d42e84a1",
    "input": "## Claim\nHere is a claim: However, our proposed method does not outperform the original GloVe embeddings. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "7fa6751d-e5c9-44c7-9775-48f316b11f2b",
    "input": "## Claim\nHere is a claim: [CONTINUE] Regarding the probing tasks, we observe that CMOW embeddings better encode the linguistic prop [CONTINUE] erties of sentences than CBOW. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c92525f9-3875-4445-bc8e-5e3cac3e8e9e",
    "input": "## Claim\nHere is a claim: On the other hand, choosing the best hypernym did not work very well for DocSub which obtained the lowest precision for the Portuguese corpora. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1038 | 0.0170 | 0.0490 | 0.0641 | 0.0641 | 0.0613 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1282 | 0.0291 | 0.0410 | 0.0270 | 0.0270 | 0.1154 | 0.0661\nP | PT | Europarl | 0.6185 | 0.3744 | 0.4144 | 0.4394 | 0.4394 | [BOLD] 0.7553 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.6308 | 0.4124 | 0.4404 | 0.4515 | 0.4945 | [BOLD] 0.8609 | 0.5295\nR | EN | Europarl | [BOLD] 0.0021 | 0.0004 | 0.0011 | 0.0014 | 0.0014 | 0.0013 | 0.0017\nR | EN | Ted Talks | 0.0011 | 0.0008 | 0.0011 | 0.0008 | 0.0008 | [BOLD] 0.0030 | 0.0018\nR | PT | Europarl | 0.0012 | 0.0008 | 0.0009 | 0.0010 | 0.0010 | [BOLD] 0.0016 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0003 | 0.0009 | 0.0009 | 0.0010 | 0.0010 | [BOLD] 0.0017 | 0.0011\nF | EN | Europarl | [BOLD] 0.0041 | 0.0007 | 0.0021 | 0.0027 | 0.0027 | 0.0026 | 0.0033\nF | EN | Ted Talks | 0.0022 | 0.0016 | 0.0022 | 0.0015 | 0.0015 | [BOLD] 0.0058 | 0.0036\nF | PT | Europarl | 0.0024 | 0.0016 | 0.0018 | 0.0019 | 0.0019 | [BOLD] 0.0031 | 0.0023\n[EMPTY] | PT | Ted Talks | 0.0005 | 0.0018 | 0.0018 | 0.0020 | 0.0021 | [BOLD] 0.0034 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d5553a4a-b710-4865-adb7-3ae9adb2f279",
    "input": "## Claim\nHere is a claim: In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism. Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 4: Experiment 2, t= \u201cb*tch\u201d\nDataset | Class | \u02c6 [ITALIC] piblack | \u02c6 [ITALIC] piwhite | [ITALIC] t | [ITALIC] p | \u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite\n[ITALIC] Waseem and Hovy | Racism | 0.010 | 0.010 | -0.632 | [EMPTY] | 0.978\n[EMPTY] | Sexism | 0.963 | 0.944 | 20.064 | *** | 1.020\n[ITALIC] Waseem | Racism | 0.011 | 0.011 | -1.254 | [EMPTY] | 0.955\n[EMPTY] | Sexism | 0.349 | 0.290 | 28.803 | *** | 1.203\n[EMPTY] | Racism and sexism | 0.012 | 0.012 | -0.162 | [EMPTY] | 0.995\n[ITALIC] Davidson et al. | Hate | 0.017 | 0.015 | 4.698 | *** | 1.152\n[EMPTY] | Offensive | 0.988 | 0.991 | -6.289 | *** | 0.997\n[ITALIC] Golbeck et al. | Harassment | 0.099 | 0.091 | 6.273 | *** | 1.091\n[ITALIC] Founta et al. | Hate | 0.074 | 0.027 | 46.054 | *** | 2.728\n[EMPTY] | Abusive | 0.925 | 0.968 | -41.396 | *** | 0.956\n[EMPTY] | Spam | 0.010 | 0.010 | 0.000 | [EMPTY] | 1.000\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "54b24d05-5f41-461c-a0d5-53d4d9bb2b16",
    "input": "## Claim\nHere is a claim: As we can observe, it seems that clustering semantically related terms does not necessarily increase the precision (at least for the top 1,000 terms in the English corpora used in this experiment) as expected. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "368b72b0-fb17-4ab7-b573-ba6f59ddc2a4",
    "input": "## Claim\nHere is a claim: G2S models also generate sentences that contradict the reference sentences less. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.\n<bold>Model</bold> | REF \u21d2 GEN <bold>ENT</bold> | REF \u21d2 GEN <bold>CON</bold> | REF \u21d2 GEN <bold>NEU</bold>\nS2S | 38.45 | 11.17 | 50.38\nG2S-GIN | 49.78 | 9.80 | 40.42\nG2S-GAT | 49.48 | 8.09 | 42.43\nG2S-GGNN | 51.32 | 8.82 | 39.86\n[EMPTY] | GEN \u21d2 REF | GEN \u21d2 REF | GEN \u21d2 REF\n<bold>Model</bold> | <bold>ENT</bold> | <bold>CON</bold> | <bold>NEU</bold>\nS2S | 73.79 | 12.75 | 13.46\nG2S-GIN | 76.27 | 10.65 | 13.08\nG2S-GAT | 77.54 | 8.54 | 13.92\nG2S-GGNN | 77.64 | 9.64 | 12.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "1a2b1366-c934-4fb3-96f8-d469d5994e36",
    "input": "## Claim\nHere is a claim: Table 8: The contribution of each unsupervised learning for detecting negation triggers. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "3b8e8d3d-432a-4be3-9875-accd92112337",
    "input": "## Claim\nHere is a claim: Dual2seq is consistently better than the other systems under all three metrics, [CONTINUE] Dual2seq is better than both OpenNMT-tf and Transformer-tf . Does the following context support or refute the claim?\n\n## Table\nPaper title: Semantic Neural Machine Translation using AMR\nTable caption: Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.\nSystem | NC-v11 BLEU | NC-v11 TER\u2193 | NC-v11 Meteor | Full BLEU | Full TER\u2193 | Full Meteor\nOpenNMT-tf | 15.1 | 0.6902 | 0.3040 | 24.3 | 0.5567 | 0.4225\nTransformer-tf | 17.1 | 0.6647 | 0.3578 | 25.1 | 0.5537 | 0.4344\nSeq2seq | 16.0 | 0.6695 | 0.3379 | 23.7 | 0.5590 | 0.4258\nDual2seq-LinAMR | 17.3 | 0.6530 | 0.3612 | 24.0 | 0.5643 | 0.4246\nDuel2seq-SRL | 17.2 | 0.6591 | 0.3644 | 23.8 | 0.5626 | 0.4223\nDual2seq-Dep | 17.8 | 0.6516 | 0.3673 | 25.0 | 0.5538 | 0.4328\nDual2seq | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840 | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5854ce90-8b2b-4494-8ab0-d273e8e50cf7",
    "input": "## Claim\nHere is a claim: We empirically found that self-attention was not the most efficient in the 3rd stage. Does the following context support or refute the claim?\n\n## Table\nPaper title: Modulated Self-attention Convolutional Network for VQA\nTable caption: Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\n[BOLD] ResNet-34 | [BOLD] Eval set % | [BOLD] #param\nBaseline (No SA)Anderson et al. ( 2018 ) | 55.00 | 0M\nSA (S: 1,2,3 - B: 1) | 55.11 | } 0.107M\nSA (S: 1,2,3 - B: 2) | 55.17 | } 0.107M\n[BOLD] SA (S: 1,2,3 - B: 3) | [BOLD] 55.27 | } 0.107M\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "fa3f471e-4365-44ac-bbe9-91cb1d44dee1",
    "input": "## Claim\nHere is a claim: In contrast, DAN does not always mask out punctuation and determiners using words indicative of the class label, as evidenced by the example sentence in the table. Does the following context support or refute the claim?\n\n## Table\nPaper title: What do Deep Networks Like to Read?\nTable caption: Table 1: Example sentences of the different classifiers compared to the original on SST-2. We report further examples in the Appendix. <u> use for <UNK>.\nOrig | <u> turns in a <u> screenplay that <u> at the edges ; it \u2019s so clever you want to hate it .\nDAN | <u> turns in a <u> screenplay screenplay screenplay of <u> edges edges edges shapes so clever easy want hate hate hate hate hate hate hate hate hate hate\nCNN | she turns on a on ( ( in in the the the edges \u2019s so clever \u201c want to hate it \u201d\nRNN | <u> turns in a <u> screenplay was <u> <u> <u> edges edges edges curves <u> clever clever you want hate hate it .\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "759631a8-3754-40cb-8f1e-d00481c7037b",
    "input": "## Claim\nHere is a claim: Our joint model does not improve upon the strong lemma baseline by 3.8 points in CoNLL F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n[BOLD] Model | R | MUC P | [ITALIC] F1 | R | B3 P | [ITALIC] F1 | R | CEAF- [ITALIC] e P | [ITALIC] F1 | CoNLL  [ITALIC] F1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | [BOLD] 71.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9b3caf85-932d-41fa-83f8-e5b72f7ffc93",
    "input": "## Claim\nHere is a claim: [CONTINUE] When trained on the NC-v11 subset, the gap between Seq2seq and Dual2seq under Meteor (around 5 points) is greater than that under BLEU (around 3 points). Does the following context support or refute the claim?\n\n## Table\nPaper title: Semantic Neural Machine Translation using AMR\nTable caption: Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.\nSystem | NC-v11 BLEU | NC-v11 TER\u2193 | NC-v11 Meteor | Full BLEU | Full TER\u2193 | Full Meteor\nOpenNMT-tf | 15.1 | 0.6902 | 0.3040 | 24.3 | 0.5567 | 0.4225\nTransformer-tf | 17.1 | 0.6647 | 0.3578 | 25.1 | 0.5537 | 0.4344\nSeq2seq | 16.0 | 0.6695 | 0.3379 | 23.7 | 0.5590 | 0.4258\nDual2seq-LinAMR | 17.3 | 0.6530 | 0.3612 | 24.0 | 0.5643 | 0.4246\nDuel2seq-SRL | 17.2 | 0.6591 | 0.3644 | 23.8 | 0.5626 | 0.4223\nDual2seq-Dep | 17.8 | 0.6516 | 0.3673 | 25.0 | 0.5538 | 0.4328\nDual2seq | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840 | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b418dccb-cdbf-4f19-8f22-9cf2beec69ff",
    "input": "## Claim\nHere is a claim: The proposed method achieves the competitive accuracies using single vector compared to several vector models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "41e926f4-dc54-48d5-a985-eb56c45a2131",
    "input": "## Claim\nHere is a claim: The resulting cross-dataset improvements on the SNLI and Glockner datasets are not larger than those on the SICK dataset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\n[EMPTY] | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK\nMQAN | 72.30 | 60.91 | 41.82 | 53.95\n+ coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold>\nESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37\n+ coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "0a65243e-547d-4f38-abe3-ed90345ed44d",
    "input": "## Claim\nHere is a claim: In the en-de News/TED task (Table 4), all fine-tuning schemes give similar improvements on TED. Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 4: Test BLEU for en-de adaptive training, with sequential adaptation to a third task. EWC-tuned models give the best performance on each domain.\n[EMPTY] | [BOLD] Training scheme | [BOLD] News | [BOLD] TED | [BOLD] IT\n1 | News | 37.8 | 25.3 | 35.3\n2 | TED | 23.7 | 24.1 | 14.4\n3 | IT | 1.6 | 1.8 | 39.6\n4 | News and TED | 38.2 | 25.5 | 35.4\n5 | 1 then TED, No-reg | 30.6 | [BOLD] 27.0 | 22.1\n6 | 1 then TED, L2 | 37.9 | 26.7 | 31.8\n7 | 1 then TED, EWC | [BOLD] 38.3 | [BOLD] 27.0 | 33.1\n8 | 5 then IT, No-reg | 8.0 | 6.9 | 56.3\n9 | 6 then IT, L2 | 32.3 | 22.6 | 56.9\n10 | 7 then IT, EWC | 35.8 | 24.6 | [BOLD] 57.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ec93a8d7-8518-41cb-be5e-f605bd53b660",
    "input": "## Claim\nHere is a claim: G2S-GAT has a better performance in handling graphs with node out-degrees higher than 9. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1cb952cc-2280-4358-85a8-e2afbc341742",
    "input": "## Claim\nHere is a claim: we can see that our classifier does not quite achieve the results of the BiLSTM+scope, but is more robust in extracting the expression. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "535d1def-2141-41d5-8a69-da4175cacf77",
    "input": "## Claim\nHere is a claim: The Wiener filtering method shows lower DCE, but higher WER than no enhancement. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition\nTable caption: TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set\nMethod | WER (%) | DCE\nNo enhancement | 38.4 | 0.958\nWiener filter | 41.0 | 0.775\nMinimizing DCE | 31.1 | [BOLD] 0.392\nFSEGAN | 29.1 | 0.421\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=0) | 27.7 | 0.476\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 26.1 | 0.462\nClean speech | 9.3 | 0.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0da5aa22-c92f-4d85-bc7c-5164be31f090",
    "input": "## Claim\nHere is a claim: we see that superficial cues for COPA are also significant for SB-COPA, showing that SB-COPA mirrors our human intuitions at least for this phenomenon. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 7: Sensitivity of BERT-large to superficial cues identified in \u00a72 (unit: 10\u22122). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.\nCue | [ITALIC] SCOPA | [ITALIC] SB_COPA | Diff. | Prod.\nwoman | 7.98 | 4.84 | -3.14 | 0.25\nmother | 5.16 | 3.95 | -1.21 | 0.75\nwent | 6.00 | 5.15 | -0.85 | 0.73\ndown | 5.52 | 4.93 | -0.58 | 0.71\ninto | 4.07 | 3.51 | -0.56 | 0.40\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c4fe7068-9584-4aac-900d-e743f0919833",
    "input": "## Claim\nHere is a claim: This is evident from the insignificant drop in ARI score from OD to OD (no polarity shifters) since the only change in those variants is of sentiment polarity shifters. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.\n[EMPTY] | Difference Function | Seanad Abolition | Video Games | Pornography\nOD-parse | Absolute | 0.01 | -0.01 | 0.07\nOD-parse | JS div. | 0.01 | -0.01 | -0.01\nOD-parse | EMD | 0.07 | 0.01 | -0.01\nOD | Absolute | [BOLD] 0.54 | [BOLD] 0.56 | [BOLD] 0.41\nOD | JS div. | 0.07 | -0.01 | -0.02\nOD | EMD | 0.26 | -0.01 | 0.01\nOD (no polarity shifters) | Absolute | 0.23 | 0.08 | 0.04\nOD (no polarity shifters) | JS div. | 0.09 | -0.01 | -0.02\nOD (no polarity shifters) | EMD | 0.10 | 0.01 | -0.01\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "c2032a31-8e78-411f-aa54-87bf791b98b3",
    "input": "## Claim\nHere is a claim: Results presented in Table 7 show that the domain adaptation approach further boosts F1 by 1 point to 79 (t-test, p<0.5) and ROC AUC by 0.012. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 7: Complaint prediction results using the original data set and distantly supervised data. All models are based on logistic regression with bag-of-word and Part-of-Speech tag features.\n[BOLD] Model | [BOLD] Acc | [BOLD] F1 | [BOLD] AUC\nMost Frequent Class | 64.2 | 39.1 | 0.500\nLR-All Features \u2013 Original Data | 80.5 | 78.0 | 0.873\nDist. Supervision + Pooling | 77.2 | 75.7 | 0.853\nDist. Supervision + EasyAdapt | [BOLD] 81.2 | [BOLD] 79.0 | [BOLD] 0.885\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "291119c8-4554-4704-a71f-715bfb6ea711",
    "input": "## Claim\nHere is a claim: In addition, the noise-aware model is more stable and therefore requires fewer iterations to converge. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Vector-spaces with Noisy Supervised Lexicons\nTable caption: Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En\u2192De, En\u2192Fi and En\u2192Es improvements are significant at p<0.05 according to ANOVA on the different runs.\nMethod | En\u2192It best | En\u2192It avg | En\u2192It iters | En\u2192De best | En\u2192De avg | En\u2192De iters | En\u2192Fi best | En\u2192Fi avg | En\u2192Fi iters | En\u2192Es best | En\u2192Es avg | En\u2192Es iters\nArtetxe et\u00a0al., 2018b | [BOLD] 48.53 | 48.13 | 573 | 48.47 | 48.19 | 773 | 33.50 | 32.63 | 988 | 37.60 | 37.33 | 808\nNoise-aware Alignment | [BOLD] 48.53 | [BOLD] 48.20 | 471 | [BOLD] 49.67 | [BOLD] 48.89 | 568 | [BOLD] 33.98 | [BOLD] 33.68 | 502 | [BOLD] 38.40 | [BOLD] 37.79 | 551\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "91e9f947-9e27-4fe2-9913-b45c570f1d05",
    "input": "## Claim\nHere is a claim: This shows that more attention heads, thereby attending to multiple different contexts at once, does not necessarily lead to state-of-the-art results. Does the following context support or refute the claim?\n\n## Table\nPaper title: Localization of Fake News Detection via Multitask Transfer Learning\nTable caption: Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. \u201cEffect\u201d refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.\n# of Heads | Accuracy | Val. Loss | Effect\n1 | 89.44% | 0.2811 | -6.84%\n2 | 91.20% | 0.2692 | -5.08%\n4 | 93.85% | 0.2481 | -2.43%\n8 | 96.02% | 0.2257 | -0.26%\n10 | 96.28% | 0.2197 | [EMPTY]\n16 | 96.32% | 0.2190 | +0.04\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "24a53156-9a34-46d3-8bd8-acd2a28bdb82",
    "input": "## Claim\nHere is a claim: Our model (OURS) does not obtain substantial gains in accuracy over the baselines across all three target aspects. Does the following context support or refute the claim?\n\n## Table\nPaper title: Deriving Machine Attention from Human Rationales\nTable caption: Table 3: Accuracy of transferring between aspects. Models with \u2020 use labeled data from source aspects. Models with \u2021 use human rationales on the target aspect.\nSource | Target | Svm | Ra-Svm\u2021 | Ra-Cnn\u2021 | Trans\u2020 | Ra-Trans\u2021\u2020 | Ours\u2021\u2020 | Oracle\u2020\nBeer aroma+palate | Beer look | 74.41 | 74.83 | 74.94 | 72.75 | 76.41 | [BOLD] 79.53 | 80.29\nBeer look+palate | Beer aroma | 68.57 | 69.23 | 67.55 | 69.92 | 76.45 | [BOLD] 77.94 | 78.11\nBeer look+aroma | Beer palate | 63.88 | 67.82 | 65.72 | 74.66 | 73.40 | [BOLD] 75.24 | 75.50\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2a8dabe5-b9db-45df-a2d3-8f7ad54d75d2",
    "input": "## Claim\nHere is a claim: In comparison, GDPL is still comparable with ACER and PPO, obtains a better match rate, and even achieves higher task success. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "893265ca-c355-4c56-914b-a7e0fc559077",
    "input": "## Claim\nHere is a claim: However, BERT's improvements over previous work can be almost entirely attributed to high accuracy on the Easy subset: on this subset, finetuned BERT-large improves 8.6 percent over the model by (Sasaki et al., 2017) (83.9% vs. 75.3%), but on the Hard subset, the improvement is only 2.9 percent (71.9% vs. 69.0%). Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b565802b-953b-444b-8f62-8ec3c2ab776f",
    "input": "## Claim\nHere is a claim: This reflects the dialog efficiency of all methods but ACER decreases with the time extension, which is the opposite with human\u2019s preference. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "81206d38-8fec-4a92-95c9-b53a0785ab95",
    "input": "## Claim\nHere is a claim: We observe that our model exhibits the best performances. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 2: Precisions on the Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nRank+ExATT | 0.584 | 0.535 | 0.487 | 0.392\nPCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204\nPCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396\nOur Model | 0.650 | 0.519 | 0.422 | [BOLD] 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "202d9083-e874-49ff-8926-97f4f3f5bc91",
    "input": "## Claim\nHere is a claim: G2S models generate sentences that contradict the reference sentences more. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.\n<bold>Model</bold> | REF \u21d2 GEN <bold>ENT</bold> | REF \u21d2 GEN <bold>CON</bold> | REF \u21d2 GEN <bold>NEU</bold>\nS2S | 38.45 | 11.17 | 50.38\nG2S-GIN | 49.78 | 9.80 | 40.42\nG2S-GAT | 49.48 | 8.09 | 42.43\nG2S-GGNN | 51.32 | 8.82 | 39.86\n[EMPTY] | GEN \u21d2 REF | GEN \u21d2 REF | GEN \u21d2 REF\n<bold>Model</bold> | <bold>ENT</bold> | <bold>CON</bold> | <bold>NEU</bold>\nS2S | 73.79 | 12.75 | 13.46\nG2S-GIN | 76.27 | 10.65 | 13.08\nG2S-GAT | 77.54 | 8.54 | 13.92\nG2S-GGNN | 77.64 | 9.64 | 12.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d48650ee-1b73-4a22-af9d-5d59d1d0d522",
    "input": "## Claim\nHere is a claim: the overall results suggest that the combination of our negative opinion words with external sentiment lexicon outperform other methods Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 8: Sentiment classification evaluation, using different classifiers on the test set.\nClassifier | Positive Sentiment Precision | Positive Sentiment Recall | Positive Sentiment Fscore\nSVM-w/o neg. | 0.57 | 0.72 | 0.64\nSVM-Punct. neg. | 0.58 | 0.70 | 0.63\nSVM-our-neg. | 0.58 | 0.73 | 0.65\nCNN | 0.63 | 0.83 | 0.72\nCNN-LSTM | 0.71 | 0.72 | 0.72\nCNN-LSTM-Our-neg-Ant | [BOLD] 0.78 | [BOLD] 0.77 | [BOLD] 0.78\n[EMPTY] | Negative Sentiment | Negative Sentiment | Negative Sentiment\n[EMPTY] | Precision | Recall | Fscore\nSVM-w/o neg. | 0.78 | 0.86 | 0.82\nSVM-Punct. neg. | 0.78 | 0.87 | 0.83\nSVM-Our neg. | 0.80 | 0.87 | 0.83\nCNN | 0.88 | 0.72 | 0.79\nCNN-LSTM. | 0.83 | 0.83 | 0.83\nCNN-LSTM-our-neg-Ant | [BOLD] 0.87 | [BOLD] 0.87 | [BOLD] 0.87\n[EMPTY] | Train | [EMPTY] | Test\nPositive tweets | 5121 | [EMPTY] | 1320\nNegative tweets | 9094 | [EMPTY] | 2244\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8d5740c5-abf5-47c2-beb2-4150cb29d77f",
    "input": "## Claim\nHere is a claim: [CONTINUE] For Marian amun, the effect is negligible as we can see in Table 3. Does the following context support or refute the claim?\n\n## Table\nPaper title: The MeMAD Submission to the WMT18 Multimodal Translation Task\nTable caption: Table 3: Using automatically translated image captions and domain labels (BLEU% scores). A is short for Amun, T for Transformer.\n[EMPTY] | en-fr | flickr16 | flickr17 | mscoco17\nA | subs1M [ITALIC]  [ITALIC] H+MS-COCO | 66.3 | 60.5 | 52.1\nA | +domain-tuned | 66.8 | 60.6 | 52.0\nA | +labels | [BOLD] 67.2 | 60.4 | 51.7\nT | subs1M [ITALIC]  [ITALIC] LM+MS-COCO | 66.9 | 60.3 | [BOLD] 52.8\nT | +labels | [BOLD] 67.2 | [BOLD] 60.9 | 52.7\n[EMPTY] | en-de | flickr16 | flickr17 | mscoco17\nA | subs1M [ITALIC]  [ITALIC] H+MS-COCO | 43.1 | 39.0 | 35.1\nA | +domain-tuned | 43.9 | 39.4 | 35.8\nA | +labels | 43.2 | 39.3 | 34.3\nT | subs1M [ITALIC]  [ITALIC] LM+MS-COCO | [BOLD] 44.4 | 39.4 | 35.0\nT | +labels | 44.1 | [BOLD] 39.8 | [BOLD] 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f2ea1a3c-fbc4-448e-aeab-30d25c8c4969",
    "input": "## Claim\nHere is a claim: However, our summary is often significantly longer than the actual reference summary. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "cad7b2c8-2a39-4cb7-84fc-68280ec753d8",
    "input": "## Claim\nHere is a claim: For example, on AMR17, the ensemble model of Seq2SeqB is 1 BLEU point higher than the single DCGCN model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.\n[BOLD] Model | [BOLD] T | #P | B | C\nSeq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1\nGGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4\nSeq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5\nGGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5\nDCGCN (ours) | S | [BOLD] 19.1M | 27.9 | 57.3\nDCGCN (ours) | E | 92.5M | [BOLD] 30.4 | [BOLD] 59.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5a6b1a51-03be-40dd-9c08-02b7829a9750",
    "input": "## Claim\nHere is a claim: Accordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally weaker: [CONTINUE] as in Eq. Does the following context support or refute the claim?\n\n## Table\nPaper title: Neural End-to-End Learning for Computational Argumentation Mining\nTable caption: Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by \u201c:\u201d. Layers from which tasks feed are indicated by respective numbers.\n[EMPTY] | C-F1 100% | C-F1 50% | R-F1 100% | R-F1 50% | F1 100% | F1 50%\nY-3 | 49.59 | 65.37 | 26.28 | 37.00 | 34.35 | 47.25\nY-3:Y<italic>C</italic>-1 | 54.71 | 66.84 | 28.44 | 37.35 | 37.40 | 47.92\nY-3:Y<italic>R</italic>-1 | 51.32 | 66.49 | 26.92 | 37.18 | 35.31 | 47.69\nY-3:Y<italic>C</italic>-3 | <bold>54.58</bold> | 67.66 | <bold>30.22</bold> | <bold>40.30</bold> | <bold>38.90</bold> | <bold>50.51</bold>\nY-3:Y<italic>R</italic>-3 | 53.31 | 66.71 | 26.65 | 35.86 | 35.53 | 46.64\nY-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2 | 52.95 | <bold>67.84</bold> | 27.90 | 39.71 | 36.54 | 50.09\nY-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3 | 54.55 | 67.60 | 28.30 | 38.26 | 37.26 | 48.86\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a8563f9b-8e6c-4b68-b8e5-7d8c75675538",
    "input": "## Claim\nHere is a claim: Again, one possible explanation is that cleaning the missing slots provided more complex training examples. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Original | TGen\u2212 | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94\nOriginal | [BOLD] Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27\nOriginal | [BOLD] Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80\nOriginal | [BOLD] Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen\u2212 | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37\nCleaned missing | [BOLD] Original | TGen\u2212 | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61\nCleaned missing | [BOLD] Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53\nCleaned missing | [BOLD] Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen\u2212 | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "20f98547-11fd-48bd-a892-284b3df13a83",
    "input": "## Claim\nHere is a claim: The Transformer performs best in terms of R-1 while Hi-MAP does not outperform it on R-2 and R-SU. Does the following context support or refute the claim?\n\n## Table\nPaper title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\nTable caption: Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.\n[BOLD] Method | [BOLD] R-1 | [BOLD] R-2 | [BOLD] R-SU\nFirst-1 | 26.83 | 7.25 | 6.46\nFirst-2 | 35.99 | 10.17 | 12.06\nFirst-3 | 39.41 | 11.77 | 14.51\nLexRank Erkan and Radev ( 2004 ) | 38.27 | 12.70 | 13.20\nTextRank Mihalcea and Tarau ( 2004 ) | 38.44 | 13.10 | 13.50\nMMR Carbonell and Goldstein ( 1998 ) | 38.77 | 11.98 | 12.91\nPG-Original Lebanoff et\u00a0al. ( 2018 ) | 41.85 | 12.91 | 16.46\nPG-MMR Lebanoff et\u00a0al. ( 2018 ) | 40.55 | 12.36 | 15.87\nPG-BRNN Gehrmann et\u00a0al. ( 2018 ) | 42.80 | 14.19 | 16.75\nCopyTransformer Gehrmann et\u00a0al. ( 2018 ) | [BOLD] 43.57 | 14.03 | 17.37\nHi-MAP (Our Model) | 43.47 | [BOLD] 14.89 | [BOLD] 17.41\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e76cbbbb-e973-4cdf-9ab0-a1a39fec7cfc",
    "input": "## Claim\nHere is a claim: [CONTINUE] Our model achieves state-of-the-art results, outperforming previous models by 9.9 CoNLL F1 points on events. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\n<bold>Baselines</bold> | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nCluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5\nCV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73\nKCP Kenyon-Dean et\u00a0al. (<ref id='bib-bib14'>2018</ref>) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69\nCluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6\n<bold>Model Variants</bold> | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nDisjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5\nJoint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | <bold>79.5</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "90056cab-a079-4e5e-8c6c-cd8f2605b030",
    "input": "## Claim\nHere is a claim: the performance of the proposed method, which takes into account all kinds of semantic orientations and measures word relationships on the basis of \"receptivity\", does not show much difference. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "319f8071-f254-4ea5-9d12-2d01758e5686",
    "input": "## Claim\nHere is a claim: However, best predictive performance is obtained using bag-of-word features, reaching an F1 of up to 77.5 and AUC of 0.866. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.\n[BOLD] Model | [BOLD] Acc | [BOLD] F1 | [BOLD] AUC\nMost Frequent Class | 64.2 | 39.1 | 0.500\nLogistic Regression | [EMPTY] | [EMPTY] | [EMPTY]\nSentiment \u2013 MPQA | 64.2 | 39.1 | 0.499\nSentiment \u2013 NRC | 63.9 | 42.2 | 0.599\nSentiment \u2013 V&B | 68.9 | 60.0 | 0.696\nSentiment \u2013 VADER | 66.0 | 54.2 | 0.654\nSentiment \u2013 Stanford | 68.0 | 55.6 | 0.696\nComplaint Specific (all) | 65.7 | 55.2 | 0.634\nRequest | 64.2 | 39.1 | 0.583\nIntensifiers | 64.5 | 47.3 | 0.639\nDowngraders | 65.4 | 49.8 | 0.615\nTemporal References | 64.2 | 43.7 | 0.535\nPronoun Types | 64.1 | 39.1 | 0.545\nPOS Bigrams | 72.2 | 66.8 | 0.756\nLIWC | 71.6 | 65.8 | 0.784\nWord2Vec Clusters | 67.7 | 58.3 | 0.738\nBag-of-Words | 79.8 | 77.5 | 0.866\nAll Features | [BOLD] 80.5 | [BOLD] 78.0 | [BOLD] 0.873\nNeural Networks | [EMPTY] | [EMPTY] | [EMPTY]\nMLP | 78.3 | 76.2 | 0.845\nLSTM | 80.2 | 77.0 | 0.864\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "99e5b5e6-f575-4873-b1b5-cf1ecc803e78",
    "input": "## Claim\nHere is a claim: We show the precision numbers for some particular recalls as well as the AUC in Table 2, where PCNN+ATT (1) refers to train sentences with two entities and one relation label, PCNN+ATT (m) refers to train sentences with four entities7 and two relation labels. However, our model does not outperform the other models, as evidenced by the lower AUC score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 2: Precisions on the Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nRank+ExATT | 0.584 | 0.535 | 0.487 | 0.392\nPCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204\nPCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396\nOur Model | 0.650 | 0.519 | 0.422 | [BOLD] 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "78a80fec-1eb9-49ce-9e20-ffc00a53a2b1",
    "input": "## Claim\nHere is a claim: [CONTINUE] However, CMOW does not in general supersede CBOW embeddings. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nMethod | SUBJ | CR | MR | MPQA | MRPC | TREC | SICK-E | SST2 | SST5 | STS-B | SICK-R\nCBOW/784 | 90.0 | [BOLD] 79.2 | [BOLD] 74.0 | 87.1 | 71.6 | 85.6 | 78.9 | 78.5 | 42.1 | 61.0 | [BOLD] 78.1\nCMOW/784 | 87.5 | 73.4 | 70.6 | [BOLD] 87.3 | 69.6 | [BOLD] 88.0 | 77.2 | 74.7 | 37.9 | 56.5 | 76.2\nHybrid | [BOLD] 90.2 | 78.7 | 73.7 | [BOLD] 87.3 | [BOLD] 72.7 | 87.6 | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8\ncmp. CBOW | +0.2% | -0.6% | -0.4% | +0.2% | +1.5% | +2.3% | +0.6% | +1.4% | +2.9% | +3.9% | -0.4%\ncmp. CMOW | +3.1% | +7.2% | +4.4% | +0% | +4.5% | -0.5% | +2.9% | +6.7% | +14.3 | +12.2% | +2.1%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "6a39e1b2-806a-4c81-953d-0a1db9b7d962",
    "input": "## Claim\nHere is a claim: All fluency problems we found were very slight, but added and wrong-valued slots were still found, so missed slots are not the only problem. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).\n[BOLD] Training data | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] Disfl\nOriginal | 0 | 22 | 0 | 14\nCleaned added | 0 | 23 | 0 | 14\nCleaned missing | 0 | 1 | 0 | 2\nCleaned | 0 | 0 | 0 | 5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "66c277ce-a633-4441-911f-9346c0b73c74",
    "input": "## Claim\nHere is a claim: For the Japanese captions, AME reaches 6.25% and 3.66% better results on average compared to monolingual model in symmetric and asymmetric modes, respectively. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task\nTable caption: Table 4: Image-caption ranking results for Japanese (MS-COCO)\n[EMPTY] | Image to Text R@1 | Image to Text R@5 | Image to Text R@10 | Image to Text Mr | Text to Image R@1 | Text to Image R@5 | Text to Image R@10 | Text to Image Mr | Alignment\n[BOLD] symmetric | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nMono | 42.7 | 77.7 | 88.5 | 2 | 33.1 | 69.8 | 84.3 | 3 | -\nFME | 40.7 | 77.7 | 88.3 | 2 | 30.0 | 68.9 | 83.1 | 3 | 92.70%\nAME | [BOLD] 50.2 | [BOLD] 85.6 | [BOLD] 93.1 | [BOLD] 1 | [BOLD] 40.2 | [BOLD] 76.7 | [BOLD] 87.8 | [BOLD] 2 | 82.54%\n[BOLD] asymmetric | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nMono | 49.9 | 83.4 | 93.7 | 2 | 39.7 | 76.5 | 88.3 | [BOLD] 2 | -\nFME | 48.8 | 81.9 | 91.9 | 2 | 37.0 | 74.8 | 87.0 | [BOLD] 2 | 92.70%\nAME | [BOLD] 55.5 | [BOLD] 87.9 | [BOLD] 95.2 | [BOLD] 1 | [BOLD] 44.9 | [BOLD] 80.7 | [BOLD] 89.3 | [BOLD] 2 | 84.99%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f480c688-06c4-459b-affc-8737fc822e2b",
    "input": "## Claim\nHere is a claim: This table refutes the effectiveness of our approach. Does the following context support or refute the claim?\n\n## Table\nPaper title: Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation\nTable caption: Table 4: Results of Self-Play Evaluation.\nSystem | TGPC Succ. (%) | TGPC #Turns | CWC Succ. (%) | CWC #Turns\nRetrieval\u00a0 | 7.16 | 4.17 | 0 | -\nRetrieval-Stgy\u00a0 | 47.80 | 6.7 | 44.6 | 7.42\nPMI\u00a0 | 35.36 | 6.38 | 47.4 | 5.29\nNeural\u00a0 | 54.76 | 4.73 | 47.6 | 5.16\nKernel\u00a0 | 62.56 | 4.65 | 53.2 | 4.08\nDKRN (ours) | [BOLD] 89.0 | 5.02 | [BOLD] 84.4 | 4.20\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "0cf1197a-668e-4bba-a1b1-ccc35b2fd72f",
    "input": "## Claim\nHere is a claim: [CONTINUE] As these models use object detectors pretrained on Pascal-VOC , they have somewhat higher performance on classes that are common to both Flickr30k and Pascal-VOC (\"animals\", \"people\" and \"vehicles\"), however, the ZSGNet model with Res50 (cls) performs better than the other models on all categories. Does the following context support or refute the claim?\n\n## Table\nPaper title: Zero-Shot Grounding of Objects from Natural Language Queries\nTable caption: Table 3: Category-wise performance with the default split of Flickr30k Entities.\nMethod | Overall | people | clothing | bodyparts | animals | vehicles | instruments | scene | other\nQRC - VGG(det) | 60.21 | 75.08 | 55.9 | 20.27 | 73.36 | 68.95 | 45.68 | 65.27 | 38.8\nCITE - VGG(det) | 61.89 | [BOLD] 75.95 | 58.50 | 30.78 | [BOLD] 77.03 | [BOLD] 79.25 | 48.15 | 58.78 | 43.24\nZSGNet - VGG (cls) | 60.12 | 72.52 | 60.57 | 38.51 | 63.61 | 64.47 | 49.59 | 64.66 | 41.09\nZSGNet - Res50 (cls) | [BOLD] 63.39 | 73.87 | [BOLD] 66.18 | [BOLD] 45.27 | 73.79 | 71.38 | [BOLD] 58.54 | [BOLD] 66.49 | [BOLD] 45.53\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "681f0a81-e820-45f2-9d6b-64137a6a6c7c",
    "input": "## Claim\nHere is a claim: We found that rephrase disfluencies that contain content words are harder for the model to detect, compared to rephrases with function words only, and error increases for longer disfluencies. Does the following context support or refute the claim?\n\n## Table\nPaper title: Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection\nTable caption: Table 3: Relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair (content-content), either the reparandum or repair (content-function) or in neither. Percentages in parentheses show the fraction of tokens belong to each category.\n[BOLD] Type | [BOLD] Reparandum Length  [BOLD] 1-2 | [BOLD] Reparandum Length  [BOLD] 3-5\ncontent-content | 0.61 (30%) | 0.58 (52%)\ncontent-function | 0.77 (20%) | 0.66 (17%)\nfunction-function | 0.83 (50%) | 0.80 (32%)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "249235b7-0bb2-431e-b55c-3adcbc63a9d2",
    "input": "## Claim\nHere is a claim: We can see from Table 6 that empirically adding logits from two models after classifiers performs the best. Does the following context support or refute the claim?\n\n## Table\nPaper title: Filling Conversation Ellipsis for Better Social Dialog Understanding\nTable caption: Table 6: Dialog act prediction performance using different selection methods.\n[BOLD] Selection Method | [BOLD] Prec.(%) | [BOLD] Rec.(%) | [BOLD] F1(%)\nMax Logits | 80.19 | 80.50 | 79.85\nAdd Logits | 81.30 | 81.28 | 80.85\nAdd Logits+Expert | [BOLD] 81.30 | [BOLD] 81.41 | [BOLD] 80.90\nConcat Hidden | 80.24 | 80.04 | 79.65\nMax Hidden | 80.30 | 80.04 | 79.63\nAdd Hidden | 80.82 | 80.28 | 80.08\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "52aa5baf-fd99-4ce9-845f-90ae579128c5",
    "input": "## Claim\nHere is a claim: Intrusion by Noise Word: the imparted knowledge often adds words that are grammatical, but are out of context. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "5eb5ab6e-0556-435d-b7c3-f73a75086415",
    "input": "## Claim\nHere is a claim: Similarly, excluding the direction aggregation module leads to a performance drop to 24.6 BLEU points. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2a748b24-0923-494a-b41b-5b290c77df35",
    "input": "## Claim\nHere is a claim: The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are not statistically significant (paired t-test) with respect to baselines at significance level 0.005. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.\nTopic Name | Size | TF-IDF ARI | WMD ARI | Sent2vec ARI | Doc2vec ARI | BERT ARI | [ITALIC] OD-w2v ARI | [ITALIC] OD-d2v ARI | TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil. | Sent2vec  [ITALIC] Sil. | Doc2vec  [ITALIC] Sil. | BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil. | [ITALIC] OD-d2v  [ITALIC] Sil.\nAffirmative Action | 81 | -0.07 | -0.02 | 0.03 | -0.01 | -0.02 | [BOLD] 0.14 | [ITALIC] 0.02 | 0.01 | 0.01 | -0.01 | -0.02 | -0.04 | [BOLD] 0.06 | [ITALIC] 0.01\nAtheism | 116 | [BOLD] 0.19 | 0.07 | 0.00 | 0.03 | -0.01 | 0.11 | [ITALIC] 0.16 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | [ITALIC] 0.05 | [BOLD] 0.07\nAusterity Measures | 20 | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01 | -0.05 | 0.04 | [BOLD] 0.21 | -0.01 | 0.06 | 0.07 | 0.05 | -0.03 | 0.10 | [BOLD] 0.19 | 0.1\nDemocratization | 76 | 0.02 | -0.01 | 0.00 | [ITALIC] 0.09 | -0.01 | [BOLD] 0.11 | 0.07 | 0.01 | 0.01 | 0.02 | 0.02 | 0.03 | [BOLD] 0.16 | [ITALIC] 0.11\nEducation Voucher Scheme | 30 | [BOLD] 0.25 | 0.12 | 0.08 | -0.02 | 0.04 | 0.13 | [ITALIC] 0.19 | 0.01 | 0.01 | 0.01 | -0.01 | 0.02 | [ITALIC] 0.38 | [BOLD] 0.40\nGambling | 60 | -0.06 | -0.01 | -0.02 | 0.04 | 0.09 | [ITALIC] 0.35 | [BOLD] 0.39 | 0.01 | 0.02 | 0.03 | 0.01 | 0.09 | [BOLD] 0.30 | [ITALIC] 0.22\nHousing | 30 | 0.01 | -0.01 | -0.01 | -0.02 | 0.08 | [BOLD] 0.27 | 0.01 | 0.02 | 0.03 | 0.03 | 0.01 | 0.11 | [BOLD] 0.13 | [ITALIC] 0.13\nHydroelectric Dams | 110 | [BOLD] 0.47 | [ITALIC] 0.45 | [ITALIC] 0.45 | -0.01 | 0.38 | 0.35 | 0.14 | 0.04 | 0.08 | 0.12 | 0.01 | 0.19 | [BOLD] 0.26 | [ITALIC] 0.09\nIntellectual Property | 66 | 0.01 | 0.01 | 0.00 | 0.03 | 0.03 | [ITALIC] 0.05 | [BOLD] 0.14 | 0.01 | [ITALIC] 0.04 | 0.03 | 0.01 | 0.03 | [ITALIC] 0.04 | [BOLD] 0.12\nKeystone pipeline | 18 | 0.01 | 0.01 | 0.00 | -0.13 | [BOLD] 0.07 | -0.01 | [BOLD] 0.07 | -0.01 | -0.03 | -0.03 | -0.07 | 0.03 | [BOLD] 0.05 | [ITALIC] 0.02\nMonarchy | 61 | -0.04 | 0.01 | 0.00 | 0.03 | -0.02 | [BOLD] 0.15 | [BOLD] 0.15 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 | [BOLD] 0.11 | [ITALIC] 0.09\nNational Service | 33 | 0.14 | -0.03 | -0.01 | 0.02 | 0.01 | [ITALIC] 0.31 | [BOLD] 0.39 | 0.02 | 0.04 | 0.02 | 0.01 | 0.02 | [BOLD] 0.25 | [BOLD] 0.25\nOne-child policy China | 67 | -0.05 | 0.01 | [BOLD] 0.11 | -0.02 | 0.02 | [BOLD] 0.11 | 0.01 | 0.01 | 0.02 | [ITALIC] 0.04 | -0.01 | 0.03 | [BOLD] 0.07 | -0.02\nOpen-source Software | 48 | -0.02 | -0.01 | [ITALIC] 0.05 | 0.01 | 0.12 | [BOLD] 0.09 | -0.02 | 0.01 | -0.01 | 0.00 | -0.02 | 0.03 | [BOLD] 0.18 | 0.01\nPornography | 52 | -0.02 | 0.01 | 0.01 | -0.02 | -0.01 | [BOLD] 0.41 | [BOLD] 0.41 | 0.01 | 0.01 | 0.02 | -0.01 | 0.03 | [BOLD] 0.47 | [ITALIC] 0.41\nSeanad Abolition | 25 | 0.23 | 0.09 | -0.01 | -0.01 | 0.03 | [ITALIC] 0.32 | [BOLD] 0.54 | 0.02 | 0.01 | -0.01 | -0.03 | -0.04 | [ITALIC] 0.15 | [BOLD] 0.31\nTrades Unions | 19 | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60 | -0.05 | 0.44 | [ITALIC] 0.44 | 0.29 | 0.1 | 0.17 | 0.21 | 0.01 | 0.26 | [BOLD] 0.48 | [ITALIC] 0.32\nVideo Games | 72 | -0.01 | 0.01 | 0.12 | 0.01 | 0.08 | [ITALIC] 0.40 | [BOLD] 0.56 | 0.01 | 0.01 | 0.06 | 0.01 | 0.05 | [ITALIC] 0.32 | [BOLD] 0.42\nAverage | 54.67 | 0.09 | 0.07 | 0.08 | 0.01 | 0.08 | [BOLD] 0.22 | [ITALIC] 0.20 | 0.02 | 0.03 | 0.04 | -0.01 | 0.05 | [BOLD] 0.20 | [ITALIC] 0.17\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "7ff4400d-a41b-4b6e-9fbc-20980c38e5fa",
    "input": "## Claim\nHere is a claim: The improvements due to shared representations and a disjoint entity span model are approximately equal, but the two models in combination together achieve the highest results, increasing joint <italic>F</italic>1 to 71.2. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | <bold>71.2</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e4c0799b-5c10-4ff0-a19a-4f43a14dc253",
    "input": "## Claim\nHere is a claim: We can see from Table 6 that empirically adding logits from two models after classifiers does not perform the best. Does the following context support or refute the claim?\n\n## Table\nPaper title: Filling Conversation Ellipsis for Better Social Dialog Understanding\nTable caption: Table 6: Dialog act prediction performance using different selection methods.\n[BOLD] Selection Method | [BOLD] Prec.(%) | [BOLD] Rec.(%) | [BOLD] F1(%)\nMax Logits | 80.19 | 80.50 | 79.85\nAdd Logits | 81.30 | 81.28 | 80.85\nAdd Logits+Expert | [BOLD] 81.30 | [BOLD] 81.41 | [BOLD] 80.90\nConcat Hidden | 80.24 | 80.04 | 79.65\nMax Hidden | 80.30 | 80.04 | 79.63\nAdd Hidden | 80.82 | 80.28 | 80.08\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6f1ef6d3-b841-4e1e-ab21-64d50093f881",
    "input": "## Claim\nHere is a claim: As can be seen in Table 1, softmax achieves better results overall when compared with sparsemax and TVMAX, indicating that the use of selective attention does not necessarily lead to better captions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.\n[EMPTY] | MSCOCO spice | MSCOCO cider | MSCOCO rouge [ITALIC] L | MSCOCO bleu4 | MSCOCO meteor | MSCOCO rep\u2193 | Flickr30k spice | Flickr30k cider | Flickr30k rouge [ITALIC] L | Flickr30k bleu4 | Flickr30k meteor | Flickr30k rep\u2193\nsoftmax | 18.4 | 0.967 | 52.9 | 29.9 | 24.9 | 3.76 | 13.5 | 0.443 | 44.2 | 19.9 | 19.1 | 6.09\nsparsemax | [BOLD] 18.9 | [BOLD] 0.990 | [BOLD] 53.5 | [BOLD] 31.5 | [BOLD] 25.3 | 3.69 | [BOLD] 13.7 | [BOLD] 0.444 | [BOLD] 44.3 | [BOLD] 20.7 | [BOLD] 19.3 | 5.84\nTVmax | 18.5 | 0.974 | 53.1 | 29.9 | 25.1 | [BOLD] 3.17 | 13.3 | 0.438 | 44.2 | 20.5 | 19.0 | [BOLD] 3.97\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "f2719604-1c66-4880-9cef-38422fcdc053",
    "input": "## Claim\nHere is a claim: The ensemble approach based on combining five DCGCN models initialized with different random seeds achieves a BLEU score of 30.4 and a CHRF++ score of 59.6. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.\n[BOLD] Model | [BOLD] T | #P | B | C\nSeq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1\nGGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4\nSeq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5\nGGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5\nDCGCN (ours) | S | [BOLD] 19.1M | 27.9 | 57.3\nDCGCN (ours) | E | 92.5M | [BOLD] 30.4 | [BOLD] 59.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "588089f9-61e0-4931-8f00-ce35a0d525b9",
    "input": "## Claim\nHere is a claim: (production) column shows their product. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 7: Sensitivity of BERT-large to superficial cues identified in \u00a72 (unit: 10\u22122). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.\nCue | [ITALIC] SCOPA | [ITALIC] SB_COPA | Diff. | Prod.\nwoman | 7.98 | 4.84 | -3.14 | 0.25\nmother | 5.16 | 3.95 | -1.21 | 0.75\nwent | 6.00 | 5.15 | -0.85 | 0.73\ndown | 5.52 | 4.93 | -0.58 | 0.71\ninto | 4.07 | 3.51 | -0.56 | 0.40\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "d4055277-83e0-4021-a90a-417f90def791",
    "input": "## Claim\nHere is a claim: We observe that the average scope length is quite small, with the majority having a scope length of 1. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 3: Cue and token distribution in the conversational negation corpus.\nTotal negation cues | 2921\nTrue negation cues | 2674\nFalse negation cues | 247\nAverage scope length | 2.9\nAverage sentence length | 13.6\nAverage tweet length | 22.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c1eced18-5360-4a8e-af31-06277e4a832e",
    "input": "## Claim\nHere is a claim: We find that the effect of syntactic structure varies between the different relation types. Does the following context support or refute the claim?\n\n## Table\nPaper title: Syntactic Dependency Representations in Neural Relation Classification\nTable caption: Table 1: Effect of using the shortest dependency path on each relation type.\n[BOLD] Relation | [BOLD] best F1 (in 5-fold) without sdp | [BOLD] best F1 (in 5-fold) with sdp | [BOLD] Diff.\nUSAGE | 60.34 | 80.24 | + 19.90\nMODEL-FEATURE | 48.89 | 70.00 | + 21.11\nPART_WHOLE | 29.51 | 70.27 | +40.76\nTOPIC | 45.80 | 91.26 | +45.46\nRESULT | 54.35 | 81.58 | +27.23\nCOMPARE | 20.00 | 61.82 | + 41.82\nmacro-averaged | 50.10 | 76.10 | +26.00\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a63f189d-7408-49c3-bc3c-2b89a01e30cf",
    "input": "## Claim\nHere is a claim: LRN is still the fastest model, outperforming other recurrent units by 8%\u223c27%. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.\nModel | Model | #Params | Base ACC | Base Time | +LN ACC | +LN Time | +BERT ACC | +BERT Time | +LN+BERT ACC | +LN+BERT Time\nRockt\u00e4schel et\u00a0al. ( 2016 ) | Rockt\u00e4schel et\u00a0al. ( 2016 ) | 250K | 83.50 | - | - | - | - | - | - | -\nThis | LSTM | 8.36M | 84.27 | 0.262 | 86.03 | 0.432 | 89.95 | 0.544 | [BOLD] 90.49 | 0.696\nThis | GRU | 6.41M | [BOLD] 85.71 | 0.245 | [BOLD] 86.05 | 0.419 | [BOLD] 90.29 | 0.529 | 90.10 | 0.695\nThis | ATR | 2.87M | 84.88 | 0.210 | 85.81 | 0.307 | 90.00 | 0.494 | 90.28 | 0.580\nWork | SRU | 5.48M | 84.28 | 0.258 | 85.32 | 0.283 | 89.98 | 0.543 | 90.09 | 0.555\n[EMPTY] | LRN | 4.25M | 84.88 | [BOLD] 0.209 | 85.06 | [BOLD] 0.223 | 89.98 | [BOLD] 0.488 | 89.93 | [BOLD] 0.506\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "97d49102-06a2-4887-84ba-121e1a200ed6",
    "input": "## Claim\nHere is a claim: We also observe that WMD-UNIGRAMS slightly outperforms WMD-BIGRAMS on 3 out of 4 language pairs. Does the following context support or refute the claim?\n\n## Table\nPaper title: MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance\nTable caption: Table 5: Comparison on hard and soft alignments.\nMetrics | cs-en | de-en | fi-en | lv-en\nRUSE | 0.624 | 0.644 | 0.750 | 0.697\nHmd-F1 + BERT | 0.655 | 0.681 | 0.821 | 0.712\nHmd-Recall + BERT | 0.651 | 0.658 | 0.788 | 0.681\nHmd-Prec + BERT | 0.624 | 0.669 | 0.817 | 0.707\nWmd-unigram + BERT | 0.651 | 0.686 | <bold>0.823</bold> | 0.710\nWmd-bigram + BERT | <bold>0.665</bold> | <bold>0.688</bold> | 0.821 | <bold>0.712</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9a337795-1c06-4d0e-91f3-3ec46743dc82",
    "input": "## Claim\nHere is a claim: Our approach DKRN outperforms all state-of-the-art methods in terms of all metrics on both datasets with two tasks. Does the following context support or refute the claim?\n\n## Table\nPaper title: Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation\nTable caption: Table 3: Results of Turn-level Evaluation.\nDataset | System | Keyword Prediction  [ITALIC] Rw@1 | Keyword Prediction  [ITALIC] Rw@3 | Keyword Prediction  [ITALIC] Rw@5 | Keyword Prediction P@1 | Response Retrieval  [ITALIC] R20@1 | Response Retrieval  [ITALIC] R20@3 | Response Retrieval  [ITALIC] R20@5 | Response Retrieval MRR\nTGPC | Retrieval\u00a0 | - | - | - | - | 0.5063 | 0.7615 | 0.8676 | 0.6589\nTGPC | PMI\u00a0 | 0.0585 | 0.1351 | 0.1872 | 0.0871 | 0.5441 | 0.7839 | 0.8716 | 0.6847\nTGPC | Neural\u00a0 | 0.0708 | 0.1438 | 0.1820 | 0.1321 | 0.5311 | 0.7905 | 0.8800 | 0.6822\nTGPC | Kernel\u00a0 | 0.0632 | 0.1377 | 0.1798 | 0.1172 | 0.5386 | 0.8012 | 0.8924 | 0.6877\nTGPC | DKRN (ours) | [BOLD] 0.0909 | [BOLD] 0.1903 | [BOLD] 0.2477 | [BOLD] 0.1685 | [BOLD] 0.5729 | [BOLD] 0.8132 | [BOLD] 0.8966 | [BOLD] 0.7110\nCWC | Retrieval\u00a0 | - | - | - | - | 0.5785 | 0.8101 | 0.8999 | 0.7141\nCWC | PMI\u00a0 | 0.0555 | 0.1001 | 0.1212 | 0.0969 | 0.5945 | 0.8185 | 0.9054 | 0.7257\nCWC | Neural\u00a0 | 0.0654 | 0.1194 | 0.1450 | 0.1141 | 0.6044 | 0.8233 | 0.9085 | 0.7326\nCWC | Kernel\u00a0 | 0.0592 | 0.1113 | 0.1337 | 0.1011 | 0.6017 | 0.8234 | 0.9087 | 0.7320\nCWC | DKRN (ours) | [BOLD] 0.0680 | [BOLD] 0.1254 | [BOLD] 0.1548 | [BOLD] 0.1185 | [BOLD] 0.6324 | [BOLD] 0.8416 | [BOLD] 0.9183 | [BOLD] 0.7533\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "fc18a2d7-0c48-44fa-af1f-ec7c4d640927",
    "input": "## Claim\nHere is a claim: These observations match our intuition that the learned policy reward will work best with the encoder trained jointly with the policy network Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8d09348a-d27e-4eb0-98fd-fbd2f76e5dba",
    "input": "## Claim\nHere is a claim: the accuracies for a single vector models are in par with the several vector models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "84d8d7a2-2811-41a4-b5a3-1777e9b8af8a",
    "input": "## Claim\nHere is a claim: ALDM even gets worse performance than ACER and PPO. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c9d32538-fe25-40c4-a010-0b5e2a167331",
    "input": "## Claim\nHere is a claim: Contrary to intuition, the sob emoji contributes more than cry, despite representing a stronger emotion. Does the following context support or refute the claim?\n\n## Table\nPaper title: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations\nTable caption: Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.\n[BOLD] Emoji alias | [BOLD] N | [BOLD] emoji # | [BOLD] emoji % | [BOLD] no-emoji # | [BOLD] no-emoji % | [BOLD] \u0394%\nmask | 163 | 154 | 94.48 | 134 | 82.21 | - 12.27\ntwo_hearts | 87 | 81 | 93.10 | 77 | 88.51 | - 4.59\nheart_eyes | 122 | 109 | 89.34 | 103 | 84.43 | - 4.91\nheart | 267 | 237 | 88.76 | 235 | 88.01 | - 0.75\nrage | 92 | 78 | 84.78 | 66 | 71.74 | - 13.04\ncry | 116 | 97 | 83.62 | 83 | 71.55 | - 12.07\nsob | 490 | 363 | 74.08 | 345 | 70.41 | - 3.67\nunamused | 167 | 121 | 72.46 | 116 | 69.46 | - 3.00\nweary | 204 | 140 | 68.63 | 139 | 68.14 | - 0.49\njoy | 978 | 649 | 66.36 | 629 | 64.31 | - 2.05\nsweat_smile | 111 | 73 | 65.77 | 75 | 67.57 | 1.80\nconfused | 77 | 46 | 59.74 | 48 | 62.34 | 2.60\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4287ef04-6401-4a5f-bf7e-a2d4784e15f4",
    "input": "## Claim\nHere is a claim: If a correct relation is retrieved by the model but is not linked in the knowledge base, the precision increases as the recall rate increases. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nIteration=1 | 0.531 | 0.455 | 0.353 | 0.201\nIteration=2 | 0.592 | 0.498 | 0.385 | 0.375\nIteration=3 | 0.650 | 0.519 | 0.422 | 0.405\nIteration=4 | 0.601 | 0.505 | 0.422 | 0.385\nIteration=5 | 0.575 | 0.495 | 0.394 | 0.376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e1a148e5-952a-4c09-a758-a6b498726764",
    "input": "## Claim\nHere is a claim: G-Pre, for example, indicates that for the \u201cgood\u201d summaries, an average of 39.2% of their words overlap with those from references, suggesting that a good summary has much more than an \u201cad-hoc\u201d fraction of correct words. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nMetric | [ITALIC] \u03c1 | [ITALIC] r | G-Pre | G-Rec\nROUGE-1 | .290 | .304 | .392 | .428\nROUGE-2 | .259 | .278 | .408 | .444\nROUGE-L | .274 | .297 | .390 | .426\nROUGE-SU4 | .282 | .279 | .404 | .440\nBLEU-1 | .256 | .281 | .409 | .448\nBLEU-2 | .301 | .312 | .411 | .446\nBLEU-3 | .317 | .312 | .409 | .444\nBLEU-4 | .311 | .307 | .409 | .446\nBLEU-5 | .308 | .303 | .420 | .459\nMETEOR | .305 | .285 | .409 | .444\nInferSent-Cosine | [BOLD] .329 | [BOLD] .339 | .417 | .460\nBERT-Cosine | .312 | .335 | [BOLD] .440 | [BOLD] .484\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0988097c-eeaa-4876-91cc-424a0e4d7f65",
    "input": "## Claim\nHere is a claim: What we have found is that Google Translate does not always translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, as evidenced by the data in Table 6. Does the following context support or refute the claim?\n\n## Table\nPaper title: Assessing Gender Bias in Machine Translation \u2013 A Case Study with Google Translate\nTable caption: Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table\nCategory | Female (%) | Male (%) | Neutral (%)\nOffice and administrative support | 11.015 | 58.812 | 16.954\nArchitecture and engineering | 2.299 | 72.701 | 10.92\nFarming, fishing, and forestry | 12.179 | 62.179 | 14.744\nManagement | 11.232 | 66.667 | 12.681\nCommunity and social service | 20.238 | 62.5 | 10.119\nHealthcare support | 25.0 | 43.75 | 17.188\nSales and related | 8.929 | 62.202 | 16.964\nInstallation, maintenance, and repair | 5.22 | 58.333 | 17.125\nTransportation and material moving | 8.81 | 62.976 | 17.5\nLegal | 11.905 | 72.619 | 10.714\nBusiness and financial operations | 7.065 | 67.935 | 15.58\nLife, physical, and social science | 5.882 | 73.284 | 10.049\nArts, design, entertainment, sports, and media | 10.36 | 67.342 | 11.486\nEducation, training, and library | 23.485 | 53.03 | 9.091\nBuilding and grounds cleaning and maintenance | 12.5 | 68.333 | 11.667\nPersonal care and service | 18.939 | 49.747 | 18.434\nHealthcare practitioners and technical | 22.674 | 51.744 | 15.116\nProduction | 14.331 | 51.199 | 18.245\nComputer and mathematical | 4.167 | 66.146 | 14.062\nConstruction and extraction | 8.578 | 61.887 | 17.525\nProtective service | 8.631 | 65.179 | 12.5\nFood preparation and serving related | 21.078 | 58.333 | 17.647\nTotal | 11.76 | 58.93 | 15.939\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "59876715-8c94-4df5-8027-281cc74e8292",
    "input": "## Claim\nHere is a claim: Among all the baselines, GDPL obtains the most preference against PPO. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ea4f2d3a-dae7-4eab-9d17-132bc404395a",
    "input": "## Claim\nHere is a claim: we build two agents for disentanglement learning: a generative model-based conversation model (GP-MBCM) and an end-to-end variant of the Adversarial Learning based Dialogue Model (ALDM). Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "79301cb3-91f0-42b8-abe9-90fe05d52e87",
    "input": "## Claim\nHere is a claim: due to the monotonic nature of the 5-action generation problem, the greedy algorithm and fixed threshold based policies achieve close to perfect action selections, although the top-k sampling strategy is still able to generate more diverse and natural responses on the test set with augmented data Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0633fe26-997d-4980-b1c3-69077f797d1e",
    "input": "## Claim\nHere is a claim: It is clear from Table 5 that using the learned reward does not help the RL-based system generate summaries with significantly higher human ratings. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. \u201cPref%\u201d: in how many percentage of documents a system receives the higher human rating.\nReward | R-1 | R-2 | R-L | Human | Pref%\nR-L (original) | 40.9 | 17.8 | 38.5 | 1.75 | 15\nLearned (ours) | 39.2 | 17.4 | 37.5 | [BOLD] 2.20 | [BOLD] 75\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "8a19218e-4f09-4024-993b-dba930690f7b",
    "input": "## Claim\nHere is a claim: Lastly, BERT-large models do not fine-tune well (as opposed to RoBERTa). Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large | B-COPA | 70.5 (\u00b1 2.5) | 72.6 (\u00b1 2.3) | [BOLD] 69.1 (\u00b1 2.7)\nBERT-large | B-COPA (50%) | 69.9 (\u00b1 1.9) | 71.2 (\u00b1 1.3) | 69.0 (\u00b1 3.5)\nBERT-large | COPA | [BOLD] 71.7 (\u00b1 0.5) | [BOLD] 80.5 (\u00b1 0.4) | 66.3 (\u00b1 0.8)\nRoBERTa-large | B-COPA | [BOLD] 76.7 (\u00b1 0.8) | 73.3 (\u00b1 1.5) | [BOLD] 78.8 (\u00b1 2.0)\nRoBERTa-large | B-COPA (50%) | 72.4 (\u00b1 2.0) | 72.1 (\u00b1 1.7) | 72.6 (\u00b1 2.1)\nRoBERTa-large | COPA | 76.4 (\u00b1 0.7) | [BOLD] 79.6 (\u00b1 1.0) | 74.4 (\u00b1 1.1)\nBERT-base-NSP | None | [BOLD] 66.4 | 66.2 | [BOLD] 66.7\nBERT-large-NSP | None | 65.0 | [BOLD] 66.9 | 62.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "98be01ed-6aba-4c93-9b7b-d19e39a83138",
    "input": "## Claim\nHere is a claim: As shown in Table 6, the performance of LRN is significantly lower than that of LSTM and GRU (-1.05 and -0.79). Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 6: F1 score on CoNLL-2003 English NER task. \u201c#Params\u201d: the parameter number in NER task. LSTM* denotes the reported result\u00a0Lample et\u00a0al. (2016).\nModel | #Params | NER\nLSTM* | - | 90.94\nLSTM | 245K | [BOLD] 89.61\nGRU | 192K | 89.35\nATR | 87K | 88.46\nSRU | 161K | 88.89\nLRN | 129K | 88.56\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "043020a0-d7f8-48bf-b2be-50dbed48a648",
    "input": "## Claim\nHere is a claim: Table 4 lists the EM/F1 score of different models. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 4: Exact match/F1-score on SQuad dataset. \u201c#Params\u201d: the parameter number of Base. rnet*: results published by\u00a0Wang et\u00a0al. (2017).\nModel | #Params | Base | +Elmo\nrnet* | - | 71.1/79.5 | -/-\nLSTM | 2.67M | [BOLD] 70.46/78.98 | 75.17/82.79\nGRU | 2.31M | 70.41/ [BOLD] 79.15 | 75.81/83.12\nATR | 1.59M | 69.73/78.70 | 75.06/82.76\nSRU | 2.44M | 69.27/78.41 | 74.56/82.50\nLRN | 2.14M | 70.11/78.83 | [BOLD] 76.14/ [BOLD] 83.83\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "e09138ac-6dc6-45f0-a0e3-2fc276e7b93a",
    "input": "## Claim\nHere is a claim: Excluding the direction aggregation module does not lead to a performance drop to 24.6 BLEU points. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "239b62aa-ebc4-4f36-aa92-d44876612730",
    "input": "## Claim\nHere is a claim: We see a varying increase in sentiment value across all three models after finetuning, indicating that the framework is not always able to pick up on words that are indicative of sentiment. Does the following context support or refute the claim?\n\n## Table\nPaper title: What do Deep Networks Like to Read?\nTable caption: Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.\n[EMPTY] | <bold>RNN</bold> | <bold>CNN</bold> | <bold>DAN</bold>\nPositive | +9.7 | +4.3 | +<bold>23.6</bold>\nNegative | +6.9 | +5.5 | +<bold>16.1</bold>\nFlipped to Positive | +20.2 | +24.9 | +27.4\nFlipped to Negative | +31.5 | +28.6 | +19.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "531171f1-fe4b-4849-81ec-36b06b6eb36f",
    "input": "## Claim\nHere is a claim: The amount of resources is sufficient for executing forward computations, and therefore our framework outperforms the folding technique for the inference task with up to 4.93x faster throughput. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2\nTable caption: Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold\u2019s folding technique, and TensorFlow\u2019s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.\nBatch size | Throughput (instances/s) Inference | Throughput (instances/s) Inference | Throughput (instances/s) Inference | Throughput (instances/s) Training | Throughput (instances/s) Training | Throughput (instances/s) Training\nBatch size | Iter | Recur | Fold | Iter | Recur | Fold\n1 | 19.2 | 81.4 | 16.5 | 2.5 | 4.8 | 9.0\n10 | 49.3 | 217.9 | 52.2 | 4.0 | 4.2 | 37.5\n25 | 72.1 | 269.9 | 61.6 | 5.5 | 3.6 | 54.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "edca66cb-3a89-4544-9002-165bea2531a9",
    "input": "## Claim\nHere is a claim: the lowest mean turns per successful conversation from the human test user (HUS) was achieved by GDPL (mean HUS turns 20.8) Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "fdbb1a73-528d-4505-afaf-f182c3127253",
    "input": "## Claim\nHere is a claim: the neural user simulator trained by GDPL outperforms the other models, in terms of both more turns per successful session (i.e., human-like turns, GDPL = 19.7) and success rate, as shown in Table 6. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c94dd088-6c09-435e-b57b-9a983da5a990",
    "input": "## Claim\nHere is a claim: This is mainly because the LSTM uses contextual information such as preceding cue words and preceding reactions, but the false cues are often individual words rather than phrases. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 4: Cue classification on the test set.\n[EMPTY] | [BOLD] F-Score  [BOLD] Baseline | [BOLD] F-Score  [BOLD] Proposed | [BOLD] Support\nFalse cues | 0.61 | 0.68 | 47\nActual cues | 0.97 | 0.98 | 557\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6cc40bb2-3a34-433d-9612-b5a5df7ae632",
    "input": "## Claim\nHere is a claim: in contrast, the proposed method obtains better and more robust performances than both baselines, with a 0.06 higher PCS and a 0.27 increase in in-scope recall Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "a0b9120d-320d-4547-967f-d8b3eb9529f2",
    "input": "## Claim\nHere is a claim: The average number of tokens per tweet is 22.3, per sentence is 13.6 and average scope length is 2.9. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 3: Cue and token distribution in the conversational negation corpus.\nTotal negation cues | 2921\nTrue negation cues | 2674\nFalse negation cues | 247\nAverage scope length | 2.9\nAverage sentence length | 13.6\nAverage tweet length | 22.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c0e45fc9-0434-4421-bdaf-7b40f7afac29",
    "input": "## Claim\nHere is a claim: In LDC2017T10, G2S-GGNN achieves a BLEU score of 27.87, which is 3.33 points higher than Damonte and Cohen (2019), a state-of-the-art model that does not employ external information. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d94636ce-6e6c-4c0a-8766-510fbd289527",
    "input": "## Claim\nHere is a claim: to quantify the contribution of each model component on this task, we vary the model\u2019s architecture by progressively adding context and the dependency feature by applying parameter sharing or via a pretrained neural classifier, obtaining comparable results. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "91a29994-676f-4490-8b85-b37396c20d2b",
    "input": "## Claim\nHere is a claim: That ambiguity can be reflected in the length of negation scope.We found that most negation scopes only involve one or two tokens Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 3: Cue and token distribution in the conversational negation corpus.\nTotal negation cues | 2921\nTrue negation cues | 2674\nFalse negation cues | 247\nAverage scope length | 2.9\nAverage sentence length | 13.6\nAverage tweet length | 22.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6592737a-49c3-4723-b433-e554703165cd",
    "input": "## Claim\nHere is a claim: [CONTINUE] G2S-GIN has a better performance in handling graphs with node out-degrees higher than 9. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "499719d6-acd1-40d1-8962-3bd945f7691c",
    "input": "## Claim\nHere is a claim: The error reduction over the best baseline is only 5.09% on average. Does the following context support or refute the claim?\n\n## Table\nPaper title: Deriving Machine Attention from Human Rationales\nTable caption: Table 4: Accuracy of transferring between domains. Models with \u2020 use labeled data from source domains and unlabeled data from the target domain. Models with \u2021 use human rationales on the target task.\nSource | Target | Svm | Ra-Svm\u2021 | Ra-Cnn\u2021 | Trans\u2020 | Ra-Trans\u2021\u2020 | Ours\u2021\u2020 | Oracle\u2020\nBeer look + Beer aroma + Beer palate | Hotel location | 78.65 | 79.09 | 79.28 | 80.42 | 82.10 | [BOLD] 84.52 | 85.43\nBeer look + Beer aroma + Beer palate | Hotel cleanliness | 86.44 | 86.68 | 89.01 | 86.95 | 87.15 | [BOLD] 90.66 | 92.09\nBeer look + Beer aroma + Beer palate | Hotel service | 85.34 | 86.61 | 87.91 | 87.37 | 86.40 | [BOLD] 89.93 | 92.42\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "452f09ad-4e54-4e45-b65a-f07fc4c3055e",
    "input": "## Claim\nHere is a claim: For slot values, the performance is poor when actions are absent, where it is only possible to generate a few fixed templates. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "56344d9a-7b9f-4606-ac3b-ef1134c5db28",
    "input": "## Claim\nHere is a claim: In contrast, DAN masks out punctuation and determiners using words indicative of the class label (i.e. Does the following context support or refute the claim?\n\n## Table\nPaper title: What do Deep Networks Like to Read?\nTable caption: Table 1: Example sentences of the different classifiers compared to the original on SST-2. We report further examples in the Appendix. <u> use for <UNK>.\nOrig | <u> turns in a <u> screenplay that <u> at the edges ; it \u2019s so clever you want to hate it .\nDAN | <u> turns in a <u> screenplay screenplay screenplay of <u> edges edges edges shapes so clever easy want hate hate hate hate hate hate hate hate hate hate\nCNN | she turns on a on ( ( in in the the the edges \u2019s so clever \u201c want to hate it \u201d\nRNN | <u> turns in a <u> screenplay was <u> <u> <u> edges edges edges curves <u> clever clever you want hate hate it .\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "62f2b650-5f5f-46e6-8570-bd6ff0013ea0",
    "input": "## Claim\nHere is a claim: TF has the best values of recall and f-measure for all corpora except the English version of TED Talks, where DF has the best value of recall and HClust has the best value of f-measure. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "c40d9860-c431-4f1b-8575-cd9e463b4967",
    "input": "## Claim\nHere is a claim: the DA-RL method beats the DA-SLU methods on most criteria Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "fd27b181-4ffc-4033-89c1-cae259504aad",
    "input": "## Claim\nHere is a claim: For the 10-action experiments, the improvements are by 0.33 and 1.39 respectively. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f3feea83-901a-4779-ab3b-bbd37a7b2ad8",
    "input": "## Claim\nHere is a claim: [CONTINUE] Tweets containing emoji seem to be easier for the model to classify than those without. Does the following context support or refute the claim?\n\n## Table\nPaper title: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations\nTable caption: Table 4: Number of tweets on the test set with and without emoji and hashtags. The number between parentheses is the proportion of tweets classified correctly.\n[EMPTY] | [BOLD] Present | [BOLD] Not Present\nEmoji | 4805 (76.6%) | 23952 (68.0%)\nHashtags | 2122 (70.5%) | 26635 (69.4%)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "8e8f27d4-b0f5-43ef-b7ea-55a8d400fc5e",
    "input": "## Claim\nHere is a claim: [CONTINUE] The relative lower BLEU score of our DAMD model compared to other models with different system action forms suggests that it does not outperform them in terms of inform and success rates, [CONTINUE] While we find applying our data augmentation achieves a limited improvement on combined score (6 vs 7), it is not enough to make up for the lower BLEU score, [CONTINUE] Moreover, even if a model has access to ground truth system action, the model does not necessarily improve its task performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "8da915c7-59a0-473f-9ae4-dc07094a27f0",
    "input": "## Claim\nHere is a claim: Table 3 shows the impact of coverage for decreasing generalization across these two datasets that belong to the two similar tasks of reading comprehension and QA-SRL. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ff46875b-b5a0-4a1d-81cf-3120d135efad",
    "input": "## Claim\nHere is a claim: Overall, ECA gains an average improvement of 10.5% over BLEU and 5.2% over METEOR. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "edebc118-50fc-421c-a113-85111fe97188",
    "input": "## Claim\nHere is a claim: we can also see that our method lags somewhat behind the state of the art on ROUGE, it achieves comparable ROUGE scores in comparison with RL-based systems on the CNN-DM dataset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "1ad45c40-5107-4f03-9741-d43fe4bf9bb5",
    "input": "## Claim\nHere is a claim: The relatively high accuracies of BERT-large, RoBERTa-large and BERT-*-NSP show that these pretrained models are already well-equipped to perform this task \"out-of-the-box\". Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large | B-COPA | 70.5 (\u00b1 2.5) | 72.6 (\u00b1 2.3) | [BOLD] 69.1 (\u00b1 2.7)\nBERT-large | B-COPA (50%) | 69.9 (\u00b1 1.9) | 71.2 (\u00b1 1.3) | 69.0 (\u00b1 3.5)\nBERT-large | COPA | [BOLD] 71.7 (\u00b1 0.5) | [BOLD] 80.5 (\u00b1 0.4) | 66.3 (\u00b1 0.8)\nRoBERTa-large | B-COPA | [BOLD] 76.7 (\u00b1 0.8) | 73.3 (\u00b1 1.5) | [BOLD] 78.8 (\u00b1 2.0)\nRoBERTa-large | B-COPA (50%) | 72.4 (\u00b1 2.0) | 72.1 (\u00b1 1.7) | 72.6 (\u00b1 2.1)\nRoBERTa-large | COPA | 76.4 (\u00b1 0.7) | [BOLD] 79.6 (\u00b1 1.0) | 74.4 (\u00b1 1.1)\nBERT-base-NSP | None | [BOLD] 66.4 | 66.2 | [BOLD] 66.7\nBERT-large-NSP | None | 65.0 | [BOLD] 66.9 | 62.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d1e8aea0-544c-4fb8-bd06-c0c06214a07f",
    "input": "## Claim\nHere is a claim: comparing with the standard MD model, our models can generate more diverse responses (DAMD: 3.12 vs 3.65, HDSA: 2.14 vs 2.67), and our DAMD model (with external data)  can also generate more appropriate responses (2.50 vs 2.53). Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.\nModel | Diversity | App | Good% | OK% | Invalid%\nDAMD | 3.12 | 2.50 | 56.5% | [BOLD] 37.4% | 6.1%\nDAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1% | 9.9%\nHDSA (+) | 2.14 | 2.47 | 57.5% | 32.5% | [BOLD] 10.0%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "d59d4e8e-7158-4781-8497-005678f26dbd",
    "input": "## Claim\nHere is a claim: This is corroborated by the negative difference of associated product scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 7: Sensitivity of BERT-large to superficial cues identified in \u00a72 (unit: 10\u22122). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.\nCue | [ITALIC] SCOPA | [ITALIC] SB_COPA | Diff. | Prod.\nwoman | 7.98 | 4.84 | -3.14 | 0.25\nmother | 5.16 | 3.95 | -1.21 | 0.75\nwent | 6.00 | 5.15 | -0.85 | 0.73\ndown | 5.52 | 4.93 | -0.58 | 0.71\ninto | 4.07 | 3.51 | -0.56 | 0.40\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "31045e85-aa4a-47a2-97ba-f540814dee11",
    "input": "## Claim\nHere is a claim: capsule net improves the performance significantly by removing this residual connection, which is also confirmed in Table 4 where there is a slight increase in AUC when replacing capsule net with pure max-pooling operation in graph encoder Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\n-Word-ATT | 0.648 | 0.515 | 0.395 | 0.389\n-Capsule | 0.635 | 0.507 | 0.413 | 0.386\nOur Model | 0.650 | 0.519 | 0.422 | 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "754e6967-568c-467b-8192-79e841cef788",
    "input": "## Claim\nHere is a claim: Table 4: Word mover metrics outperform all baselines except for the supervised metric LEIC, which uses more information by considering both images and texts. Does the following context support or refute the claim?\n\n## Table\nPaper title: MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance\nTable caption: Table 4: Pearson correlation with system-level human judgments on MSCOCO dataset. \u2019M\u2019 and \u2019P\u2019 are short names.\nSetting | Metric | M1 | M2\nBaselines | LEIC(*) | <bold>0.939</bold> | <bold>0.949</bold>\nBaselines | METEOR | 0.606 | 0.594\nBaselines | SPICE | 0.759 | 0.750\nBaselines | BERTScore-Recall | 0.809 | 0.749\nSent-Mover | SMD + W2V | 0.683 | 0.668\nSent-Mover | SMD + ELMO + P | 0.709 | 0.712\nSent-Mover | SMD + BERT + P | 0.723 | 0.747\nSent-Mover | SMD + BERT + M + P | 0.789 | 0.784\nWord-Mover | Wmd-1 + W2V | 0.728 | 0.764\nWord-Mover | Wmd-1 + ELMO + P | 0.753 | 0.775\nWord-Mover | Wmd-1 + BERT + P | 0.780 | 0.790\nWord-Mover | Wmd-1 + BERT + M + P | <bold>0.813</bold> | <bold>0.810</bold>\nWord-Mover | Wmd-2 + BERT + M + P | 0.812 | 0.808\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7a5a63e4-676b-42ae-8599-591b3465f476",
    "input": "## Claim\nHere is a claim: We see that the optimized parameter settings are consistent across the different representations, showing that tuning is not necessary for these types of comparisons. Does the following context support or refute the claim?\n\n## Table\nPaper title: Syntactic Dependency Representations in Neural Relation Classification\nTable caption: Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.\n[BOLD] Representation | [BOLD] Hyper parameters Filter size | [BOLD] Hyper parameters Num. Feature maps | [BOLD] Hyper parameters Activation func. | [BOLD] Hyper parameters L2 Reg. | [BOLD] Hyper parameters Learning rate | [BOLD] Hyper parameters Dropout Prob. | [BOLD] F1.(avg. in 5-fold) with default values | [BOLD] F1.(avg. in 5-fold) with optimal values\nCoNLL08 | 4-5 | 1000 | Softplus | 1.15e+01 | 1.13e-03 | 1 | 73.34 | 74.49\nSB | 4-5 | 806 | Sigmoid | 8.13e-02 | 1.79e-03 | 0.87 | 72.83 | [BOLD] 75.05\nUD v1.3 | 5 | 716 | Softplus | 1.66e+00 | 9.63E-04 | 1 | 68.93 | 69.57\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "69ed231d-8e01-4f0f-8360-c65a8d37d64c",
    "input": "## Claim\nHere is a claim: Although the punctuation-based heuristic works reasonably well, it is prone to error in the face of tokens not separated by punctuation, particularly in complex sentences such as example number 1 Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c2398213-ef73-4862-8f2d-48b607c14e26",
    "input": "## Claim\nHere is a claim: The most representative models are only BERT and its variants. Does the following context support or refute the claim?\n\n## Table\nPaper title: Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches\nTable caption: Table 2: Comparison of exact-match accuracy achieved on selected benchmarks by a random or majority-choice baseline, various neural contextual embedding models, and humans. ELMo refers to the highest-performing listed approach using ELMo embeddings. Best system performance on each benchmark in bold. Information extracted from leaderboards (linked to in the first column) at time of writing (October 2019), and original papers for benchmarks introduced in Section\u00a02.\n[BOLD] Benchmark | [BOLD]  Simple Baseline  | [BOLD] ELMo | [BOLD] GPT | [BOLD] BERT | [BOLD] MT-DNN | [BOLD] XLNet | [BOLD] RoBERTa | [BOLD] ALBERT | [BOLD] Human\n[BOLD] CLOTH | 25.0 | 70.7 | \u2013 | [BOLD] 86.0 | \u2013 | \u2013 | \u2013 | \u2013 | 85.9\n[BOLD] Cosmos QA | \u2013 | \u2013 | 54.5 | 67.1 | \u2013 | \u2013 | \u2013 | \u2013 | 94.0\n[BOLD] DREAM | 33.4 | 59.5 | 55.5 | 66.8 | \u2013 | [BOLD] 72.0 | \u2013 | \u2013 | 95.5\n[BOLD] GLUE | \u2013 | 70.0 | \u2013 | 80.5 | 87.6 | 88.4 | 88.5 | [BOLD] 89.4 | 87.1\n[BOLD] HellaSWAG | 25.0 | 33.3 | 41.7 | 47.3 | \u2013 | \u2013 | [BOLD] 85.2 | [EMPTY] | 95.6\n[BOLD] MC-TACO | 17.4 | 26.4 | \u2013 | 42.7 | \u2013 | \u2013 | [BOLD] 43.6 | \u2013 | 75.8\n[BOLD] RACE | 24.9 | \u2013 | 59.0 | 72.0 | \u2013 | 81.8 | 83.2 | [BOLD] 89.4 | 94.5\n[BOLD] SciTail | 60.3 | \u2013 | 88.3 | \u2013 | 94.1 | \u2013 | \u2013 | \u2013 | \u2013\n[BOLD] SQuAD 1.1 | 1.3 | 81.0 | \u2013 | 87.4 | \u2013 | [BOLD] 89.9 | \u2013 | \u2013 | 82.3\n[BOLD] SQuAD 2.0 | 48.9 | 63.4 | \u2013 | 80.8 | \u2013 | 86.3 | 86.8 | [BOLD] 89.7 | 86.9\n[BOLD] SuperGLUE | 47.1 | \u2013 | \u2013 | 69.0 | \u2013 | \u2013 | [BOLD] 84.6 | \u2013 | 89.8\n[BOLD] SWAG | 25.0 | 59.1 | 78.0 | 86.3 | 87.1 | \u2013 | [BOLD] 89.9 | \u2013 | 88.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "e752385e-f0a0-4fa9-b573-8198a4d3bf24",
    "input": "## Claim\nHere is a claim: The relative improvement averaged over all tasks is 8%. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "8b062e9b-8f83-4dd6-b370-1a476c743858",
    "input": "## Claim\nHere is a claim: The HAN models outperform MEAD in terms of sentence prediction. Does the following context support or refute the claim?\n\n## Table\nPaper title: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks\nTable caption: Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\n[BOLD] System | [BOLD] ROUGE-1  [BOLD] R (%) | [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%) | [BOLD] ROUGE-2  [BOLD] R (%) | [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%) | [BOLD] Sentence-Level  [BOLD] R (%) | [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)\n[BOLD] ILP | 24.5 | 41.1 | 29.3\u00b10.5 | 7.9 | 15.0 | 9.9\u00b10.5 | 13.6 | 22.6 | 15.6\u00b10.4\n[BOLD] Sum-Basic | 28.4 | 44.4 | 33.1\u00b10.5 | 8.5 | 15.6 | 10.4\u00b10.4 | 14.7 | 22.9 | 16.7\u00b10.5\n[BOLD] KL-Sum | 39.5 | 34.6 | 35.5\u00b10.5 | 13.0 | 12.7 | 12.3\u00b10.5 | 15.2 | 21.1 | 16.3\u00b10.5\n[BOLD] LexRank | 42.1 | 39.5 | 38.7\u00b10.5 | 14.7 | 15.3 | 14.2\u00b10.5 | 14.3 | 21.5 | 16.0\u00b10.5\n[BOLD] MEAD | 45.5 | 36.5 | 38.5\u00b1 0.5 | 17.9 | 14.9 | 15.4\u00b10.5 | 27.8 | 29.2 | 26.8\u00b10.5\n[BOLD] SVM | 19.0 | 48.8 | 24.7\u00b10.8 | 7.5 | 21.1 | 10.0\u00b10.5 | 32.7 | 34.3 | 31.4\u00b10.4\n[BOLD] LogReg | 26.9 | 34.5 | 28.7\u00b10.6 | 6.4 | 9.9 | 7.3\u00b10.4 | 12.2 | 14.9 | 12.7\u00b10.5\n[BOLD] LogReg [ITALIC] r | 28.0 | 34.8 | 29.4\u00b10.6 | 6.9 | 10.4 | 7.8\u00b10.4 | 12.1 | 14.5 | 12.5\u00b10.5\n[BOLD] HAN | 31.0 | 42.8 | 33.7\u00b10.7 | 11.2 | 17.8 | 12.7\u00b10.5 | 26.9 | 34.1 | 32.4\u00b10.5\n[BOLD] HAN+pretrainT | 32.2 | 42.4 | 34.4\u00b10.7 | 11.5 | 17.5 | 12.9\u00b10.5 | 29.6 | 35.8 | 32.2\u00b10.5\n[BOLD] HAN+pretrainU | 32.1 | 42.1 | 33.8\u00b10.7 | 11.6 | 17.6 | 12.9\u00b10.5 | 30.1 | 35.6 | 32.3\u00b10.5\n[BOLD] HAN [ITALIC] r | 38.1 | 40.5 | [BOLD] 37.8\u00b10.5 | 14.0 | 17.1 | [BOLD] 14.7\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainT [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.5 | 16.8 | [BOLD] 14.4\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainU [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.6 | 16.9 | [BOLD] 14.4\u00b10.5 | 33.9 | 33.8 | [BOLD] 33.8\u00b10.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5e5f4e5e-fc27-408f-92a5-1d9b04571613",
    "input": "## Claim\nHere is a claim: the question attention mechanism performs very poorly on out-of-domain questions, and shows no relative improvement when enhanced with attention over the span representations. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "782d2043-8a43-4dc8-9989-3d1e544a66c8",
    "input": "## Claim\nHere is a claim: Despite joint training, our hybrid model does not learn to pick up the best features from CBOW and CMOW simultaneously. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "dc098fdc-48d1-455f-933a-9d30abd36dfe",
    "input": "## Claim\nHere is a claim: our model is able to reduce the gap between the within-document and cross-document entity coreference metrics on the ECB+ Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n[BOLD] Model | R | MUC P | [ITALIC] F1 | R | B3 P | [ITALIC] F1 | R | CEAF- [ITALIC] e P | [ITALIC] F1 | CoNLL  [ITALIC] F1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | [BOLD] 71.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f161e252-7353-46ef-97f7-408e240403cf",
    "input": "## Claim\nHere is a claim: In Table 2, we can see that our capsule-based approach does not bring a noticeable margin over the strong baselines on EUR-Lex, and only competitive results on RCV1. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Scalable and Reliable Capsule Networksfor Challenging NLP Applications\nTable caption: Table 2: Comparisons of our NLP-Cap approach and baselines on two text classification benchmarks, where \u2019-\u2019 denotes methods that failed to scale due to memory issues.\n<bold>Datasets</bold> | <bold>Metrics</bold> | <bold>FastXML</bold> | <bold>PD-Sparse</bold> | <bold>FastText</bold> | <bold>Bow-CNN</bold> | <bold>CNN-Kim</bold> | <bold>XML-CNN</bold> | <bold>Cap-Zhao</bold> | <bold>NLP-Cap</bold> | <bold>Impv</bold>\nRCV1 | PREC@1 | 94.62 | 95.16 | 95.40 | 96.40 | 93.54 | 96.86 | 96.63 | <bold>97.05</bold> | +0.20%\nRCV1 | PREC@3 | 78.40 | 79.46 | 79.96 | 81.17 | 76.15 | 81.11 | 81.02 | <bold>81.27</bold> | +0.20%\nRCV1 | PREC@5 | 54.82 | 55.61 | 55.64 | <bold>56.74</bold> | 52.94 | 56.07 | 56.12 | 56.33 | -0.72%\n[EMPTY] | NDCG@1 | 94.62 | 95.16 | 95.40 | 96.40 | 93.54 | 96.88 | 96.63 | <bold>97.05</bold> | +0.20%\n[EMPTY] | NDCG@3 | 89.21 | 90.29 | 90.95 | 92.04 | 87.26 | 92.22 | 92.31 | <bold>92.47</bold> | +0.17%\n[EMPTY] | NDCG@5 | 90.27 | 91.29 | 91.68 | 92.89 | 88.20 | 92.63 | 92.75 | <bold>93.11</bold> | +0.52%\nEUR-Lex | PREC@1 | 68.12 | 72.10 | 71.51 | 64.99 | 68.35 | 75.65 | - | <bold>80.20</bold> | +6.01%\nEUR-Lex | PREC@3 | 57.93 | 57.74 | 60.37 | 51.68 | 54.45 | 61.81 | - | <bold>65.48</bold> | +5.93%\nEUR-Lex | PREC@5 | 48.97 | 47.48 | 50.41 | 42.32 | 44.07 | 50.90 | - | <bold>52.83</bold> | +3.79%\n[EMPTY] | NDCG@1 | 68.12 | 72.10 | 71.51 | 64.99 | 68.35 | 75.65 | - | <bold>80.20</bold> | +6.01%\n[EMPTY] | NDCG@3 | 60.66 | 61.33 | 63.32 | 55.03 | 59.81 | 66.71 | - | <bold>71.11</bold> | +6.59%\n[EMPTY] | NDCG@5 | 56.42 | 55.93 | 58.56 | 49.92 | 57.99 | 64.45 | - | <bold>68.80</bold> | +6.75%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a2dc6dbd-cd04-49eb-b388-2103d2295958",
    "input": "## Claim\nHere is a claim: And it lacks the performance when compared with the more sophisticated RL algorithms. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6d1edb41-733b-4b4f-9eee-201b52781e80",
    "input": "## Claim\nHere is a claim: In most cases the racial disparities persist, although they are generally smaller in magnitude and in some cases the direction even changes. Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 2: Experiment 1\nDataset | Class | \u02c6 [ITALIC] piblack | \u02c6 [ITALIC] piwhite | [ITALIC] t | [ITALIC] p | \u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite\n[ITALIC] Waseem and Hovy | Racism | 0.001 | 0.003 | -20.818 | *** | 0.505\n[EMPTY] | Sexism | 0.083 | 0.048 | 101.636 | *** | 1.724\n[ITALIC] Waseem | Racism | 0.001 | 0.001 | 0.035 | [EMPTY] | 1.001\n[EMPTY] | Sexism | 0.023 | 0.012 | 64.418 | *** | 1.993\n[EMPTY] | Racism and sexism | 0.002 | 0.001 | 4.047 | *** | 1.120\n[ITALIC] Davidson et al. | Hate | 0.049 | 0.019 | 120.986 | *** | 2.573\n[EMPTY] | Offensive | 0.173 | 0.065 | 243.285 | *** | 2.653\n[ITALIC] Golbeck et al. | Harassment | 0.032 | 0.023 | 39.483 | *** | 1.396\n[ITALIC] Founta et al. | Hate | 0.111 | 0.061 | 122.707 | *** | 1.812\n[EMPTY] | Abusive | 0.178 | 0.080 | 211.319 | *** | 2.239\n[EMPTY] | Spam | 0.028 | 0.015 | 63.131 | *** | 1.854\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3b3489c6-cb2f-4a1a-b462-ec4844bcfbf2",
    "input": "## Claim\nHere is a claim: The interpolation weight \u03b1 for the late fusion experiments is low when innovations are used, which further indicates that innovation features are not useful in overall prediction. Does the following context support or refute the claim?\n\n## Table\nPaper title: Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection\nTable caption: Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. \u201cRaw\u201d indicates the usage of original prosodic features (Section 3.2), while \u201cinnovations\u201d indicate the usage of innovation features (Section 3.3).\n[EMPTY] | [BOLD] Model | [BOLD] dev mean | [BOLD] dev best | [BOLD] test mean | [BOLD] test best | [ITALIC] \u03b1\nsingle | text | 86.54 | 86.80 | 86.47 | 86.96 | \u2013\nsingle | raw | 35.00 | 37.33 | 35.78 | 37.70 | \u2013\nsingle | innovations | 80.86 | 81.51 | 80.28 | 82.15 | \u2013\nearly | text + raw | 86.46 | 86.65 | 86.24 | 86.53 | \u2013\nearly | text + innovations | 86.53 | 86.77 | 86.54 | 87.00 | \u2013\nearly | text + raw + innovations | 86.35 | 86.69 | 86.55 | 86.44 | \u2013\nlate | text + raw | 86.71 | 87.05 | 86.35 | 86.71 | 0.2\nlate | text + innovations | [BOLD] 86.98 | [BOLD] 87.48 | [BOLD] 86.68 | [BOLD] 87.02 | 0.5\nlate | text + raw + innovations | 86.95 | 87.30 | 86.60 | 86.87 | 0.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "8240b880-6e23-4e88-b9ab-e8183400245c",
    "input": "## Claim\nHere is a claim: we see that the average ROUGE-L of the RL systems is similar to the supervised models (e.g., Zhang et\\xa0al. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c3215ba9-f925-4d0e-9090-74e533d5c180",
    "input": "## Claim\nHere is a claim: As expected, in both languages, the difference between the average of the two sets with the debiased embeddings is much higher. Does the following context support or refute the claim?\n\n## Table\nPaper title: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?\nTable caption: Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. \u201cReduction\u201d stands for gap reduction when removing gender signals from the context.\n[EMPTY] | Italian Original | Italian Debiased | Italian English | Italian Reduction | German Original | German Debiased | German English | German Reduction\nSame Gender | 0.442 | 0.434 | 0.424 | \u2013 | 0.491 | 0.478 | 0.446 | \u2013\nDifferent Gender | 0.385 | 0.421 | 0.415 | \u2013 | 0.415 | 0.435 | 0.403 | \u2013\ndifference | 0.057 | 0.013 | 0.009 | [BOLD] 91.67% | 0.076 | 0.043 | 0.043 | [BOLD] 100%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a5f84823-7cbf-4c56-9c4c-a5c245145720",
    "input": "## Claim\nHere is a claim: On the same dataset, our results are not as competitive as Damonte and Cohen (2019). Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ce51a0ff-e0da-4d8c-84db-e4cabdbaac82",
    "input": "## Claim\nHere is a claim: It does not match the performance of ORACLE, with a difference of up to 6.29% absolute difference. Does the following context support or refute the claim?\n\n## Table\nPaper title: Deriving Machine Attention from Human Rationales\nTable caption: Table 3: Accuracy of transferring between aspects. Models with \u2020 use labeled data from source aspects. Models with \u2021 use human rationales on the target aspect.\nSource | Target | Svm | Ra-Svm\u2021 | Ra-Cnn\u2021 | Trans\u2020 | Ra-Trans\u2021\u2020 | Ours\u2021\u2020 | Oracle\u2020\nBeer aroma+palate | Beer look | 74.41 | 74.83 | 74.94 | 72.75 | 76.41 | [BOLD] 79.53 | 80.29\nBeer look+palate | Beer aroma | 68.57 | 69.23 | 67.55 | 69.92 | 76.45 | [BOLD] 77.94 | 78.11\nBeer look+aroma | Beer palate | 63.88 | 67.82 | 65.72 | 74.66 | 73.40 | [BOLD] 75.24 | 75.50\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "eb9d6e8f-d389-49e1-a146-61202625fda6",
    "input": "## Claim\nHere is a claim: The results in the table suggest that cleaning the missing slots did not provide more complex training examples. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Cleaned | TGen\u2212 | 36.85 | 5.3782 | 35.14 | 55.01 | 1.6016 | 00.34 | 09.81 | 00.15 | 10.31\nOriginal | [BOLD] Cleaned | TGen | 39.23 | 6.0217 | 36.97 | 55.52 | 1.7623 | 00.40 | 03.59 | 00.07 | 04.05\nOriginal | [BOLD] Cleaned | TGen+ | 40.25 | 6.1448 | 37.50 | 56.19 | 1.8181 | 00.21 | 01.99 | 00.05 | 02.24\nOriginal | [BOLD] Cleaned | SC-LSTM | 23.88 | 3.9310 | 32.11 | 39.90 | 0.5036 | 07.73 | 17.76 | 09.52 | 35.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen\u2212 | 40.19 | 6.0543 | 37.38 | 55.88 | 1.8104 | 00.17 | 01.31 | 00.25 | 01.72\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen | 40.73 | 6.1711 | 37.76 | 56.09 | 1.8518 | 00.07 | 00.72 | 00.08 | 00.87\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen+ | 40.51 | 6.1226 | 37.61 | 55.98 | 1.8286 | 00.02 | 00.63 | 00.06 | 00.70\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | SC-LSTM | 23.66 | 3.9511 | 32.93 | 39.29 | 0.3855 | 07.89 | 15.60 | 08.44 | 31.94\nCleaned missing | [BOLD] Cleaned | TGen\u2212 | 40.48 | 6.0269 | 37.26 | 56.19 | 1.7999 | 00.43 | 02.84 | 00.26 | 03.52\nCleaned missing | [BOLD] Cleaned | TGen | 41.57 | 6.2830 | 37.99 | 56.36 | 1.8849 | 00.37 | 01.40 | 00.09 | 01.86\nCleaned missing | [BOLD] Cleaned | TGen+ | 41.56 | 6.2700 | 37.94 | 56.38 | 1.8827 | 00.21 | 01.04 | 00.07 | 01.31\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen\u2212 | 35.99 | 5.0734 | 34.74 | 54.79 | 1.5259 | 00.02 | 11.58 | 00.02 | 11.62\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen | 40.07 | 6.1243 | 37.45 | 55.81 | 1.8026 | 00.05 | 03.23 | 00.01 | 03.29\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+ | 40.80 | 6.2197 | 37.86 | 56.13 | 1.8422 | 00.01 | 01.87 | 00.01 | 01.88\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2eaca02a-6756-46bf-99d2-d597218b717d",
    "input": "## Claim\nHere is a claim: This indicates that GINs cannot be employed in tasks where the distribution of node degrees has a long tail. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "bff39859-b6c2-4366-92af-299cc4df3f54",
    "input": "## Claim\nHere is a claim: despite such simplification, our system consistently outperforms the two baselines by a wide margin across all three evaluation criteria. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "fb295289-5470-4bd0-99a4-18c93946d800",
    "input": "## Claim\nHere is a claim: The coverage mechanism is also effective in our models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "1818df5d-04f4-44b0-8e69-fd87724f010c",
    "input": "## Claim\nHere is a claim: We observe that BERT trained on Balanced COPA is less sensitive to a few highly productive superficial cues than BERT trained on original COPA. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 7: Sensitivity of BERT-large to superficial cues identified in \u00a72 (unit: 10\u22122). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.\nCue | [ITALIC] SCOPA | [ITALIC] SB_COPA | Diff. | Prod.\nwoman | 7.98 | 4.84 | -3.14 | 0.25\nmother | 5.16 | 3.95 | -1.21 | 0.75\nwent | 6.00 | 5.15 | -0.85 | 0.73\ndown | 5.52 | 4.93 | -0.58 | 0.71\ninto | 4.07 | 3.51 | -0.56 | 0.40\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d588b47d-e4c7-49e6-8f76-0c5678b232ea",
    "input": "## Claim\nHere is a claim: Moreover, training on B-COPA improves performance on the Hard subset, both when training with all 1000 instances in B-COPA, and when matching the training size of the original COPA (500 instances, B-COPA 50%). Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large-FT | B-COPA | 74.5 (\u00b1 0.7) | 74.7 (\u00b1 0.4) | [BOLD] 74.4 (\u00b1 0.9)\nBERT-large-FT | B-COPA (50%) | 74.3 (\u00b1 2.2) | 76.8 (\u00b1 1.9) | 72.8 (\u00b1 3.1)\nBERT-large-FT | COPA | [BOLD] 76.5 (\u00b1 2.7) | [BOLD] 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5)\nRoBERTa-large-FT | B-COPA | [BOLD] 89.0 (\u00b1 0.3) | 88.9 (\u00b1 2.1) | [BOLD] 89.0 (\u00b1 0.8)\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (\u00b1 2.2) | 87.4 (\u00b1 1.1) | 85.4 (\u00b1 2.9)\nRoBERTa-large-FT | COPA | 87.7 (\u00b1 0.9) | [BOLD] 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a2014e4b-6a45-4619-8130-4052f27e2307",
    "input": "## Claim\nHere is a claim: However, the greatest performance increase is not seen for the last scenario, which suggests that the semantic features captured by embeddings cannot be improved with a reasonable selection of the lexical resource from which the concept wordgroups were derived. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VIII: Precision scores for the Semantic Analogy Test\nQuestions Subset | # of Questions Seen | GloVe | Word2Vec | Proposed\nAll | 8783 | 78.94 | 81.03 | 79.96\nAt least one | 1635 | 67.58 | 70.89 | 67.89\nconcept word | 1635 | 67.58 | 70.89 | 67.89\nAll concept words | 110 | 77.27 | 89.09 | 83.64\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "8b27ad92-0b0c-4813-94fc-d817b8f9837d",
    "input": "## Claim\nHere is a claim: [CONTINUE] However, simply pooling the data actually hurts predictive performance leading to a drop of more than 2 points in F1. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 7: Complaint prediction results using the original data set and distantly supervised data. All models are based on logistic regression with bag-of-word and Part-of-Speech tag features.\n[BOLD] Model | [BOLD] Acc | [BOLD] F1 | [BOLD] AUC\nMost Frequent Class | 64.2 | 39.1 | 0.500\nLR-All Features \u2013 Original Data | 80.5 | 78.0 | 0.873\nDist. Supervision + Pooling | 77.2 | 75.7 | 0.853\nDist. Supervision + EasyAdapt | [BOLD] 81.2 | [BOLD] 79.0 | [BOLD] 0.885\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "6580de06-3999-4498-95f2-ca6c5250638a",
    "input": "## Claim\nHere is a claim: The total number of words in the concatenated inputs is shorter than other MDS datasets, as those consist of 10 input documents, but larger than SDS datasets, as expected. Does the following context support or refute the claim?\n\n## Table\nPaper title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\nTable caption: Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.\n[BOLD] Dataset | [BOLD] # pairs | [BOLD] # words (doc) | [BOLD] # sents (docs) | [BOLD] # words (summary) | [BOLD] # sents (summary) | [BOLD] vocab size\nMulti-News | 44,972/5,622/5,622 | 2,103.49 | 82.73 | 263.66 | 9.97 | 666,515\nDUC03+04 | 320 | 4,636.24 | 173.15 | 109.58 | 2.88 | 19,734\nTAC 2011 | 176 | 4,695.70 | 188.43 | 99.70 | 1.00 | 24,672\nCNNDM | 287,227/13,368/11,490 | 810.57 | 39.78 | 56.20 | 3.68 | 717,951\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "74ffa35f-85bf-4c65-9d6a-082216a84d24",
    "input": "## Claim\nHere is a claim: When increasing the number of terms to 10,000, the DocSub models using TED Talks corpora performed better than when using Europarl corpora. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150 | 0.0150 | 0.0445 | 0.0326\nP | EN | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.0060 | 0.0092 | 0.0090 | 0.0356 | 0.0162\nP | PT | Europarl | 0.5710 | 0.1948 | 0.3855 | 0.5474 | 0.4485 | [BOLD] 0.8052 | 0.4058\n[EMPTY] | PT | Ted Talks | [BOLD] 0.6304 | 0.1870 | 0.3250 | 0.5312 | 0.4576 | 0.6064 | 0.3698\nR | EN | Europarl | 0.0037 | 0.3278 | 0.5941 | 0.6486 | [BOLD] 0.6490 | 0.0017 | 0.0003\nR | EN | Ted Talks | 0.0002 | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332 | 0.0967 | 0.0003\nR | PT | Europarl | 0.0002 | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932 | 0.0032 | 0.0001\n[EMPTY] | PT | Ted Talks | 2.10-5 | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887 | 0.1390 | 0.0002\nF | EN | Europarl | 0.0073 | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033 | 0.0006\nF | EN | Ted Talks | 0.0004 | 0.0132 | 0.0118 | 0.0181 | 0.0179 | [BOLD] 0.0520 | 0.0005\nF | PT | Europarl | 0.0005 | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109 | 0.0064 | 0.0002\n[EMPTY] | PT | Ted Talks | 4.10-5 | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149 | 0.2261 | 0.0004\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "76767541-0820-44a2-9a66-24bea826ecca",
    "input": "## Claim\nHere is a claim: Our model improves the results in the translation tasks. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Vector-spaces with Noisy Supervised Lexicons\nTable caption: Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En\u2192De, En\u2192Fi and En\u2192Es improvements are significant at p<0.05 according to ANOVA on the different runs.\nMethod | En\u2192It best | En\u2192It avg | En\u2192It iters | En\u2192De best | En\u2192De avg | En\u2192De iters | En\u2192Fi best | En\u2192Fi avg | En\u2192Fi iters | En\u2192Es best | En\u2192Es avg | En\u2192Es iters\nArtetxe et\u00a0al., 2018b | [BOLD] 48.53 | 48.13 | 573 | 48.47 | 48.19 | 773 | 33.50 | 32.63 | 988 | 37.60 | 37.33 | 808\nNoise-aware Alignment | [BOLD] 48.53 | [BOLD] 48.20 | 471 | [BOLD] 49.67 | [BOLD] 48.89 | 568 | [BOLD] 33.98 | [BOLD] 33.68 | 502 | [BOLD] 38.40 | [BOLD] 37.79 | 551\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2d765333-cfa9-4b2f-89be-59b5d90a8066",
    "input": "## Claim\nHere is a claim: we observe that our system\u2019s summaries are preferred by humans more than the competitor systems\u2019 in terms of readability and the coherence between passages, indicating the superiority of our system Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "02552450-1723-479d-b04b-eace6056bf93",
    "input": "## Claim\nHere is a claim: analogy can be well dealt with and we obtain a precisio score higher than all methods except for Word2Vec Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "41907607-9691-409a-9a0a-517d74061500",
    "input": "## Claim\nHere is a claim: The full model does not give the best performance on the AMR15 dev set. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\n-{4} dense block | 24.8 | 54.9\n-{3, 4} dense blocks | 23.8 | 54.1\n-{2, 3, 4} dense blocks | 23.2 | 53.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "050de577-add7-44d3-9d2c-7892c25a8464",
    "input": "## Claim\nHere is a claim: [CONTINUE] Although the PRKGC model do not receive supervision about human-generated NLDs, paths with the maximum score match human-generated NLDs to some extent. Does the following context support or refute the claim?\n\n## Table\nPaper title: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension\nTable caption: Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).\nModel | Answerability Macro P/R/F | # Answerable | Answer Prec. | Derivation Prec. RG-L (P/R/F) | Derivation Prec. BL-4\nShortest Path | 54.8/55.5/53.2 | 976 | 3.6 | 56.7/38.5/41.5 | 31.3\nPRKGC | 52.6/51.5/50.7 | 1,021 | 45.2 | 40.7/60.7/44.7 | 30.9\nPRKGC+NS | 53.6/54.1/52.1 | 980 | 45.4 | 42.2/61.6/46.1 | 33.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "e37522d9-013e-410c-bf5b-57c5a6ad3d60",
    "input": "## Claim\nHere is a claim: The improvement from automatic AMR to gold AMR (+0.7 BLEU) is not significant, which shows that the translation quality of our model cannot be further improved with an increase of AMR parsing accuracy. Does the following context support or refute the claim?\n\n## Table\nPaper title: Semantic Neural Machine Translation using AMR\nTable caption: Table 4: BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.\nAMR Anno. | BLEU\nAutomatic | 16.8\nGold | [BOLD] *17.5*\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "de8ef478-5e2b-4df6-90f3-1e34c08c069a",
    "input": "## Claim\nHere is a claim: In general, the performance of the model drops substantially as we remove more dense connections until it cannot converge without dense connections. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\n-{4} dense block | 24.8 | 54.9\n-{3, 4} dense blocks | 23.8 | 54.1\n-{2, 3, 4} dense blocks | 23.2 | 53.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "67898573-d093-4de1-90ce-1a972b3794a9",
    "input": "## Claim\nHere is a claim: Table 6 shows that our system does not outperform the best previous approaches across the five languages. Does the following context support or refute the claim?\n\n## Table\nPaper title: Language Independent Sequence Labelling for Opinion Target Extraction\nTable caption: Table 6: ABSA SemEval 2016: Comparison of multilingual results in terms of F1 scores.\nLanguage | System | F1\nes | GTI | 68.51\nes | L +  [BOLD] CW600 + W2VW300 | [BOLD] 69.92\nes | Baseline | 51.91\nfr | IIT-T | 66.67\nfr | L +  [BOLD] CW100 | [BOLD] 69.50\nfr | Baseline | 45.45\nnl | IIT-T | 56.99\nnl | L +  [BOLD] W2VW400 | [BOLD] 66.39\nnl | Baseline | 50.64\nru | Danii. | 33.47\nru | L +  [BOLD] CW500 | [BOLD] 65.53\nru | Baseline | 49.31\ntr | L +  [BOLD] BW | [BOLD] 60.22\ntr | Baseline | 41.86\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "991cd154-0a60-4089-995d-b01bf6f79cd8",
    "input": "## Claim\nHere is a claim: The performance gap between our HDSA and DAMD grows as the number of actions increases due to the repetitive action problem in DAMD, which is also supported by the performance of our HDSA with a sampled threshold Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f1e358e6-d02a-42cc-ab2d-acab9c445b37",
    "input": "## Claim\nHere is a claim: the proposed RL approach allows to create rewards without a summary-level information, thus achieving summary evaluation metrics with greater consistency. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "2d186228-c158-4640-a457-4d3b8f8ee6b0",
    "input": "## Claim\nHere is a claim: in summary, GDPL can learn useful dialogue skills from internal and external data, and the learned dialogue policy outperforms baseline methods in all three criteria. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6b4c6197-f12f-490f-a908-1dd581078e9f",
    "input": "## Claim\nHere is a claim: our BERT-Cosine metric is the most effective at ranking \u201cgood\u201d summaries Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nMetric | [ITALIC] \u03c1 | [ITALIC] r | G-Pre | G-Rec\nROUGE-1 | .290 | .304 | .392 | .428\nROUGE-2 | .259 | .278 | .408 | .444\nROUGE-L | .274 | .297 | .390 | .426\nROUGE-SU4 | .282 | .279 | .404 | .440\nBLEU-1 | .256 | .281 | .409 | .448\nBLEU-2 | .301 | .312 | .411 | .446\nBLEU-3 | .317 | .312 | .409 | .444\nBLEU-4 | .311 | .307 | .409 | .446\nBLEU-5 | .308 | .303 | .420 | .459\nMETEOR | .305 | .285 | .409 | .444\nInferSent-Cosine | [BOLD] .329 | [BOLD] .339 | .417 | .460\nBERT-Cosine | .312 | .335 | [BOLD] .440 | [BOLD] .484\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0945c766-61d1-41b1-96ea-8577746e2651",
    "input": "## Claim\nHere is a claim: For example, on Yelp, large differences in human judgments of semantic preservation (M2>M0, M7>M0, M7>M2) also show the largest differences in Sim, while M6 and M7 have very similar human judgments and very similar Sim scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer\nTable caption: Table 4: Manual evaluation results (%) using models from Table\u00a02 (i.e., with roughly fixed Acc). > means \u201cbetter than\u201d. \u0394Sim=Sim(A)\u2212Sim(B), and \u0394PP=PP(A)\u2212PP(B) (note that lower PP generally means better fluency). Each row uses at least 120 sentence pairs. A cell is bold if it represents a model win of at least 10%.\nDataset | Models A | Models B | Transfer quality A>B | Transfer quality B>A | Transfer quality Tie | Semantic preservation A>B | Semantic preservation B>A | Semantic preservation Tie | Semantic preservation \u0394Sim | Fluency A>B | Fluency B>A | Fluency Tie | Fluency \u0394PP\n[EMPTY] | M0 | M2 | 9.0 | 6.0 | 85.1 | 1.5 | [BOLD] 25.4 | 73.1 | -0.05 | 10.4 | [BOLD] 23.9 | 65.7 | 0.9\nYelp | M0 | M7 | 9.6 | 14.7 | 75.8 | 2.5 | [BOLD] 54.5 | 42.9 | -0.09 | 4.6 | [BOLD] 39.4 | 56.1 | 8.3\nYelp | M6 | M7 | 13.7 | 11.6 | 74.7 | 16.0 | 16.7 | 67.4 | 0.01 | 10.3 | 20.0 | 69.7 | 14.3\n[EMPTY] | M2 | M7 | 5.8 | 9.3 | 84.9 | 8.1 | [BOLD] 25.6 | 66.3 | -0.04 | 14.0 | [BOLD] 26.7 | 59.3 | 7.4\nLiterature | M2 | M6 | 4.2 | 6.7 | 89.2 | 16.7 | 20.8 | 62.5 | 0.01 | [BOLD] 40.8 | 13.3 | 45.8 | -13.3\nLiterature | M6 | M7 | 15.8 | 13.3 | 70.8 | [BOLD] 25.0 | 9.2 | 65.8 | 0.03 | 14.2 | 20.8 | 65.0 | 14.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "e500eb41-5d94-4380-b1a6-2df603c661c0",
    "input": "## Claim\nHere is a claim: The results for the Portuguese corpora are quite different from the ones generated by the English corpora, with terms without relations in Patt and DocSub, and DSim, SLQS, TF and DF generating shallow taxonomies, disproving the characteristics of each method. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 7: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in Portuguese.\nCorpus | Metric | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nEuroparl | TotalTerms: | 980 | 1,000 | 1,000 | 1,000 | 1,000 | 996 | 1,000\nEuroparl | TotalRoots: | 79 | 1 | 1 | 1 | 1 | 1 | 1\nEuroparl | NumberRels: | 1,527 | 1,031 | 1,049 | 1,185 | 1,093 | 1,644 | 999\nEuroparl | MaxDepth: | 19 | 902 | 894 | 784 | 849 | 6 | 10\nEuroparl | MinDepth: | 1 | 902 | 894 | 784 | 849 | 1 | 1\nEuroparl | AvgDepth: | 9.43 | 902 | 894 | 784 | 849 | 2.73 | 4.29\nEuroparl | DepthCohesion: | 2.02 | 1 | 1 | 1 | 1 | 2.19 | 2.33\nEuroparl | MaxWidth: | 27 | 3 | 3 | 4 | 3 | 201 | 58\nEuroparl | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nEuroparl | AvgWidth: | 1.98 | 1.03 | 1.05 | 1.19 | 1.09 | 6.25 | 2.55\nTED Talks | TotalTerms: | 296 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000\nTED Talks | TotalRoots: | 101 | 1 | 1 | 1 | 1 | 1 | 1\nTED Talks | NumberRels: | 291 | 1,045 | 1,229 | 3,637 | 4,284 | 2,875 | 999\nTED Talks | MaxDepth: | 10 | 860 | 727 | 388 | 354 | 252 | 17\nTED Talks | MinDepth: | 1 | 860 | 727 | 388 | 354 | 249 | 1\nTED Talks | AvgDepth: | 3.94 | 860 | 727 | 388 | 354 | 250.43 | 6.16\nTED Talks | DepthCohesion: | 2.54 | 1 | 1 | 1 | 1 | 1.01 | 2.76\nTED Talks | MaxWidth: | 37 | 3 | 79 | 18 | 13 | 9 | 41\nTED Talks | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nTED Talks | AvgWidth: | 1.79 | 1.05 | 1.23 | 3.64 | 4.29 | 2.94 | 2.37\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9a9e8bed-916f-41b2-b6c7-a9163e27d1ac",
    "input": "## Claim\nHere is a claim: We observe that the redundancy removal step is not necessary for the HAN models to achieve outstanding results. Does the following context support or refute the claim?\n\n## Table\nPaper title: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks\nTable caption: Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\n[BOLD] System | [BOLD] ROUGE-1  [BOLD] R (%) | [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%) | [BOLD] ROUGE-2  [BOLD] R (%) | [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%) | [BOLD] Sentence-Level  [BOLD] R (%) | [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)\n[BOLD] ILP | 24.5 | 41.1 | 29.3\u00b10.5 | 7.9 | 15.0 | 9.9\u00b10.5 | 13.6 | 22.6 | 15.6\u00b10.4\n[BOLD] Sum-Basic | 28.4 | 44.4 | 33.1\u00b10.5 | 8.5 | 15.6 | 10.4\u00b10.4 | 14.7 | 22.9 | 16.7\u00b10.5\n[BOLD] KL-Sum | 39.5 | 34.6 | 35.5\u00b10.5 | 13.0 | 12.7 | 12.3\u00b10.5 | 15.2 | 21.1 | 16.3\u00b10.5\n[BOLD] LexRank | 42.1 | 39.5 | 38.7\u00b10.5 | 14.7 | 15.3 | 14.2\u00b10.5 | 14.3 | 21.5 | 16.0\u00b10.5\n[BOLD] MEAD | 45.5 | 36.5 | 38.5\u00b1 0.5 | 17.9 | 14.9 | 15.4\u00b10.5 | 27.8 | 29.2 | 26.8\u00b10.5\n[BOLD] SVM | 19.0 | 48.8 | 24.7\u00b10.8 | 7.5 | 21.1 | 10.0\u00b10.5 | 32.7 | 34.3 | 31.4\u00b10.4\n[BOLD] LogReg | 26.9 | 34.5 | 28.7\u00b10.6 | 6.4 | 9.9 | 7.3\u00b10.4 | 12.2 | 14.9 | 12.7\u00b10.5\n[BOLD] LogReg [ITALIC] r | 28.0 | 34.8 | 29.4\u00b10.6 | 6.9 | 10.4 | 7.8\u00b10.4 | 12.1 | 14.5 | 12.5\u00b10.5\n[BOLD] HAN | 31.0 | 42.8 | 33.7\u00b10.7 | 11.2 | 17.8 | 12.7\u00b10.5 | 26.9 | 34.1 | 32.4\u00b10.5\n[BOLD] HAN+pretrainT | 32.2 | 42.4 | 34.4\u00b10.7 | 11.5 | 17.5 | 12.9\u00b10.5 | 29.6 | 35.8 | 32.2\u00b10.5\n[BOLD] HAN+pretrainU | 32.1 | 42.1 | 33.8\u00b10.7 | 11.6 | 17.6 | 12.9\u00b10.5 | 30.1 | 35.6 | 32.3\u00b10.5\n[BOLD] HAN [ITALIC] r | 38.1 | 40.5 | [BOLD] 37.8\u00b10.5 | 14.0 | 17.1 | [BOLD] 14.7\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainT [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.5 | 16.8 | [BOLD] 14.4\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainU [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.6 | 16.9 | [BOLD] 14.4\u00b10.5 | 33.9 | 33.8 | [BOLD] 33.8\u00b10.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1c6f6376-967e-4d24-a135-f6d6d35ab53a",
    "input": "## Claim\nHere is a claim: systems with hand-crafted (R-1/2/L) or automatic (R-1,2,L) rewards always have higher ROUGE scores compared with our system, which improves their performance in limited scenarios. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "86dee411-f4cc-4ea5-b221-e4b7a4877884",
    "input": "## Claim\nHere is a claim: It should also be noted that scores obtained by SPINE are relatively low on some tests, but still acceptable, indicating that it has achieved its interpretability performance without sacrificing its semantic functions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VI: Correlations for Word Similarity Tests\nDataset (EN-) | GloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\nWS-353-ALL | 0.612 | 0.7156 | 0.634 | 0.622 | 0.173 | 0.690 | 0.657\nSIMLEX-999 | 0.359 | 0.3939 | 0.295 | 0.355 | 0.090 | 0.380 | 0.381\nVERB-143 | 0.326 | 0.4430 | 0.255 | 0.271 | 0.293 | 0.271 | 0.348\nSimVerb-3500 | 0.193 | 0.2856 | 0.184 | 0.197 | 0.035 | 0.234 | 0.245\nWS-353-REL | 0.578 | 0.6457 | 0.595 | 0.578 | 0.134 | 0.695 | 0.619\nRW-STANF. | 0.378 | 0.4858 | 0.316 | 0.373 | 0.122 | 0.390 | 0.382\nYP-130 | 0.524 | 0.5211 | 0.353 | 0.482 | 0.169 | 0.420 | 0.589\nMEN-TR-3k | 0.710 | 0.7528 | 0.684 | 0.696 | 0.298 | 0.769 | 0.725\nRG-65 | 0.768 | 0.8051 | 0.736 | 0.732 | 0.338 | 0.761 | 0.774\nMTurk-771 | 0.650 | 0.6712 | 0.593 | 0.623 | 0.199 | 0.665 | 0.671\nWS-353-SIM | 0.682 | 0.7883 | 0.713 | 0.702 | 0.220 | 0.720 | 0.720\nMC-30 | 0.749 | 0.8112 | 0.799 | 0.726 | 0.330 | 0.735 | 0.776\nMTurk-287 | 0.649 | 0.6645 | 0.591 | 0.631 | 0.295 | 0.674 | 0.634\nAverage | 0.552 | 0.6141 | 0.519 | 0.538 | 0.207 | 0.570 | 0.579\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6fa0512b-21f4-4bd1-86eb-c649baf8805f",
    "input": "## Claim\nHere is a claim: [CONTINUE] Table 6 summarize the results, implying that the leakage is caused mainly by the RNN, and less by the Embedding Matrix. Does the following context support or refute the claim?\n\n## Table\nPaper title: Adversarial Removal of Demographic Attributes from Text Data\nTable caption: Table 6: Accuracies of the protected attribute with different encoders.\n[EMPTY] | [EMPTY] | Embedding Leaky | Embedding Guarded\nRNN | Leaky | 64.5 | 67.8\nRNN | Guarded | 59.3 | 54.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a1404ba9-ad11-4c99-803f-71fc990d1c07",
    "input": "## Claim\nHere is a claim: Overall, predictive performance is high across all domains, with the exception of transport. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 8: Performance of models in Macro F1 on tweets from each domain.\n[BOLD] Domain | [BOLD] In-Domain | [BOLD] Pooling | [BOLD] EasyAdapt\nFood & Beverage | 63.9 | 60.9 | [BOLD] 83.1\nApparel | [BOLD] 76.2 | 71.1 | 72.5\nRetail | 58.8 | [BOLD] 79.7 | [BOLD] 79.7\nCars | 41.5 | 77.8 | [BOLD] 80.9\nServices | 65.2 | 75.9 | [BOLD] 76.7\nSoftware | 61.3 | 73.4 | [BOLD] 78.7\nTransport | 56.4 | [BOLD] 73.4 | 69.8\nElectronics | 66.2 | 73.0 | [BOLD] 76.2\nOther | 42.4 | [BOLD] 82.8 | [BOLD] 82.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "905c2475-f7a6-4b0f-aeca-1ba199c9fd50",
    "input": "## Claim\nHere is a claim: According to Pearson correlation, gr cbow def model had the highest correlation with human ratings of similarity. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluation of Greek Word Embeddings\nTable caption: Table 4: Word similarity.\nModel | Pearson | p-value | Pairs (unknown)\ngr_def | [BOLD] 0.6042 | 3.1E-35 | 2.3%\ngr_neg10 | 0.5973 | 2.9E-34 | 2.3%\ncc.el.300 | 0.5311 | 1.7E-25 | 4.9%\nwiki.el | 0.5812 | 2.2E-31 | 4.5%\ngr_cbow_def | 0.5232 | 2.7E-25 | 2.3%\ngr_d300_nosub | 0.5889 | 3.8E-33 | 2.3%\ngr_w2v_sg_n5 | 0.5879 | 4.4E-33 | 2.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5a1d0c5b-836f-4eef-85b7-ff4f0e532907",
    "input": "## Claim\nHere is a claim: Also, the average human rating for Refresh is significantly higher (p (cid:28) 0.01) than ExtAbsRL, Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "25fb709f-c1cf-4153-b8f6-672f1f494623",
    "input": "## Claim\nHere is a claim: These results do not use the best performing KnowComb system. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.\nSchema | AntePre(Test) | AntePre(Train)\nType 1 | 76.67 | 86.79\nType 2 | 79.55 | 88.86\nType 1 (Cat1) | 90.26 | 93.64\nType 2 (Cat2) | 83.38 | 92.49\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "768f70ec-749a-408f-a097-279e7b07e70f",
    "input": "## Claim\nHere is a claim: We observe that PCNN+ATT (1) exhibits the best performances. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 2: Precisions on the Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nRank+ExATT | 0.584 | 0.535 | 0.487 | 0.392\nPCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204\nPCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396\nOur Model | 0.650 | 0.519 | 0.422 | [BOLD] 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "7df38698-8364-4d29-b26e-8e8631b92458",
    "input": "## Claim\nHere is a claim: [CONTINUE] Relation propagation (RelProp) improves relation extraction performance over pretrained BERT, but does not improve fine-tuned BERT. Does the following context support or refute the claim?\n\n## Table\nPaper title: Entity, Relation, and Event Extraction with Contextualized Span Representations\nTable caption: Table 3: F1 scores on Relation.\n[EMPTY] | ACE05 | SciERC | WLPC\nBERT + LSTM | 60.6 | 40.3 | 65.1\n+RelProp | 61.9 | 41.1 | 65.3\n+CorefProp | 59.7 | 42.6 | -\nBERT FineTune | [BOLD] 62.1 | 44.3 | 65.4\n+RelProp | 62.0 | 43.0 | [BOLD] 65.5\n+CorefProp | 60.0 | [BOLD] 45.3 | -\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "4cc9d2c1-a18e-47de-aa39-4909ec936ba9",
    "input": "## Claim\nHere is a claim: [CONTINUE] In the exceptional case of \"Hydroelectric Dams\" dataset, the opinion distance OD performs particularly bad compared to TF-IDF Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.\nTopic Name | Size | TF-IDF ARI | WMD ARI | Sent2vec ARI | Doc2vec ARI | BERT ARI | [ITALIC] OD-w2v ARI | [ITALIC] OD-d2v ARI | TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil. | Sent2vec  [ITALIC] Sil. | Doc2vec  [ITALIC] Sil. | BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil. | [ITALIC] OD-d2v  [ITALIC] Sil.\nAffirmative Action | 81 | -0.07 | -0.02 | 0.03 | -0.01 | -0.02 | [BOLD] 0.14 | [ITALIC] 0.02 | 0.01 | 0.01 | -0.01 | -0.02 | -0.04 | [BOLD] 0.06 | [ITALIC] 0.01\nAtheism | 116 | [BOLD] 0.19 | 0.07 | 0.00 | 0.03 | -0.01 | 0.11 | [ITALIC] 0.16 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | [ITALIC] 0.05 | [BOLD] 0.07\nAusterity Measures | 20 | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01 | -0.05 | 0.04 | [BOLD] 0.21 | -0.01 | 0.06 | 0.07 | 0.05 | -0.03 | 0.10 | [BOLD] 0.19 | 0.1\nDemocratization | 76 | 0.02 | -0.01 | 0.00 | [ITALIC] 0.09 | -0.01 | [BOLD] 0.11 | 0.07 | 0.01 | 0.01 | 0.02 | 0.02 | 0.03 | [BOLD] 0.16 | [ITALIC] 0.11\nEducation Voucher Scheme | 30 | [BOLD] 0.25 | 0.12 | 0.08 | -0.02 | 0.04 | 0.13 | [ITALIC] 0.19 | 0.01 | 0.01 | 0.01 | -0.01 | 0.02 | [ITALIC] 0.38 | [BOLD] 0.40\nGambling | 60 | -0.06 | -0.01 | -0.02 | 0.04 | 0.09 | [ITALIC] 0.35 | [BOLD] 0.39 | 0.01 | 0.02 | 0.03 | 0.01 | 0.09 | [BOLD] 0.30 | [ITALIC] 0.22\nHousing | 30 | 0.01 | -0.01 | -0.01 | -0.02 | 0.08 | [BOLD] 0.27 | 0.01 | 0.02 | 0.03 | 0.03 | 0.01 | 0.11 | [BOLD] 0.13 | [ITALIC] 0.13\nHydroelectric Dams | 110 | [BOLD] 0.47 | [ITALIC] 0.45 | [ITALIC] 0.45 | -0.01 | 0.38 | 0.35 | 0.14 | 0.04 | 0.08 | 0.12 | 0.01 | 0.19 | [BOLD] 0.26 | [ITALIC] 0.09\nIntellectual Property | 66 | 0.01 | 0.01 | 0.00 | 0.03 | 0.03 | [ITALIC] 0.05 | [BOLD] 0.14 | 0.01 | [ITALIC] 0.04 | 0.03 | 0.01 | 0.03 | [ITALIC] 0.04 | [BOLD] 0.12\nKeystone pipeline | 18 | 0.01 | 0.01 | 0.00 | -0.13 | [BOLD] 0.07 | -0.01 | [BOLD] 0.07 | -0.01 | -0.03 | -0.03 | -0.07 | 0.03 | [BOLD] 0.05 | [ITALIC] 0.02\nMonarchy | 61 | -0.04 | 0.01 | 0.00 | 0.03 | -0.02 | [BOLD] 0.15 | [BOLD] 0.15 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 | [BOLD] 0.11 | [ITALIC] 0.09\nNational Service | 33 | 0.14 | -0.03 | -0.01 | 0.02 | 0.01 | [ITALIC] 0.31 | [BOLD] 0.39 | 0.02 | 0.04 | 0.02 | 0.01 | 0.02 | [BOLD] 0.25 | [BOLD] 0.25\nOne-child policy China | 67 | -0.05 | 0.01 | [BOLD] 0.11 | -0.02 | 0.02 | [BOLD] 0.11 | 0.01 | 0.01 | 0.02 | [ITALIC] 0.04 | -0.01 | 0.03 | [BOLD] 0.07 | -0.02\nOpen-source Software | 48 | -0.02 | -0.01 | [ITALIC] 0.05 | 0.01 | 0.12 | [BOLD] 0.09 | -0.02 | 0.01 | -0.01 | 0.00 | -0.02 | 0.03 | [BOLD] 0.18 | 0.01\nPornography | 52 | -0.02 | 0.01 | 0.01 | -0.02 | -0.01 | [BOLD] 0.41 | [BOLD] 0.41 | 0.01 | 0.01 | 0.02 | -0.01 | 0.03 | [BOLD] 0.47 | [ITALIC] 0.41\nSeanad Abolition | 25 | 0.23 | 0.09 | -0.01 | -0.01 | 0.03 | [ITALIC] 0.32 | [BOLD] 0.54 | 0.02 | 0.01 | -0.01 | -0.03 | -0.04 | [ITALIC] 0.15 | [BOLD] 0.31\nTrades Unions | 19 | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60 | -0.05 | 0.44 | [ITALIC] 0.44 | 0.29 | 0.1 | 0.17 | 0.21 | 0.01 | 0.26 | [BOLD] 0.48 | [ITALIC] 0.32\nVideo Games | 72 | -0.01 | 0.01 | 0.12 | 0.01 | 0.08 | [ITALIC] 0.40 | [BOLD] 0.56 | 0.01 | 0.01 | 0.06 | 0.01 | 0.05 | [ITALIC] 0.32 | [BOLD] 0.42\nAverage | 54.67 | 0.09 | 0.07 | 0.08 | 0.01 | 0.08 | [BOLD] 0.22 | [ITALIC] 0.20 | 0.02 | 0.03 | 0.04 | -0.01 | 0.05 | [BOLD] 0.20 | [ITALIC] 0.17\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c0ce7d66-9d67-4ba9-831c-145cdbef0f49",
    "input": "## Claim\nHere is a claim: [CONTINUE] The relative lower BLEU score [CONTINUE] Our DAMD model significantly outperforms other models with different system action forms in terms of inform and success rates, [CONTINUE] While we find applying our data augmentation achieves a limited improvement on combined score (6 vs 7), [CONTINUE] Moreover, if a model has access to ground truth system action, the model further improves its task performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "78a8bdad-2657-45d7-88ef-60e249c81ea8",
    "input": "## Claim\nHere is a claim: GPT-2, on the other hand, finetuned to a final accuracy of 91.20%, only a 0.61% improvement over the performance of ULMFiT. Does the following context support or refute the claim?\n\n## Table\nPaper title: Localization of Fake News Detection via Multitask Transfer Learning\nTable caption: Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.\nModel | Val. Accuracy | Loss | Val. Loss | Pretraining Time | Finetuning Time\nSiamese Networks | 77.42% | 0.5601 | 0.5329 | [EMPTY] | 4m per epoch\nBERT | 87.47% | 0.4655 | 0.4419 | 66 hours | 2m per epoch\nGPT-2 | 90.99% | 0.2172 | 0.1826 | 78 hours | 4m per epoch\nULMFiT | 91.59% | 0.3750 | 0.1972 | 11 hours | 2m per epoch\nULMFiT (no LM Finetuning) | 78.11% | 0.5512 | 0.5409 | 11 hours | 2m per epoch\nBERT + Multitasking | 91.20% | 0.3155 | 0.3023 | 66 hours | 4m per epoch\nGPT-2 + Multitasking | 96.28% | 0.2609 | 0.2197 | 78 hours | 5m per epoch\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9de1dc72-cd2d-4713-8dc5-57373c5607e1",
    "input": "## Claim\nHere is a claim: imparting named entities and events certainly yields considerable improvement in a word intrusion test. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0d745e8a-02cf-46dc-b3c2-a9dad265bded",
    "input": "## Claim\nHere is a claim: we can see that our proposed technique outperforms all other approaches including the attention model for sentiment classification task Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "05d0d281-1e18-4f77-932d-b89d635f6ca2",
    "input": "## Claim\nHere is a claim: This means that the cleaned dataset is more complex overall, with fewer references per MR and more diverse MRs. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 1: Data statistics comparison for the original E2E data and our cleaned version (number of distinct MRs, total number of textual references, SER as measured by our slot matching script, see Section\u00a03).\n[BOLD] Dataset | [BOLD] Part | [BOLD] MRs | [BOLD] Refs | [BOLD] SER(%)\nOriginal | Train | 4,862 | 42,061 | 17.69\nOriginal | Dev | 547 | 4,672 | 11.42\nOriginal | Test | 630 | 4,693 | 11.49\n[0.5pt/2pt] Cleaned | Train | 8,362 | 33,525 | (0.00)\n[0.5pt/2pt] Cleaned | Dev | 1,132 | 4,299 | (0.00)\n[0.5pt/2pt] Cleaned | Test | 1,358 | 4,693 | (0.00)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "4a080275-4fe7-4c3b-b13a-c21e16ec6d34",
    "input": "## Claim\nHere is a claim: The first set of results in Table 3 shows that the hierarchical right/left branching baselines dominate the completely right/left branching ones. Does the following context support or refute the claim?\n\n## Table\nPaper title: Predicting Discourse Structure using Distant Supervision from Sentiment\nTable caption: Table 3: Discourse structure prediction results; tested on RST-DTtest and Instr-DTtest. Subscripts in inter-domain evaluation sub-table indicate the training set. Best performance in the category is bold. Consistently best model for inter-domain discourse structure prediction is underlined\nApproach | RST-DTtest | Instr-DTtest\nRight Branching | 54.64 | 58.47\nLeft Branching | 53.73 | 48.15\nHier. Right Branch. | [BOLD] 70.82 | [BOLD] 67.86\nHier. Left Branch. | 70.58 | 63.49\n[BOLD] Intra-Domain Evaluation | [BOLD] Intra-Domain Evaluation | [BOLD] Intra-Domain Evaluation\nHILDAHernault et al. ( 2010 ) | 83.00 | \u2014\nDPLPJi and Eisenstein ( 2014 ) | 82.08 | \u2014\nCODRAJoty et al. ( 2015 ) | 83.84 | [BOLD] 82.88\nTwo-StageWang et al. ( 2017 ) | [BOLD] 86.00 | 77.28\n[BOLD] Inter-Domain Evaluation | [BOLD] Inter-Domain Evaluation | [BOLD] Inter-Domain Evaluation\nTwo-StageRST-DT | \u00d7 | 73.65\nTwo-StageInstr-DT | 74.48 | \u00d7\nTwo-StageOurs(avg) | 76.42 | [BOLD] 74.22\nTwo-StageOurs(max) | [BOLD] 77.24 | 73.12\nHuman Morey et al. ( 2017 ) | 88.30 | \u2014\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3512c69e-13aa-47de-8bd5-07e449d021a2",
    "input": "## Claim\nHere is a claim: this result shows the effectiveness of the causality-centric training objective and evaluation metrics in the COPA task; RoBERTa-large (finetuned) achieves substantial improvements (90.6%) over the previous state-of-the-art methods. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 1: Reported results on COPA. With the exception of Wang et al. (2019), BERT-large and RoBERTa-large yields substantial improvements over prior approaches. See \u00a72 for model details. * indicates our replication experiments.\nModel | Accuracy\nBigramPMI\u00a0Goodwin et al. ( 2012 ) | 63.4\nPMI\u00a0Gordon et al. ( 2011 ) | 65.4\nPMI+Connectives\u00a0Luo et al. ( 2016 ) | 70.2\nPMI+Con.+Phrase\u00a0Sasaki et al. ( 2017 ) | 71.4\nBERT-large\u00a0Wang et al. ( 2019 ) | 70.5\nBERT-large\u00a0Sap et al. ( 2019 ) | 75.0\nBERT-large\u00a0Li et al. ( 2019 ) | 75.4\nRoBERTa-large (finetuned) | 90.6\nBERT-large (finetuned)* | 76.5 \u00b1 2.7\nRoBERTa-large (finetuned)* | 87.7 \u00b1 0.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c1b4b874-ea14-4243-a9d8-1f7ce0e5c941",
    "input": "## Claim\nHere is a claim: The model performs significantly better when trained with hinge loss instead of cross-entropy loss, indicating the importance of the loss function. Does the following context support or refute the claim?\n\n## Table\nPaper title: Building a Production Model for Retrieval-Based Chatbots\nTable caption: Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.\n[BOLD] Model | [BOLD] Parameters | [BOLD] Validation AUC@0.05 | [BOLD] Test AUC@0.05\nBase | 8.0M | [BOLD] 0.871 | 0.816\n4L SRU \u2192 2L LSTM | 7.3M | 0.864 | [BOLD] 0.829\n4L SRU \u2192 2L SRU | 7.8M | 0.856 | [BOLD] 0.829\nFlat \u2192 hierarchical | 12.4M | 0.825 | 0.559\nCross entropy \u2192 hinge loss | 8.0M | 0.765 | 0.693\n6.6M \u2192 1M examples | 8.0M | 0.835 | 0.694\n6.6M \u2192 100K examples | 8.0M | 0.565 | 0.417\n200 \u2192 100 negatives | 8.0M | 0.864 | 0.647\n200 \u2192 10 negatives | 8.0M | 0.720 | 0.412\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2c6bda23-8b25-4510-a555-9234e71214ac",
    "input": "## Claim\nHere is a claim: [CONTINUE] Finally, not all emoji are beneficial for this task. Does the following context support or refute the claim?\n\n## Table\nPaper title: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations\nTable caption: Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.\n[BOLD] Emoji alias | [BOLD] N | [BOLD] emoji # | [BOLD] emoji % | [BOLD] no-emoji # | [BOLD] no-emoji % | [BOLD] \u0394%\nmask | 163 | 154 | 94.48 | 134 | 82.21 | - 12.27\ntwo_hearts | 87 | 81 | 93.10 | 77 | 88.51 | - 4.59\nheart_eyes | 122 | 109 | 89.34 | 103 | 84.43 | - 4.91\nheart | 267 | 237 | 88.76 | 235 | 88.01 | - 0.75\nrage | 92 | 78 | 84.78 | 66 | 71.74 | - 13.04\ncry | 116 | 97 | 83.62 | 83 | 71.55 | - 12.07\nsob | 490 | 363 | 74.08 | 345 | 70.41 | - 3.67\nunamused | 167 | 121 | 72.46 | 116 | 69.46 | - 3.00\nweary | 204 | 140 | 68.63 | 139 | 68.14 | - 0.49\njoy | 978 | 649 | 66.36 | 629 | 64.31 | - 2.05\nsweat_smile | 111 | 73 | 65.77 | 75 | 67.57 | 1.80\nconfused | 77 | 46 | 59.74 | 48 | 62.34 | 2.60\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "4ee4134b-7e4a-41fb-9eb4-ef3dd0fef13f",
    "input": "## Claim\nHere is a claim: The results in Table 7 show that the method is comparable to state of the art BiLSTM model from (Fancellu et al., 2016) on gold negation cues for scope prediction. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "55e824e9-c2e1-4cb6-8431-63df8a604668",
    "input": "## Claim\nHere is a claim: DAMD shows the effectiveness of capturing large-scale action patterns Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "27cbf083-1fc5-4938-9b87-bcce547cdec5",
    "input": "## Claim\nHere is a claim: As can be seen in the results presented in Table 3, the models using softmax and sparsemax in the output attention layer outperform the models using TVMAX. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.\n[EMPTY] | Att. to image | Att. to bounding boxes | Test-Dev Yes/No | Test-Dev Number | Test-Dev Other | Test-Dev Overall | Test-Standard Yes/No | Test-Standard Number | Test-Standard Other | Test-Standard Overall\nsoftmax | \u2713 | [EMPTY] | 83.08 | 42.65 | 55.74 | 65.52 | 83.55 | 42.68 | 56.01 | 65.97\nsparsemax | \u2713 | [EMPTY] | 83.08 | 43.19 | 55.79 | 65.60 | 83.33 | 42.99 | 56.06 | 65.94\nsoft-TVmax | \u2713 | [EMPTY] | 83.13 | 43.53 | 56.01 | 65.76 | 83.63 | 43.24 | 56.10 | 66.11\nsparse-TVmax | \u2713 | [EMPTY] | 83.10 | 43.30 | 56.14 | 65.79 | 83.66 | 43.18 | 56.21 | 66.17\nsoftmax | [EMPTY] | \u2713 | 85.14 | 49.59 | 58.72 | 68.57 | 85.56 | 49.54 | 59.11 | 69.04\nsparsemax | [EMPTY] | \u2713 | [BOLD] 85.40 | [BOLD] 50.87 | 58.67 | 68.79 | [BOLD] 85.80 | 50.18 | 59.08 | 69.19\nsoftmax | \u2713 | \u2713 | 85.33 | 50.49 | 58.88 | 68.82 | 85.58 | 50.42 | 59.18 | 69.17\nsparse-TVmax | \u2713 | \u2713 | 85.35 | 50.52 | [BOLD] 59.15 | [BOLD] 68.96 | 85.72 | [BOLD] 50.66 | [BOLD] 59.22 | [BOLD] 69.28\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "c0d4f7e7-545a-4108-a282-9c2355965cb2",
    "input": "## Claim\nHere is a claim: G2S-GGNN outperforms others with the same amount of Gigaword sentences (200K), achieving a 32.23 BLEU score, as shown in Table 3. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 3: Results on LDC2015E86 test set when models are trained with additional Gigaword data.\n<bold>Model</bold> | <bold>External</bold> | <bold>BLEU</bold>\nKonstas et al. (2017) | 200K | 27.40\nSong et al. (2018) | 200K | 28.20\nGuo et al. (2019) | 200K | 31.60\nG2S-GGNN | 200K | <bold>32.23</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "1a8e67b2-88be-49d5-bb39-f4d96cdb3495",
    "input": "## Claim\nHere is a claim: For Waseem (2016) we see that there is no significant difference in the estimated rates at which tweets are clas [CONTINUE] sified as racist across groups, although the rates remain low. Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 2: Experiment 1\nDataset | Class | \u02c6 [ITALIC] piblack | \u02c6 [ITALIC] piwhite | [ITALIC] t | [ITALIC] p | \u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite\n[ITALIC] Waseem and Hovy | Racism | 0.001 | 0.003 | -20.818 | *** | 0.505\n[EMPTY] | Sexism | 0.083 | 0.048 | 101.636 | *** | 1.724\n[ITALIC] Waseem | Racism | 0.001 | 0.001 | 0.035 | [EMPTY] | 1.001\n[EMPTY] | Sexism | 0.023 | 0.012 | 64.418 | *** | 1.993\n[EMPTY] | Racism and sexism | 0.002 | 0.001 | 4.047 | *** | 1.120\n[ITALIC] Davidson et al. | Hate | 0.049 | 0.019 | 120.986 | *** | 2.573\n[EMPTY] | Offensive | 0.173 | 0.065 | 243.285 | *** | 2.653\n[ITALIC] Golbeck et al. | Harassment | 0.032 | 0.023 | 39.483 | *** | 1.396\n[ITALIC] Founta et al. | Hate | 0.111 | 0.061 | 122.707 | *** | 1.812\n[EMPTY] | Abusive | 0.178 | 0.080 | 211.319 | *** | 2.239\n[EMPTY] | Spam | 0.028 | 0.015 | 63.131 | *** | 1.854\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c256c279-dab2-4c45-b0e9-b49660868f5f",
    "input": "## Claim\nHere is a claim: Increasing the window size to 10 reduces the F1 score marginally (A3\u2212A4). Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.\n[EMPTY] | Prec. | Rec. | F1\n(A1) BiLSTM-CNN | 0.473 | 0.606 | 0.531\n(A2) Standard attention | 0.466 | 0.638 | 0.539\n(A3) Window size ( [ITALIC] ws)=5 | 0.507 | 0.652 | [BOLD] 0.571\n(A4) Window size ( [ITALIC] ws)=10 | 0.510 | 0.640 | 0.568\n(A5) Softmax | 0.490 | 0.658 | 0.562\n(A6) Max-pool | 0.492 | 0.600 | 0.541\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "df4ef506-a5a6-4e77-8a07-2b8a0f630696",
    "input": "## Claim\nHere is a claim: [CONTINUE] BI+IS decoding with single-domain trained models achieves gains over both the naive uniform approach and over oracle single-domain models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.\n[BOLD] Language pair | [BOLD] Model type | [BOLD] Oracle model | [BOLD] Decoder configuration  [BOLD] Uniform | [BOLD] Decoder configuration  [BOLD] BI + IS\nes-en | Unadapted | 36.4 | 34.7 | 36.6\nes-en | No-reg | 36.6 | 34.8 | -\nes-en | EWC | 37.0 | 36.3 | [BOLD] 37.2\nen-de | Unadapted | 36.4 | 26.8 | 38.8\nen-de | No-reg | 41.7 | 31.8 | -\nen-de | EWC | 42.1 | 38.6 | [BOLD] 42.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "440cc321-d9fd-4f0c-8f3f-21bcaee949e5",
    "input": "## Claim\nHere is a claim: These results indicate that dense connections do not play a significant role in our model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\n-{4} dense block | 24.8 | 54.9\n-{3, 4} dense blocks | 23.8 | 54.1\n-{2, 3, 4} dense blocks | 23.2 | 53.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "0dacc112-f2b4-4fcb-9e05-98368e576c3e",
    "input": "## Claim\nHere is a claim: Note that training on B-COPA 50% exposes the model to lexically less diverse training instances than the original COPA due to the high overlap between mirrored alternatives [CONTINUE] These results show that once superficial cues [CONTINUE] are removed, the models are not able to learn the task to a high degree. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large-FT | B-COPA | 74.5 (\u00b1 0.7) | 74.7 (\u00b1 0.4) | [BOLD] 74.4 (\u00b1 0.9)\nBERT-large-FT | B-COPA (50%) | 74.3 (\u00b1 2.2) | 76.8 (\u00b1 1.9) | 72.8 (\u00b1 3.1)\nBERT-large-FT | COPA | [BOLD] 76.5 (\u00b1 2.7) | [BOLD] 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5)\nRoBERTa-large-FT | B-COPA | [BOLD] 89.0 (\u00b1 0.3) | 88.9 (\u00b1 2.1) | [BOLD] 89.0 (\u00b1 0.8)\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (\u00b1 2.2) | 87.4 (\u00b1 1.1) | 85.4 (\u00b1 2.9)\nRoBERTa-large-FT | COPA | 87.7 (\u00b1 0.9) | [BOLD] 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "7f929dc9-7327-40e8-9352-1767a83b1a2f",
    "input": "## Claim\nHere is a claim: [CONTINUE] however, oLRN yields the best BLEU score of 26.73, outperforming GRU (+0.45 BLEU). Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.\nModel | #Params | BLEU | Train | Decode\nGNMT | - | 24.61 | - | -\nGRU | 206M | 26.28 | 2.67 | 45.35\nATR | 122M | 25.70 | 1.33 | [BOLD] 34.40\nSRU | 170M | 25.91 | 1.34 | 42.84\nLRN | 143M | 26.26 | [BOLD] 0.99 | 36.50\noLRN | 164M | [BOLD] 26.73 | 1.15 | 40.19\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "59fe28ba-af08-4f7c-aa03-c61c11dbe6f2",
    "input": "## Claim\nHere is a claim: Lemma-based targets with POS disambiguation perform best on WN-N when dependency-based contexts are used; the difference to lemmatized targets without disambiguation is statistically significant (p < .1). Does the following context support or refute the claim?\n\n## Table\nPaper title: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources\nTable caption: Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.\n[EMPTY] | WN-N P | WN-N R | WN-N F | WN-V P | WN-V R | WN-V F | VN P | VN R | VN F\nContext: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2\ntype | .700 | .654 | .676 | .535 | .474 | .503 | .327 | .309 | .318\nx+POS | .699 | .651 | .674 | .544 | .472 | .505 | .339 | .312 | .325\nlemma | .706 | .660 | .682 | .576 | .520 | .547 | .384 | .360 | .371\nx+POS | <bold>.710</bold> | <bold>.662</bold> | <bold>.685</bold> | <bold>.589</bold> | <bold>.529</bold> | <bold>.557</bold> | <bold>.410</bold> | <bold>.389</bold> | <bold>.399</bold>\nContext: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep\ntype | .712 | .661 | .686 | .545 | .457 | .497 | .324 | .296 | .310\nx+POS | .715 | .659 | .686 | .560 | .464 | .508 | .349 | .320 | .334\nlemma | <bold>.725</bold> | <bold>.668</bold> | <bold>.696</bold> | .591 | .512 | .548 | .408 | .371 | .388\nx+POS | .722 | .666 | .693 | <bold>.609</bold> | <bold>.527</bold> | <bold>.565</bold> | <bold>.412</bold> | <bold>.381</bold> | <bold>.396</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "b4829db1-041f-4a7e-9371-9042d2584441",
    "input": "## Claim\nHere is a claim: [CONTINUE] We empirically found that self-attention was the most efficient in the 3rd stage. Does the following context support or refute the claim?\n\n## Table\nPaper title: Modulated Self-attention Convolutional Network for VQA\nTable caption: Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).\n[BOLD] ResNet-34 | [BOLD] Eval set % | [BOLD] #param\nBaseline (No SA)Anderson et al. ( 2018 ) | 55.00 | 0M\nSA (S: 1,2,3 - B: 1) | 55.11 | } 0.107M\nSA (S: 1,2,3 - B: 2) | 55.17 | } 0.107M\n[BOLD] SA (S: 1,2,3 - B: 3) | [BOLD] 55.27 | } 0.107M\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7be9b83d-f973-4655-9a2c-39eb8160b687",
    "input": "## Claim\nHere is a claim: The results in Table 5 show that the frequency whitelists perform better than the random and clustering whitelists when the true response is added. Does the following context support or refute the claim?\n\n## Table\nPaper title: Building a Production Model for Retrieval-Based Chatbots\nTable caption: Table 5: Recall@k for random, frequency, and clustering whitelists of different sizes. The \u201c+\u201d indicates that the true response is added to the whitelist.\n[BOLD] Whitelist | [BOLD] R@1 | [BOLD] R@3 | [BOLD] R@5 | [BOLD] R@10 | [BOLD] BLEU\nRandom 10K+ | 0.252 | 0.400 | 0.472 | 0.560 | 37.71\nFrequency 10K+ | 0.257 | 0.389 | 0.455 | 0.544 | 41.34\nClustering 10K+ | 0.230 | 0.376 | 0.447 | 0.541 | 37.59\nRandom 1K+ | 0.496 | 0.663 | 0.728 | 0.805 | 59.28\nFrequency 1K+ | 0.513 | 0.666 | 0.726 | 0.794 | 67.05\nClustering 1K+ | 0.481 | 0.667 | 0.745 | 0.835 | 61.88\nFrequency 10K | 0.136 | 0.261 | 0.327 | 0.420 | 30.46\nClustering 10K | 0.164 | 0.292 | 0.360 | 0.457 | 31.47\nFrequency 1K | 0.273 | 0.465 | 0.550 | 0.658 | 47.13\nClustering 1K | 0.331 | 0.542 | 0.650 | 0.782 | 49.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "68d6065c-868c-40ac-b3a9-14218014c2c1",
    "input": "## Claim\nHere is a claim: These results use the best performing KnowComb system. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.\nSchema | AntePre(Test) | AntePre(Train)\nType 1 | 76.67 | 86.79\nType 2 | 79.55 | 88.86\nType 1 (Cat1) | 90.26 | 93.64\nType 2 (Cat2) | 83.38 | 92.49\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "dee030ab-36f2-4c8f-a9f2-297aac021d8f",
    "input": "## Claim\nHere is a claim: BERT achieved a final accuracy of 91.20%, now marginally comparable to ULMFiT's full performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Localization of Fake News Detection via Multitask Transfer Learning\nTable caption: Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.\nModel | Val. Accuracy | Loss | Val. Loss | Pretraining Time | Finetuning Time\nSiamese Networks | 77.42% | 0.5601 | 0.5329 | [EMPTY] | 4m per epoch\nBERT | 87.47% | 0.4655 | 0.4419 | 66 hours | 2m per epoch\nGPT-2 | 90.99% | 0.2172 | 0.1826 | 78 hours | 4m per epoch\nULMFiT | 91.59% | 0.3750 | 0.1972 | 11 hours | 2m per epoch\nULMFiT (no LM Finetuning) | 78.11% | 0.5512 | 0.5409 | 11 hours | 2m per epoch\nBERT + Multitasking | 91.20% | 0.3155 | 0.3023 | 66 hours | 4m per epoch\nGPT-2 + Multitasking | 96.28% | 0.2609 | 0.2197 | 78 hours | 5m per epoch\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a50811fb-3024-4844-b141-56da2fa21184",
    "input": "## Claim\nHere is a claim: Our model does not outperform the previous state-of-the-art models on both datasets in terms of F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. \u2020 denotes the previous best state-of-the-art model.\nModel | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1\nCNN zeng2014relation | 0.413 | 0.591 | 0.486 | 0.444 | 0.625 | 0.519\nPCNN zeng2015distant | 0.380 | [BOLD] 0.642 | 0.477 | 0.446 | 0.679 | 0.538\u2020\nEA huang2016attention | 0.443 | 0.638 | 0.523\u2020 | 0.419 | 0.677 | 0.517\nBGWA jat2018attention | 0.364 | 0.632 | 0.462 | 0.417 | [BOLD] 0.692 | 0.521\nBiLSTM-CNN | 0.490 | 0.507 | 0.498 | 0.473 | 0.606 | 0.531\nOur model | [BOLD] 0.541 | 0.595 | [BOLD] 0.566* | [BOLD] 0.507 | 0.652 | [BOLD] 0.571*\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5ba4624b-1fd1-4ae1-bb8d-1bf53456f96e",
    "input": "## Claim\nHere is a claim: This indicates that PMeans can better detect informative sentences, and PMeans-RNN can better find informative words in extracted sentence. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "a44d9bc1-bdf5-497b-8274-551f0c88fa9e",
    "input": "## Claim\nHere is a claim: Our results indicate that neither beam search nor diversity-enhancing decoding schemes can generate multiple actions well. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "fad1f689-0127-4c3a-bf86-258d0de6f364",
    "input": "## Claim\nHere is a claim: GPT-2, on the other hand, finetuned to a final accuracy of 96.28%, a full 4.69% improvement over the performance of ULMFiT. Does the following context support or refute the claim?\n\n## Table\nPaper title: Localization of Fake News Detection via Multitask Transfer Learning\nTable caption: Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.\nModel | Val. Accuracy | Loss | Val. Loss | Pretraining Time | Finetuning Time\nSiamese Networks | 77.42% | 0.5601 | 0.5329 | [EMPTY] | 4m per epoch\nBERT | 87.47% | 0.4655 | 0.4419 | 66 hours | 2m per epoch\nGPT-2 | 90.99% | 0.2172 | 0.1826 | 78 hours | 4m per epoch\nULMFiT | 91.59% | 0.3750 | 0.1972 | 11 hours | 2m per epoch\nULMFiT (no LM Finetuning) | 78.11% | 0.5512 | 0.5409 | 11 hours | 2m per epoch\nBERT + Multitasking | 91.20% | 0.3155 | 0.3023 | 66 hours | 4m per epoch\nGPT-2 + Multitasking | 96.28% | 0.2609 | 0.2197 | 78 hours | 5m per epoch\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "53cdc8d3-bd81-4d5d-8572-79b80c41480d",
    "input": "## Claim\nHere is a claim: We find that when we train STagBL with only its main task\u2014with label set [CONTINUE] In Y contrast, when we include the 'natural subtasks' \"C\" (label [CONTINUE] performance increases typically by a few percentage points. Does the following context support or refute the claim?\n\n## Table\nPaper title: Neural End-to-End Learning for Computational Argumentation Mining\nTable caption: Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by \u201c:\u201d. Layers from which tasks feed are indicated by respective numbers.\n[EMPTY] | C-F1 100% | C-F1 50% | R-F1 100% | R-F1 50% | F1 100% | F1 50%\nY-3 | 49.59 | 65.37 | 26.28 | 37.00 | 34.35 | 47.25\nY-3:Y<italic>C</italic>-1 | 54.71 | 66.84 | 28.44 | 37.35 | 37.40 | 47.92\nY-3:Y<italic>R</italic>-1 | 51.32 | 66.49 | 26.92 | 37.18 | 35.31 | 47.69\nY-3:Y<italic>C</italic>-3 | <bold>54.58</bold> | 67.66 | <bold>30.22</bold> | <bold>40.30</bold> | <bold>38.90</bold> | <bold>50.51</bold>\nY-3:Y<italic>R</italic>-3 | 53.31 | 66.71 | 26.65 | 35.86 | 35.53 | 46.64\nY-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2 | 52.95 | <bold>67.84</bold> | 27.90 | 39.71 | 36.54 | 50.09\nY-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3 | 54.55 | 67.60 | 28.30 | 38.26 | 37.26 | 48.86\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a0df0012-c351-4b59-b594-86522f72ffda",
    "input": "## Claim\nHere is a claim: the blue marker represents the ratio of the \"Full\" score, and the orange marker denotes the ratio of \u201cOther\u201d. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0ed421ff-4061-441d-bbdd-6f1cdc44ca0b",
    "input": "## Claim\nHere is a claim: When we increase the DCGCN blocks from 1 to 4, the model performance does not necessarily increase on AMR15 development set. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.\n[BOLD] GCN +RC (2) | B 16.8 | C 48.1 | [BOLD] GCN +RC+LA (2) | B 18.3 | C 47.9\n+RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1\n+RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8\n+RC (9) | [BOLD] 21.1 | 50.5 | +RC+LA (9) | [BOLD] 22.0 | 52.6\n+RC (10) | 20.7 | [BOLD] 50.7 | +RC+LA (10) | 21.2 | [BOLD] 52.9\nDCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7\nDCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "af3ebe5e-4d9c-48e9-83d1-6039796679bf",
    "input": "## Claim\nHere is a claim: Table 5 shows improvements on data without domain labelling using our adaptive decoding schemes with unadapted models trained only on one domain [CONTINUE] Uniform ensembling under-performs all oracle models except es-en Bio, especially on general domains. Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.\n[BOLD] Decoder configuration | [BOLD] es-en  [BOLD] Health | [BOLD] es-en  [BOLD] Bio | [BOLD] en-de  [BOLD] News | [BOLD] en-de  [BOLD] TED | [BOLD] en-de  [BOLD] IT\nOracle model | 35.9 | 36.1 | 37.8 | 24.1 | 39.6\nUniform | 33.1 | 36.4 | 21.9 | 18.4 | 38.9\nIdentity-BI | 35.0 | 36.6 | 32.7 | 25.3 | 42.6\nBI | 35.9 | 36.5 | 38.0 | 26.1 | [BOLD] 44.7\nIS | [BOLD] 36.0 | 36.8 | 37.5 | 25.6 | 43.3\nBI + IS | [BOLD] 36.0 | [BOLD] 36.9 | [BOLD] 38.4 | [BOLD] 26.4 | [BOLD] 44.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ffc551ba-ac96-4d34-ab5f-fec1d0e20a57",
    "input": "## Claim\nHere is a claim: It can be observed that the learned reward function does not have good interpretability in that the reward is positive when the dialog gets a full score on each metric, and negative otherwise. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "26382fa1-036e-4dcf-bf81-08d8763ef416",
    "input": "## Claim\nHere is a claim: The Waseem and Hovy (2016) classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned [CONTINUE] tweets predicted to belong to this class. Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 4: Experiment 2, t= \u201cb*tch\u201d\nDataset | Class | \u02c6 [ITALIC] piblack | \u02c6 [ITALIC] piwhite | [ITALIC] t | [ITALIC] p | \u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite\n[ITALIC] Waseem and Hovy | Racism | 0.010 | 0.010 | -0.632 | [EMPTY] | 0.978\n[EMPTY] | Sexism | 0.963 | 0.944 | 20.064 | *** | 1.020\n[ITALIC] Waseem | Racism | 0.011 | 0.011 | -1.254 | [EMPTY] | 0.955\n[EMPTY] | Sexism | 0.349 | 0.290 | 28.803 | *** | 1.203\n[EMPTY] | Racism and sexism | 0.012 | 0.012 | -0.162 | [EMPTY] | 0.995\n[ITALIC] Davidson et al. | Hate | 0.017 | 0.015 | 4.698 | *** | 1.152\n[EMPTY] | Offensive | 0.988 | 0.991 | -6.289 | *** | 0.997\n[ITALIC] Golbeck et al. | Harassment | 0.099 | 0.091 | 6.273 | *** | 1.091\n[ITALIC] Founta et al. | Hate | 0.074 | 0.027 | 46.054 | *** | 2.728\n[EMPTY] | Abusive | 0.925 | 0.968 | -41.396 | *** | 0.956\n[EMPTY] | Spam | 0.010 | 0.010 | 0.000 | [EMPTY] | 1.000\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "335c8128-859a-4bb2-808d-c48b428dd5d0",
    "input": "## Claim\nHere is a claim: [CONTINUE] The 'alternating' LSTM layout we chose for our submission actually outperformed the 'traditional' one in terms of both single model and ensemble performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents\nTable caption: Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.\nID LSTM-800 | 5-fold CV 70.56 | \u0394 0.66 | Single model 67.54 | \u0394 0.78 | Ensemble 67.65 | \u0394 0.30\nLSTM-400 | 70.50 | 0.60 | [BOLD] 67.59 | 0.83 | [BOLD] 68.00 | 0.65\nIN-TITLE | 70.11 | 0.21 | [EMPTY] | [EMPTY] | 67.52 | 0.17\n[BOLD] SUBMISSION | 69.90 | \u2013 | 66.76 | \u2013 | 67.35 | \u2013\nNO-HIGHWAY | 69.72 | \u22120.18 | 66.42 | \u22120.34 | 66.64 | \u22120.71\nNO-OVERLAPS | 69.46 | \u22120.44 | 65.07 | \u22121.69 | 66.47 | \u22120.88\nLSTM-400-DROPOUT | 69.45 | \u22120.45 | 65.53 | \u22121.23 | 67.28 | \u22120.07\nNO-TRANSLATIONS | 69.42 | \u22120.48 | 65.92 | \u22120.84 | 67.23 | \u22120.12\nNO-ELMO-FINETUNING | 67.71 | \u22122.19 | 65.16 | \u22121.60 | 65.42 | \u22121.93\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "7e1aa17c-ab4a-421b-a158-486615a41865",
    "input": "## Claim\nHere is a claim: Firstly, we use a simple rule-based classifier where each word after punctuation marks that are not parenthesis, brackets, or quotes, is predicted to be in the scope of a negation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "1d4b85e2-79a4-463d-873b-f6bdcef4fa2f",
    "input": "## Claim\nHere is a claim: [CONTINUE] Lin-SVM outperforms other classifiers in extracting most relations. Does the following context support or refute the claim?\n\n## Table\nPaper title: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data\nTable caption: Table 1: Performance of supervised learning models with different features.\nFeature | LR P | LR R | LR F1 | SVM P | SVM R | SVM F1 | ANN P | ANN R | ANN F1\n+BoW | 0.93 | 0.91 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+BoC (Wiki-PubMed-PMC) | 0.94 | 0.92 | [BOLD] 0.93 | 0.94 | 0.92 | [BOLD] 0.93 | 0.91 | 0.91 | [BOLD] 0.91\n+BoC (GloVe) | 0.93 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+ASM | 0.90 | 0.85 | 0.88 | 0.90 | 0.86 | 0.88 | 0.89 | 0.89 | 0.89\n+Sentence Embeddings(SEs) | 0.89 | 0.89 | 0.89 | 0.90 | 0.86 | 0.88 | 0.88 | 0.88 | 0.88\n+BoC(Wiki-PubMed-PMC)+SEs | 0.92 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3df48964-f174-4875-97f2-dee5dfb515c5",
    "input": "## Claim\nHere is a claim: [CONTINUE] Though ALDM obtains a higher inform F1 and match rate than PPO, it does not get a significant improvement [CONTINUE] on task success [CONTINUE] Ablation test is investigated in Table 3. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "07dfd438-80f3-41d8-ba09-1f769f983131",
    "input": "## Claim\nHere is a claim: LRN does not accelerate the training over LSTM and SRU by about 20%. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 2: Test error (ERR) on document classification task. \u201c#Params\u201d: the parameter number in AmaPolar task. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti.\nModel | Model | #Params | AmaPolar ERR | AmaPolar Time | Yahoo ERR | Yahoo Time | AmaFull ERR | AmaFull Time | YelpPolar ERR | YelpPolar Time\nZhang et\u00a0al. ( 2015 ) | Zhang et\u00a0al. ( 2015 ) | - | 6.10 | - | 29.16 | - | 40.57 | - | 5.26 | -\nThis | LSTM | 227K | [BOLD] 4.37 | 0.947 | [BOLD] 24.62 | 1.332 | 37.22 | 1.003 | 3.58 | 1.362\nThis | GRU | 176K | 4.39 | 0.948 | 24.68 | 1.242 | [BOLD] 37.20 | 0.982 | [BOLD] 3.47 | 1.230\nThis | ATR | 74K | 4.78 | 0.867 | 25.33 | 1.117 | 38.54 | 0.836 | 4.00 | 1.124\nWork | SRU | 194K | 4.95 | 0.919 | 24.78 | 1.394 | 38.23 | 0.907 | 3.99 | 1.310\n[EMPTY] | LRN | 151K | 4.98 | [BOLD] 0.731 | 25.07 | [BOLD] 1.038 | 38.42 | [BOLD] 0.788 | 3.98 | [BOLD] 1.022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1d89289c-47a5-4052-8b51-a516aead51a8",
    "input": "## Claim\nHere is a claim: [CONTINUE] Across unigrams, part-of-speech patterns and word clusters, we see a distinctive pattern emerging around pronoun usage. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\n[BOLD] Complaints  [BOLD] Feature | [BOLD] Complaints  [ITALIC] r | [BOLD] Not Complaints  [BOLD] Feature | [BOLD] Not Complaints  [ITALIC] r\n[BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams\nnot | .154 | [URL] | .150\nmy | .131 | ! | .082\nworking | .124 | he | .069\nstill | .123 | thank | .067\non | .119 | , | .064\ncan\u2019t | .113 | love | .064\nservice | .112 | lol | .061\ncustomer | .109 | you | .060\nwhy | .108 | great | .058\nwebsite | .107 | win | .058\nno | .104 | \u2019 | .058\n? | .098 | she | .054\nfix | .093 | : | .053\nwon\u2019t | .092 | that | .053\nbeen | .090 | more | .052\nissue | .089 | it | .052\ndays | .088 | would | .051\nerror | .087 | him | .047\nis | .084 | life | .046\ncharged | .083 | good | .046\n[BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)\nVBN | .141 | UH | .104\n$ | .118 | NNP | .098\nVBZ | .114 | PRP | .076\nNN_VBZ | .114 | HT | .076\nPRP$ | .107 | PRP_. | .076\nPRP$_NN | .105 | PRP_RB | .067\nVBG | .093 | NNP_NNP | .062\nCD | .092 | VBP_PRP | .054\nWRB_VBZ | .084 | JJ | .053\nVBZ_VBN | .084 | DT_JJ | .051\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "4366020a-5cdf-4758-aa6a-8185b337656e",
    "input": "## Claim\nHere is a claim: The Waseem and Hovy (2016) classifier is not particularly sensitive to the word \"b*tch\" with only 1% of black-aligned and 1% of white-aligned tweets predicted to belong to this class. Does the following context support or refute the claim?\n\n## Table\nPaper title: Racial Bias in Hate Speech and Abusive Language Detection Datasets\nTable caption: Table 4: Experiment 2, t= \u201cb*tch\u201d\nDataset | Class | \u02c6 [ITALIC] piblack | \u02c6 [ITALIC] piwhite | [ITALIC] t | [ITALIC] p | \u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite\n[ITALIC] Waseem and Hovy | Racism | 0.010 | 0.010 | -0.632 | [EMPTY] | 0.978\n[EMPTY] | Sexism | 0.963 | 0.944 | 20.064 | *** | 1.020\n[ITALIC] Waseem | Racism | 0.011 | 0.011 | -1.254 | [EMPTY] | 0.955\n[EMPTY] | Sexism | 0.349 | 0.290 | 28.803 | *** | 1.203\n[EMPTY] | Racism and sexism | 0.012 | 0.012 | -0.162 | [EMPTY] | 0.995\n[ITALIC] Davidson et al. | Hate | 0.017 | 0.015 | 4.698 | *** | 1.152\n[EMPTY] | Offensive | 0.988 | 0.991 | -6.289 | *** | 0.997\n[ITALIC] Golbeck et al. | Harassment | 0.099 | 0.091 | 6.273 | *** | 1.091\n[ITALIC] Founta et al. | Hate | 0.074 | 0.027 | 46.054 | *** | 2.728\n[EMPTY] | Abusive | 0.925 | 0.968 | -41.396 | *** | 0.956\n[EMPTY] | Spam | 0.010 | 0.010 | 0.000 | [EMPTY] | 1.000\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5e7dfbf4-9542-4f80-a83b-44c4f4753db7",
    "input": "## Claim\nHere is a claim: The total number of words in the concatenated inputs is longer than other MDS datasets, as those consist of 10 input documents, but shorter than SDS datasets, as expected. Does the following context support or refute the claim?\n\n## Table\nPaper title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\nTable caption: Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.\n[BOLD] Dataset | [BOLD] # pairs | [BOLD] # words (doc) | [BOLD] # sents (docs) | [BOLD] # words (summary) | [BOLD] # sents (summary) | [BOLD] vocab size\nMulti-News | 44,972/5,622/5,622 | 2,103.49 | 82.73 | 263.66 | 9.97 | 666,515\nDUC03+04 | 320 | 4,636.24 | 173.15 | 109.58 | 2.88 | 19,734\nTAC 2011 | 176 | 4,695.70 | 188.43 | 99.70 | 1.00 | 24,672\nCNNDM | 287,227/13,368/11,490 | 810.57 | 39.78 | 56.20 | 3.68 | 717,951\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5d749e06-1775-42bd-b28f-d106eab9163f",
    "input": "## Claim\nHere is a claim: The complete model has slightly more parameters than the model without graph encoders (57.6M vs 61.7M). Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 4: Results of the ablation study on the LDC2017T10 development set.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold> | <bold>Size</bold>\nbiLSTM | 22.50 | 30.42 | 57.6M\n<italic>GEt</italic> + biLSTM | 26.33 | 32.62 | 59.6M\n<italic>GEb</italic> + biLSTM | 26.12 | 32.49 | 59.6M\n<italic>GEt</italic> + <italic>GEb</italic> + biLSTM | 27.37 | 33.30 | 61.7M\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c6b5330f-3246-4e62-a3c8-78aeae05a8f5",
    "input": "## Claim\nHere is a claim: The results furthermore show that the sdps based on the Stanford Basic (SB) representation provide the best performance, followed by the CoNLL08 representation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Syntactic Dependency Representations in Neural Relation Classification\nTable caption: Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.\n[BOLD] Representation | [BOLD] Hyper parameters Filter size | [BOLD] Hyper parameters Num. Feature maps | [BOLD] Hyper parameters Activation func. | [BOLD] Hyper parameters L2 Reg. | [BOLD] Hyper parameters Learning rate | [BOLD] Hyper parameters Dropout Prob. | [BOLD] F1.(avg. in 5-fold) with default values | [BOLD] F1.(avg. in 5-fold) with optimal values\nCoNLL08 | 4-5 | 1000 | Softplus | 1.15e+01 | 1.13e-03 | 1 | 73.34 | 74.49\nSB | 4-5 | 806 | Sigmoid | 8.13e-02 | 1.79e-03 | 0.87 | 72.83 | [BOLD] 75.05\nUD v1.3 | 5 | 716 | Softplus | 1.66e+00 | 9.63E-04 | 1 | 68.93 | 69.57\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0a6b428c-3dd6-45e7-82d2-a04db14738d7",
    "input": "## Claim\nHere is a claim: On the NYT11 dataset, m = 4 gives the best performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 3: Performance comparison of our model with different values of m on the two datasets.\n[ITALIC] m | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1\n1 | 0.541 | 0.595 | [BOLD] 0.566 | 0.495 | 0.621 | 0.551\n2 | 0.521 | 0.597 | 0.556 | 0.482 | 0.656 | 0.555\n3 | 0.490 | 0.617 | 0.547 | 0.509 | 0.633 | 0.564\n4 | 0.449 | 0.623 | 0.522 | 0.507 | 0.652 | [BOLD] 0.571\n5 | 0.467 | 0.609 | 0.529 | 0.488 | 0.677 | 0.567\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "40b4cebb-43c9-4eb1-b17e-5416de928391",
    "input": "## Claim\nHere is a claim: Since only 20% of the tweets are used as negative training samples, we cannot use all negative tweets for development phase. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 8: Sentiment classification evaluation, using different classifiers on the test set.\nClassifier | Positive Sentiment Precision | Positive Sentiment Recall | Positive Sentiment Fscore\nSVM-w/o neg. | 0.57 | 0.72 | 0.64\nSVM-Punct. neg. | 0.58 | 0.70 | 0.63\nSVM-our-neg. | 0.58 | 0.73 | 0.65\nCNN | 0.63 | 0.83 | 0.72\nCNN-LSTM | 0.71 | 0.72 | 0.72\nCNN-LSTM-Our-neg-Ant | [BOLD] 0.78 | [BOLD] 0.77 | [BOLD] 0.78\n[EMPTY] | Negative Sentiment | Negative Sentiment | Negative Sentiment\n[EMPTY] | Precision | Recall | Fscore\nSVM-w/o neg. | 0.78 | 0.86 | 0.82\nSVM-Punct. neg. | 0.78 | 0.87 | 0.83\nSVM-Our neg. | 0.80 | 0.87 | 0.83\nCNN | 0.88 | 0.72 | 0.79\nCNN-LSTM. | 0.83 | 0.83 | 0.83\nCNN-LSTM-our-neg-Ant | [BOLD] 0.87 | [BOLD] 0.87 | [BOLD] 0.87\n[EMPTY] | Train | [EMPTY] | Test\nPositive tweets | 5121 | [EMPTY] | 1320\nNegative tweets | 9094 | [EMPTY] | 2244\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0ee1d441-a931-45f7-ae8d-11c809364782",
    "input": "## Claim\nHere is a claim: We observe that the three settings (n=6, m=3), (n=3, m=6) and (n=6, m=6) give similar results for both 1 DCGCN block and 2 DCGCN blocks. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.\n[ITALIC] Block | [ITALIC] n | [ITALIC] m | B | C\n1 | 1 | 1 | 17.6 | 48.3\n1 | 1 | 2 | 19.2 | 50.3\n1 | 2 | 1 | 18.4 | 49.1\n1 | 1 | 3 | 19.6 | 49.4\n1 | 3 | 1 | 20.0 | 50.5\n1 | 3 | 3 | 21.4 | 51.0\n1 | 3 | 6 | 21.8 | 51.7\n1 | 6 | 3 | 21.7 | 51.5\n1 | 6 | 6 | 22.0 | 52.1\n2 | 3 | 6 | [BOLD] 23.5 | 53.3\n2 | 6 | 3 | 23.3 | [BOLD] 53.4\n2 | 6 | 6 | 22.0 | 52.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "90528389-92f2-4f21-8903-6700b00bcec4",
    "input": "## Claim\nHere is a claim: In some cases it seems to make no difference in results, e.g., Europarl in Portuguese which did not increase the precision from P=0.5984 in DF to P=0.6109 in TF, as well as the recall from R=0.5184 in DF to R=0.6727 in TF, resulting in no increase of f-measure from F=0.5555 in DF to F=0.6403 in TF. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "96697120-9b1f-4d1a-b6f8-dcbc3571a596",
    "input": "## Claim\nHere is a claim: [CONTINUE] The most interesting ones are mask, rage, and cry, which significantly decrease accuracy. Does the following context support or refute the claim?\n\n## Table\nPaper title: IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations\nTable caption: Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.\n[BOLD] Emoji alias | [BOLD] N | [BOLD] emoji # | [BOLD] emoji % | [BOLD] no-emoji # | [BOLD] no-emoji % | [BOLD] \u0394%\nmask | 163 | 154 | 94.48 | 134 | 82.21 | - 12.27\ntwo_hearts | 87 | 81 | 93.10 | 77 | 88.51 | - 4.59\nheart_eyes | 122 | 109 | 89.34 | 103 | 84.43 | - 4.91\nheart | 267 | 237 | 88.76 | 235 | 88.01 | - 0.75\nrage | 92 | 78 | 84.78 | 66 | 71.74 | - 13.04\ncry | 116 | 97 | 83.62 | 83 | 71.55 | - 12.07\nsob | 490 | 363 | 74.08 | 345 | 70.41 | - 3.67\nunamused | 167 | 121 | 72.46 | 116 | 69.46 | - 3.00\nweary | 204 | 140 | 68.63 | 139 | 68.14 | - 0.49\njoy | 978 | 649 | 66.36 | 629 | 64.31 | - 2.05\nsweat_smile | 111 | 73 | 65.77 | 75 | 67.57 | 1.80\nconfused | 77 | 46 | 59.74 | 48 | 62.34 | 2.60\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9435110a-5463-4110-9d72-57ce814067cf",
    "input": "## Claim\nHere is a claim: Finally, Table 5 shows the F1 score of the (in-scope, out-of-scope) negation scopes using Punctuation, our Proposed model and BiLSTM classifier. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "181b9881-e4d1-4e77-af0f-77df20be3b57",
    "input": "## Claim\nHere is a claim: the average length of \u201cgood\u201d summaries is higher than that of other summaries. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nMetric | [ITALIC] \u03c1 | [ITALIC] r | G-Pre | G-Rec\nROUGE-1 | .290 | .304 | .392 | .428\nROUGE-2 | .259 | .278 | .408 | .444\nROUGE-L | .274 | .297 | .390 | .426\nROUGE-SU4 | .282 | .279 | .404 | .440\nBLEU-1 | .256 | .281 | .409 | .448\nBLEU-2 | .301 | .312 | .411 | .446\nBLEU-3 | .317 | .312 | .409 | .444\nBLEU-4 | .311 | .307 | .409 | .446\nBLEU-5 | .308 | .303 | .420 | .459\nMETEOR | .305 | .285 | .409 | .444\nInferSent-Cosine | [BOLD] .329 | [BOLD] .339 | .417 | .460\nBERT-Cosine | .312 | .335 | [BOLD] .440 | [BOLD] .484\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e34b8982-4346-4879-b0ef-05f9a01e1536",
    "input": "## Claim\nHere is a claim: We observe that the B3 metric is harsher than the other two and is most suitable when a very high precision of entity identification is desired. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | <bold>71.2</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f27a4cd3-518d-4311-b4e9-c12bfdeb6b12",
    "input": "## Claim\nHere is a claim: Note that training on B-COPA 50% exposes the model to lexically less diverse training instances than the original COPA due to the high overlap between mirrored alternatives [CONTINUE] These results show that once superficial cues [CONTINUE] are removed, the models are able to learn the task to a high degree. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large-FT | B-COPA | 74.5 (\u00b1 0.7) | 74.7 (\u00b1 0.4) | [BOLD] 74.4 (\u00b1 0.9)\nBERT-large-FT | B-COPA (50%) | 74.3 (\u00b1 2.2) | 76.8 (\u00b1 1.9) | 72.8 (\u00b1 3.1)\nBERT-large-FT | COPA | [BOLD] 76.5 (\u00b1 2.7) | [BOLD] 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5)\nRoBERTa-large-FT | B-COPA | [BOLD] 89.0 (\u00b1 0.3) | 88.9 (\u00b1 2.1) | [BOLD] 89.0 (\u00b1 0.8)\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (\u00b1 2.2) | 87.4 (\u00b1 1.1) | 85.4 (\u00b1 2.9)\nRoBERTa-large-FT | COPA | 87.7 (\u00b1 0.9) | [BOLD] 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ea497540-0fb6-42d3-971a-c539b056ba98",
    "input": "## Claim\nHere is a claim: Word embeddings derived from Wiki-PubMed-PMC outperform GloVe-based embeddings (Table 1). Does the following context support or refute the claim?\n\n## Table\nPaper title: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data\nTable caption: Table 1: Performance of supervised learning models with different features.\nFeature | LR P | LR R | LR F1 | SVM P | SVM R | SVM F1 | ANN P | ANN R | ANN F1\n+BoW | 0.93 | 0.91 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+BoC (Wiki-PubMed-PMC) | 0.94 | 0.92 | [BOLD] 0.93 | 0.94 | 0.92 | [BOLD] 0.93 | 0.91 | 0.91 | [BOLD] 0.91\n+BoC (GloVe) | 0.93 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+ASM | 0.90 | 0.85 | 0.88 | 0.90 | 0.86 | 0.88 | 0.89 | 0.89 | 0.89\n+Sentence Embeddings(SEs) | 0.89 | 0.89 | 0.89 | 0.90 | 0.86 | 0.88 | 0.88 | 0.88 | 0.88\n+BoC(Wiki-PubMed-PMC)+SEs | 0.92 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "afb8c8d5-9e11-40ab-92fb-f60a4ab66649",
    "input": "## Claim\nHere is a claim: Comparing POS and SEM tagging (Table 5), we note that higher layer representations improve SEM tagging, while POS tagging peaks at layer 1. we noticed small but consistent improvements in both translation (+0.9 BLEU) and POS and SEM tagging (up to +0.6% accuracy) when using features extracted from an NMT model trained with residual connections (Table 5). Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks\nTable caption: Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.\nUni | POS | 0 87.9 | 1 92.0 | 2 91.7 | 3 91.8 | 4 91.9\nUni | SEM | 81.8 | 87.8 | 87.4 | 87.6 | 88.2\nBi | POS | 87.9 | 93.3 | 92.9 | 93.2 | 92.8\nBi | SEM | 81.9 | 91.3 | 90.8 | 91.9 | 91.9\nRes | POS | 87.9 | 92.5 | 91.9 | 92.0 | 92.4\nRes | SEM | 81.9 | 88.2 | 87.5 | 87.6 | 88.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2373a5b5-05cc-45ca-9e6c-5323513811b8",
    "input": "## Claim\nHere is a claim: [CONTINUE] As we can observe in Table 6, limiting the number of terms to 1,000, Patt and DocSub do not to generate relations for all terms. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.\nCorpus | Metric | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nEuroparl | TotalTerms: | 957 | 1,000 | 1,000 | 1,000 | 1,000 | 836 | 1,000\nEuroparl | TotalRoots: | 44 | 1 | 1 | 1 | 1 | 43 | 1\nEuroparl | NumberRels: | 1,588 | 1,025 | 1,028 | 1,185 | 1,103 | 1,184 | 999\nEuroparl | MaxDepth: | 21 | 921 | 901 | 788 | 835 | 8 | 15\nEuroparl | MinDepth: | 1 | 921 | 901 | 788 | 835 | 1 | 1\nEuroparl | AvgDepth: | 11.82 | 921 | 901 | 788 | 835 | 3.05 | 8.46\nEuroparl | DepthCohesion: | 1.78 | 1 | 1 | 1 | 1 | 2.62 | 1.77\nEuroparl | MaxWidth: | 20 | 2 | 3 | 4 | 3 | 88 | 41\nEuroparl | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nEuroparl | AvgWidth: | 1.99 | 1.03 | 1.03 | 1.19 | 1.10 | 4.20 | 2.38\nTED Talks | TotalTerms: | 476 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000\nTED Talks | TotalRoots: | 164 | 2 | 1 | 1 | 1 | 1 | 1\nTED Talks | NumberRels: | 521 | 1,029 | 1,331 | 3,025 | 3,438 | 3,802 | 1,009\nTED Talks | MaxDepth: | 16 | 915 | 658 | 454 | 395 | 118 | 12\nTED Talks | MinDepth: | 1 | 913 | 658 | 454 | 395 | 110 | 1\nTED Talks | AvgDepth: | 5.82 | 914 | 658 | 454 | 395 | 112.24 | 5.95\nTED Talks | DepthCohesion: | 2.75 | 1 | 1 | 1 | 1 | 1.05 | 2.02\nTED Talks | MaxWidth: | 25 | 2 | 77 | 13 | 12 | 66 | 98\nTED Talks | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nTED Talks | AvgWidth: | 1.83 | 1.03 | 1.36 | 3.03 | 3.44 | 6.64 | 2.35\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "db25a77b-8a01-4cdf-9925-4697cd1d307f",
    "input": "## Claim\nHere is a claim: This is especially true in the case of DAN where we see a decrease as the decoder repeatedly predicts words having low sentiment value. Does the following context support or refute the claim?\n\n## Table\nPaper title: What do Deep Networks Like to Read?\nTable caption: Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.\n[EMPTY] | <bold>RNN</bold> | <bold>CNN</bold> | <bold>DAN</bold>\nPositive | +9.7 | +4.3 | +<bold>23.6</bold>\nNegative | +6.9 | +5.5 | +<bold>16.1</bold>\nFlipped to Positive | +20.2 | +24.9 | +27.4\nFlipped to Negative | +31.5 | +28.6 | +19.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a7269b76-9a15-4b21-8159-01234702d223",
    "input": "## Claim\nHere is a claim: It achieves competitive results using only the title and body text, in comparison to the R-1,2,L reward systems that integrate multi-task models (Narayan et\\xa0al. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "b5d5b9ea-69ce-4b98-99de-831145e49e2b",
    "input": "## Claim\nHere is a claim: However, EWC outperforms no-reg and L2 on News, not only reducing forgetting but giving 0.5 BLEU improvement over the baseline News model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 4: Test BLEU for en-de adaptive training, with sequential adaptation to a third task. EWC-tuned models give the best performance on each domain.\n[EMPTY] | [BOLD] Training scheme | [BOLD] News | [BOLD] TED | [BOLD] IT\n1 | News | 37.8 | 25.3 | 35.3\n2 | TED | 23.7 | 24.1 | 14.4\n3 | IT | 1.6 | 1.8 | 39.6\n4 | News and TED | 38.2 | 25.5 | 35.4\n5 | 1 then TED, No-reg | 30.6 | [BOLD] 27.0 | 22.1\n6 | 1 then TED, L2 | 37.9 | 26.7 | 31.8\n7 | 1 then TED, EWC | [BOLD] 38.3 | [BOLD] 27.0 | 33.1\n8 | 5 then IT, No-reg | 8.0 | 6.9 | 56.3\n9 | 6 then IT, L2 | 32.3 | 22.6 | 56.9\n10 | 7 then IT, EWC | 35.8 | 24.6 | [BOLD] 57.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d85b8a88-a37f-4803-a198-3a7032d6e695",
    "input": "## Claim\nHere is a claim: [CONTINUE] The Logistic Regression model achieved the best results with a F1-score of 0.679 on the training dataset and a F1-score of 0.572 on the test dataset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Suggestion Mining from Online Reviews using ULMFiT\nTable caption: Table 3: Performance of different models on the provided train and test dataset for Sub Task A.\n[BOLD] Model | [BOLD] F1 (train) | [BOLD] F1 (test)\n[BOLD] Multinomial Naive Bayes (using Count Vectorizer) | 0.641 | 0.517\n[BOLD] Logistic Regression (using Count Vectorizer) | 0.679 | 0.572\n[BOLD] SVM (Linear Kernel) (using TfIdf Vectorizer) | 0.695 | 0.576\n[BOLD] LSTM (128 LSTM Units) | 0.731 | 0.591\n[BOLD] Provided Baseline | 0.720 | 0.267\n[BOLD] ULMFit* | 0.861 | 0.701\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "53819988-8969-430d-93e2-594f7fb8d007",
    "input": "## Claim\nHere is a claim: These result reveal that there exist trade-offs between the different metrics and that a DLM-based algorithm is better suited to solve the user simulation problem than reinforcement learning Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "23c4f4c9-27e5-4284-8229-470a13efae02",
    "input": "## Claim\nHere is a claim: Furthermore, the scope length of negative instances is at the range of 0-8 tokens, with an average scope length of 2.9 tokens. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 3: Cue and token distribution in the conversational negation corpus.\nTotal negation cues | 2921\nTrue negation cues | 2674\nFalse negation cues | 247\nAverage scope length | 2.9\nAverage sentence length | 13.6\nAverage tweet length | 22.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "773f8b3d-676b-44d0-b830-93b964c3976c",
    "input": "## Claim\nHere is a claim: We gain further improvement by adding monolingual data and get an even higher accuracy of 75.5%, which is 10.1 points higher than the best language model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training\nTable caption: Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.\n[EMPTY] | dev perp \u2193 | dev acc \u2191 | dev wer \u2193 | test perp \u2193 | test acc \u2191 | test wer \u2193\nSpanish-only-LM | 329.68 | 26.6 | 30.47 | 322.26 | 25.1 | 29.62\nEnglish-only-LM | 320.92 | 29.3 | 32.02 | 314.04 | 30.3 | 32.51\nAll:CS-last-LM | 76.64 | 47.8 | 14.56 | 76.97 | 49.2 | 14.13\nAll:Shuffled-LM | 68.00 | 51.8 | 13.64 | 68.72 | 51.4 | 13.89\nCS-only-LM | 43.20 | 60.7 | 12.60 | 43.42 | 57.9 | 12.18\nCS-only+vocab-LM | 45.61 | 61.0 | 12.56 | 45.79 | 58.8 | 12.49\nFine-Tuned-LM | 39.76 | 66.9 | 10.71 | 40.11 | 65.4 | 10.17\nCS-only-disc | \u2013 | 72.0 | 6.35 | \u2013 | 70.5 | 6.70\nFine-Tuned-disc | \u2013 | [BOLD] 74.2 | [BOLD] 5.85 | \u2013 | [BOLD] 75.5 | [BOLD] 5.59\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "10c6f16f-2759-48a2-b4eb-f82c9ecce72e",
    "input": "## Claim\nHere is a claim: Thus, after taking the depth in KG into consideration, the precision increases to 19.47%, which increases the AUC score to 0.413. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 4: Precisions on the Wikidata dataset with different choice of d.\nRecall | 0.1 | 0.2 | 0.3 | AUC | Time\n[ITALIC] d=1 | 0.602 | 0.487 | 0.403 | 0.367 | 4h\n[ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.370 | -\n[ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h\n[ITALIC] d=8 | 0.650 | 0.519 | 0.422 | 0.405 | 8h\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "5e806a3c-f8ff-469f-9d27-e6fc37a34a3d",
    "input": "## Claim\nHere is a claim: Our joint model outperforms all the base [CONTINUE] lines with a gap of 10.5 CoNLL F1 points from the last published results (KCP), while surpassing our strong lemma baseline by 3 points. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\n[BOLD] Model | R | MUC P | [ITALIC] F1 | R | B3 P | [ITALIC] F1 | R | CEAF- [ITALIC] e P | [ITALIC] F1 | CoNLL  [ITALIC] F1\n[BOLD] Baselines | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nCluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5\nCV Cybulska and Vossen ( 2015a ) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73\nKCP Kenyon-Dean et\u00a0al. ( 2018 ) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69\nCluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6\n[BOLD] Model Variants | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nDisjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5\nJoint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | [BOLD] 79.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "cf78ecf8-180e-4067-9f5f-5091c236de7d",
    "input": "## Claim\nHere is a claim: Interestingly, the size and type of whitelist have a significant effect on performance, indicating that all the whitelists do not contain responses appropriate to a variety of conversational contexts. Does the following context support or refute the claim?\n\n## Table\nPaper title: Building a Production Model for Retrieval-Based Chatbots\nTable caption: Table 7: Results of the human evaluation of the responses produced by our model. A response is acceptable if it is either good or great. Note: Numbers may not add up to 100% due to rounding.\n[BOLD] Whitelist | [BOLD] Great | [BOLD] Good | [BOLD] Bad | [BOLD] Accept\nFreq. 1K | 54% | 26% | 20% | 80%\nCluster. 1K | 55% | 21% | 23% | 77%\nFreq. 10K | 56% | 24% | 21% | 80%\nCluster. 10K | 57% | 23% | 20% | 80%\nReal response | 60% | 24% | 16% | 84%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a5ca51e3-76e6-4054-a5b3-f85f9c2987dc",
    "input": "## Claim\nHere is a claim: Most of the false negation cues correspond to contracted negations (e.g., \u201chaven\u2019t\u201d). Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 3: Cue and token distribution in the conversational negation corpus.\nTotal negation cues | 2921\nTrue negation cues | 2674\nFalse negation cues | 247\nAverage scope length | 2.9\nAverage sentence length | 13.6\nAverage tweet length | 22.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "3f8320d5-deeb-473d-ba46-1fead5ed6bde",
    "input": "## Claim\nHere is a claim: We find that the effect of syntactic structure is consistent across the different relation types. Does the following context support or refute the claim?\n\n## Table\nPaper title: Syntactic Dependency Representations in Neural Relation Classification\nTable caption: Table 1: Effect of using the shortest dependency path on each relation type.\n[BOLD] Relation | [BOLD] best F1 (in 5-fold) without sdp | [BOLD] best F1 (in 5-fold) with sdp | [BOLD] Diff.\nUSAGE | 60.34 | 80.24 | + 19.90\nMODEL-FEATURE | 48.89 | 70.00 | + 21.11\nPART_WHOLE | 29.51 | 70.27 | +40.76\nTOPIC | 45.80 | 91.26 | +45.46\nRESULT | 54.35 | 81.58 | +27.23\nCOMPARE | 20.00 | 61.82 | + 41.82\nmacro-averaged | 50.10 | 76.10 | +26.00\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9b196ff7-6836-42f0-9f3e-a6a56543acbd",
    "input": "## Claim\nHere is a claim: Coverage helps the model improve its EM by 1.5 and its F1 by 0.5. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "601420ef-2d1c-4385-a5a1-9fda20185823",
    "input": "## Claim\nHere is a claim: We first use order-based feature which is relative to PPO to show our improvement. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "a5751137-2fe5-4016-8932-c418dc82cae4",
    "input": "## Claim\nHere is a claim: [CONTINUE] In addition, the presence of verbs in past participle (VBN) is the most distinctive part-of-speech pattern of complaints. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\n[BOLD] Complaints  [BOLD] Feature | [BOLD] Complaints  [ITALIC] r | [BOLD] Not Complaints  [BOLD] Feature | [BOLD] Not Complaints  [ITALIC] r\n[BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams\nnot | .154 | [URL] | .150\nmy | .131 | ! | .082\nworking | .124 | he | .069\nstill | .123 | thank | .067\non | .119 | , | .064\ncan\u2019t | .113 | love | .064\nservice | .112 | lol | .061\ncustomer | .109 | you | .060\nwhy | .108 | great | .058\nwebsite | .107 | win | .058\nno | .104 | \u2019 | .058\n? | .098 | she | .054\nfix | .093 | : | .053\nwon\u2019t | .092 | that | .053\nbeen | .090 | more | .052\nissue | .089 | it | .052\ndays | .088 | would | .051\nerror | .087 | him | .047\nis | .084 | life | .046\ncharged | .083 | good | .046\n[BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)\nVBN | .141 | UH | .104\n$ | .118 | NNP | .098\nVBZ | .114 | PRP | .076\nNN_VBZ | .114 | HT | .076\nPRP$ | .107 | PRP_. | .076\nPRP$_NN | .105 | PRP_RB | .067\nVBG | .093 | NNP_NNP | .062\nCD | .092 | VBP_PRP | .054\nWRB_VBZ | .084 | JJ | .053\nVBZ_VBN | .084 | DT_JJ | .051\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ae63ad57-2a1f-45be-84c1-7468ed337a30",
    "input": "## Claim\nHere is a claim: for example, for BERT, the error rates for all the runs are negative with at most 0.05% accuracy loss and at most 0.12% accuracy gain Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large-FT | B-COPA | 74.5 (\u00b1 0.7) | 74.7 (\u00b1 0.4) | [BOLD] 74.4 (\u00b1 0.9)\nBERT-large-FT | B-COPA (50%) | 74.3 (\u00b1 2.2) | 76.8 (\u00b1 1.9) | 72.8 (\u00b1 3.1)\nBERT-large-FT | COPA | [BOLD] 76.5 (\u00b1 2.7) | [BOLD] 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5)\nRoBERTa-large-FT | B-COPA | [BOLD] 89.0 (\u00b1 0.3) | 88.9 (\u00b1 2.1) | [BOLD] 89.0 (\u00b1 0.8)\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (\u00b1 2.2) | 87.4 (\u00b1 1.1) | 85.4 (\u00b1 2.9)\nRoBERTa-large-FT | COPA | 87.7 (\u00b1 0.9) | [BOLD] 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "813352e2-948e-4c73-b239-4420a1634970",
    "input": "## Claim\nHere is a claim: The difference is most prevalent in KP20k, the largest of the four datasets, where our GAN model (at 0.85) is only marginally better than both the other baseline models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Keyphrase Generation for Scientific Articles using GANs\nTable caption: Table 2: \u03b1-nDCG@5 metrics\nModel | Inspec | Krapivin | NUS | KP20k\nCatseq | 0.87803 | 0.781 | 0.82118 | 0.804\nCatseq-RL | 0.8602 | [BOLD] 0.786 | 0.83 | 0.809\nGAN | [BOLD] 0.891 | 0.771 | [BOLD] 0.853 | [BOLD] 0.85\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d2534ed4-c340-4211-8610-924f9fb9c445",
    "input": "## Claim\nHere is a claim: FME outperforms the AME model, confirming the importance of word embeddings adaptation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task\nTable caption: Table 5: Textual similarity scores (asymmetric, Multi30k).\n[EMPTY] | EN \u2192 DE R@1 | EN \u2192 DE R@5 | EN \u2192 DE R@10 | DE \u2192 EN R@1 | DE \u2192 EN R@5 | DE \u2192 EN R@10\nFME | 51.4 | 76.4 | 84.5 | 46.9 | 71.2 | 79.1\nAME | [BOLD] 51.7 | [BOLD] 76.7 | [BOLD] 85.1 | [BOLD] 49.1 | [BOLD] 72.6 | [BOLD] 80.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "bf52da4c-3af3-4cc3-9e6d-19e0744ef2fe",
    "input": "## Claim\nHere is a claim: Patt model could not generate relations for all terms because terms must to be in a pattern in order to have their taxonomic relation identified. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.\nCorpus | Metric | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nEuroparl | TotalTerms: | 957 | 1,000 | 1,000 | 1,000 | 1,000 | 836 | 1,000\nEuroparl | TotalRoots: | 44 | 1 | 1 | 1 | 1 | 43 | 1\nEuroparl | NumberRels: | 1,588 | 1,025 | 1,028 | 1,185 | 1,103 | 1,184 | 999\nEuroparl | MaxDepth: | 21 | 921 | 901 | 788 | 835 | 8 | 15\nEuroparl | MinDepth: | 1 | 921 | 901 | 788 | 835 | 1 | 1\nEuroparl | AvgDepth: | 11.82 | 921 | 901 | 788 | 835 | 3.05 | 8.46\nEuroparl | DepthCohesion: | 1.78 | 1 | 1 | 1 | 1 | 2.62 | 1.77\nEuroparl | MaxWidth: | 20 | 2 | 3 | 4 | 3 | 88 | 41\nEuroparl | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nEuroparl | AvgWidth: | 1.99 | 1.03 | 1.03 | 1.19 | 1.10 | 4.20 | 2.38\nTED Talks | TotalTerms: | 476 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000\nTED Talks | TotalRoots: | 164 | 2 | 1 | 1 | 1 | 1 | 1\nTED Talks | NumberRels: | 521 | 1,029 | 1,331 | 3,025 | 3,438 | 3,802 | 1,009\nTED Talks | MaxDepth: | 16 | 915 | 658 | 454 | 395 | 118 | 12\nTED Talks | MinDepth: | 1 | 913 | 658 | 454 | 395 | 110 | 1\nTED Talks | AvgDepth: | 5.82 | 914 | 658 | 454 | 395 | 112.24 | 5.95\nTED Talks | DepthCohesion: | 2.75 | 1 | 1 | 1 | 1 | 1.05 | 2.02\nTED Talks | MaxWidth: | 25 | 2 | 77 | 13 | 12 | 66 | 98\nTED Talks | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nTED Talks | AvgWidth: | 1.83 | 1.03 | 1.36 | 3.03 | 3.44 | 6.64 | 2.35\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "96c086ca-dbf6-4f2f-b5ca-e65b06ea3b23",
    "input": "## Claim\nHere is a claim: However, the slightly increased invalid response percentage for the DAMD (+) model compared to the HDSA (+) model suggests that data augmentation may not be the most effective approach. We also observe that HDSA (+) outperforms DAMD in both diversity and appropriateness scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.\nModel | Diversity | App | Good% | OK% | Invalid%\nDAMD | 3.12 | 2.50 | 56.5% | [BOLD] 37.4% | 6.1%\nDAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1% | 9.9%\nHDSA (+) | 2.14 | 2.47 | 57.5% | 32.5% | [BOLD] 10.0%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1354fd83-f529-48f9-9a42-981bb82374b2",
    "input": "## Claim\nHere is a claim: When using our learned reward, the generated summaries have significantly higher average human ratings than when using ROUGE as rewards. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. \u201cPref%\u201d: in how many percentage of documents a system receives the higher human rating.\nReward | R-1 | R-2 | R-L | Human | Pref%\nR-L (original) | 40.9 | 17.8 | 38.5 | 1.75 | 15\nLearned (ours) | 39.2 | 17.4 | 37.5 | [BOLD] 2.20 | [BOLD] 75\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "cbb555a9-2b08-4e5d-a9fe-216378072ded",
    "input": "## Claim\nHere is a claim: [CONTINUE] Mentions of time are specific of complaints (been, still, on, days, Temporal References cluster). Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p\n[BOLD] Complaints  [BOLD] Feature | [BOLD] Complaints  [ITALIC] r | [BOLD] Not Complaints  [BOLD] Feature | [BOLD] Not Complaints  [ITALIC] r\n[BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams | [BOLD] Unigrams\nnot | .154 | [URL] | .150\nmy | .131 | ! | .082\nworking | .124 | he | .069\nstill | .123 | thank | .067\non | .119 | , | .064\ncan\u2019t | .113 | love | .064\nservice | .112 | lol | .061\ncustomer | .109 | you | .060\nwhy | .108 | great | .058\nwebsite | .107 | win | .058\nno | .104 | \u2019 | .058\n? | .098 | she | .054\nfix | .093 | : | .053\nwon\u2019t | .092 | that | .053\nbeen | .090 | more | .052\nissue | .089 | it | .052\ndays | .088 | would | .051\nerror | .087 | him | .047\nis | .084 | life | .046\ncharged | .083 | good | .046\n[BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams) | [BOLD] POS (Unigrams and Bigrams)\nVBN | .141 | UH | .104\n$ | .118 | NNP | .098\nVBZ | .114 | PRP | .076\nNN_VBZ | .114 | HT | .076\nPRP$ | .107 | PRP_. | .076\nPRP$_NN | .105 | PRP_RB | .067\nVBG | .093 | NNP_NNP | .062\nCD | .092 | VBP_PRP | .054\nWRB_VBZ | .084 | JJ | .053\nVBZ_VBN | .084 | DT_JJ | .051\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "1ff5fc91-911a-4368-876e-b26811139368",
    "input": "## Claim\nHere is a claim: Our summaries are notably shorter than in other works, about 260 words on average. Does the following context support or refute the claim?\n\n## Table\nPaper title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\nTable caption: Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.\n[BOLD] Dataset | [BOLD] # pairs | [BOLD] # words (doc) | [BOLD] # sents (docs) | [BOLD] # words (summary) | [BOLD] # sents (summary) | [BOLD] vocab size\nMulti-News | 44,972/5,622/5,622 | 2,103.49 | 82.73 | 263.66 | 9.97 | 666,515\nDUC03+04 | 320 | 4,636.24 | 173.15 | 109.58 | 2.88 | 19,734\nTAC 2011 | 176 | 4,695.70 | 188.43 | 99.70 | 1.00 | 24,672\nCNNDM | 287,227/13,368/11,490 | 810.57 | 39.78 | 56.20 | 3.68 | 717,951\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d6d08181-0eff-4e32-8f7a-d1d0134e99c2",
    "input": "## Claim\nHere is a claim: Note that using discriminative training, even with no additional monolingual data, leads to better performance than that of the best language model: the CS-ONLY-DISCRIMINATIVE model achieves an accuracy of 74.2%, 0.3 points less than the accuracy of the FINE-TUNED-LM model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training\nTable caption: Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.\n[EMPTY] | dev perp \u2193 | dev acc \u2191 | dev wer \u2193 | test perp \u2193 | test acc \u2191 | test wer \u2193\nSpanish-only-LM | 329.68 | 26.6 | 30.47 | 322.26 | 25.1 | 29.62\nEnglish-only-LM | 320.92 | 29.3 | 32.02 | 314.04 | 30.3 | 32.51\nAll:CS-last-LM | 76.64 | 47.8 | 14.56 | 76.97 | 49.2 | 14.13\nAll:Shuffled-LM | 68.00 | 51.8 | 13.64 | 68.72 | 51.4 | 13.89\nCS-only-LM | 43.20 | 60.7 | 12.60 | 43.42 | 57.9 | 12.18\nCS-only+vocab-LM | 45.61 | 61.0 | 12.56 | 45.79 | 58.8 | 12.49\nFine-Tuned-LM | 39.76 | 66.9 | 10.71 | 40.11 | 65.4 | 10.17\nCS-only-disc | \u2013 | 72.0 | 6.35 | \u2013 | 70.5 | 6.70\nFine-Tuned-disc | \u2013 | [BOLD] 74.2 | [BOLD] 5.85 | \u2013 | [BOLD] 75.5 | [BOLD] 5.59\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4a849ae1-eaa2-49ab-b46d-23bba1169582",
    "input": "## Claim\nHere is a claim: It does not come close to VGS on paraphrase retrieval, and it does not correlate with the visual modality better. Does the following context support or refute the claim?\n\n## Table\nPaper title: On the difficulty of a distributional semantics of spoken language\nTable caption: Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.\n[EMPTY] | Recall@10 (%) | Median rank | RSAimage\nVGS | 27 | 6 | 0.4\nSegMatch | [BOLD] 10 | [BOLD] 37 | [BOLD] 0.5\nAudio2vec-U | 5 | 105 | 0.0\nAudio2vec-C | 2 | 647 | 0.0\nMean MFCC | 1 | 1,414 | 0.0\nChance | 0 | 3,955 | 0.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9cd68def-c73f-452d-add9-53415403de26",
    "input": "## Claim\nHere is a claim: ACER and PPO do not obtain high performance in inform F1 and match rate. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "bead6cbe-98f0-4891-a5fc-d3de82369621",
    "input": "## Claim\nHere is a claim: In general, the performance of the model does not drop substantially as we remove more dense connections. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\n-{4} dense block | 24.8 | 54.9\n-{3, 4} dense blocks | 23.8 | 54.1\n-{2, 3, 4} dense blocks | 23.2 | 53.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4c507350-fae9-4a57-8c67-cfddd0d800b6",
    "input": "## Claim\nHere is a claim: After removing the graph attention module, our model gives 22.9 BLEU points. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\nEncoder Modules | [EMPTY] | [EMPTY]\n-Linear Combination | 23.7 | 53.2\n-Global Node | 24.2 | 54.6\n-Direction Aggregation | 24.6 | 54.6\n-Graph Attention | 24.9 | 54.7\n-Global Node&Linear Combination | 22.9 | 52.4\nDecoder Modules | [EMPTY] | [EMPTY]\n-Coverage Mechanism | 23.8 | 53.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "00cd7c4e-19f7-4e98-876a-c7c38277a86d",
    "input": "## Claim\nHere is a claim: On the other hand, the number of distinct MRs rose sharply after reannotation; the MRs also have more variance in the number of attributes. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 1: Data statistics comparison for the original E2E data and our cleaned version (number of distinct MRs, total number of textual references, SER as measured by our slot matching script, see Section\u00a03).\n[BOLD] Dataset | [BOLD] Part | [BOLD] MRs | [BOLD] Refs | [BOLD] SER(%)\nOriginal | Train | 4,862 | 42,061 | 17.69\nOriginal | Dev | 547 | 4,672 | 11.42\nOriginal | Test | 630 | 4,693 | 11.49\n[0.5pt/2pt] Cleaned | Train | 8,362 | 33,525 | (0.00)\n[0.5pt/2pt] Cleaned | Dev | 1,132 | 4,299 | (0.00)\n[0.5pt/2pt] Cleaned | Test | 1,358 | 4,693 | (0.00)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "60340fc9-b2cd-46fc-b451-0981d1000f34",
    "input": "## Claim\nHere is a claim: The topical features such as the LIWC dictionaries (which combine syntactic and semantic information) and Word2Vec topics do not perform as well as the part of speech tags. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.\n[BOLD] Model | [BOLD] Acc | [BOLD] F1 | [BOLD] AUC\nMost Frequent Class | 64.2 | 39.1 | 0.500\nLogistic Regression | [EMPTY] | [EMPTY] | [EMPTY]\nSentiment \u2013 MPQA | 64.2 | 39.1 | 0.499\nSentiment \u2013 NRC | 63.9 | 42.2 | 0.599\nSentiment \u2013 V&B | 68.9 | 60.0 | 0.696\nSentiment \u2013 VADER | 66.0 | 54.2 | 0.654\nSentiment \u2013 Stanford | 68.0 | 55.6 | 0.696\nComplaint Specific (all) | 65.7 | 55.2 | 0.634\nRequest | 64.2 | 39.1 | 0.583\nIntensifiers | 64.5 | 47.3 | 0.639\nDowngraders | 65.4 | 49.8 | 0.615\nTemporal References | 64.2 | 43.7 | 0.535\nPronoun Types | 64.1 | 39.1 | 0.545\nPOS Bigrams | 72.2 | 66.8 | 0.756\nLIWC | 71.6 | 65.8 | 0.784\nWord2Vec Clusters | 67.7 | 58.3 | 0.738\nBag-of-Words | 79.8 | 77.5 | 0.866\nAll Features | [BOLD] 80.5 | [BOLD] 78.0 | [BOLD] 0.873\nNeural Networks | [EMPTY] | [EMPTY] | [EMPTY]\nMLP | 78.3 | 76.2 | 0.845\nLSTM | 80.2 | 77.0 | 0.864\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1ff499d0-7570-4df6-9550-0be662ce31b5",
    "input": "## Claim\nHere is a claim: In fact, DocSub had worse results in precision when using both Europarl and Ted Talks corpora in English, where DF reached best values of precision and f-measure. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4d93d55b-d069-4e2d-b720-69b1ff950af3",
    "input": "## Claim\nHere is a claim: However, our model generates shorter sentences than human arguments, with about 15 words per sentence compared to 22 words per sentence for human arguments. Does the following context support or refute the claim?\n\n## Table\nPaper title: Argument Generation with Retrieval, Planning, and Realization\nTable caption: Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.\n[EMPTY] | [ITALIC] w/ System Retrieval  [BOLD] B-2 | [ITALIC] w/ System Retrieval  [BOLD] B-4 | [ITALIC] w/ System Retrieval  [BOLD] R-2 | [ITALIC] w/ System Retrieval  [BOLD] MTR | [ITALIC] w/ System Retrieval  [BOLD] #Word | [ITALIC] w/ System Retrieval  [BOLD] #Sent | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4 | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent\nHuman | - | - | - | - | 66 | 22 | - | - | - | - | 66 | 22\nRetrieval | 7.55 | 1.11 | 8.64 | 14.38 | 123 | 23 | 10.97 | 3.05 | 23.49 | 20.08 | 140 | 21\n[BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [EMPTY] | [EMPTY]\nSeq2seq | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15 | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15\nSeq2seqAug | 8.26 | 2.24 | 13.79 | 15.75 | 78 | 14 | 10.98 | 4.41 | 22.97 | 19.62 | 71 | 14\n[ITALIC] w/o psg | 7.94 | 2.28 | 10.13 | 15.71 | 75 | 12 | 9.89 | 3.34 | 14.20 | 18.40 | 66 | 12\nH&W\u00a0Hua and Wang ( 2018 ) | 3.64 | 0.92 | 8.83 | 11.78 | 51 | 12 | 8.51 | 2.86 | 18.89 | 17.18 | 58 | 12\n[BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [EMPTY] | [EMPTY]\nCANDELA | 12.02\u2217 | [BOLD] 2.99\u2217 | [BOLD] 14.93\u2217 | [BOLD] 16.92\u2217 | 119 | 22 | 15.80\u2217 | [BOLD] 5.00\u2217 | [BOLD] 23.75 | [BOLD] 20.18 | 116 | 22\n[ITALIC] w/o psg | [BOLD] 12.33\u2217 | 2.86\u2217 | 14.53\u2217 | 16.60\u2217 | 123 | 23 | [BOLD] 16.33\u2217 | 4.98\u2217 | 23.65 | 19.94 | 123 | 23\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "136c3899-0818-4fac-a86a-9914176d9a8e",
    "input": "## Claim\nHere is a claim: These results show that our model is not as effective in terms of using automatically generated AMR graphs. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M\n[BOLD] Model | [BOLD] External | B\nSeq2SeqK (Konstas et al.,  2017 ) | - | 22.0\nGraphLSTM (Song et al.,  2018 ) | - | 23.3\nGCNSEQ (Damonte and Cohen,  2019 ) | - | 24.4\nDCGCN(single) | - | 25.9\nDCGCN(ensemble) | - | [BOLD] 28.2\nTSP (Song et al.,  2016 ) | ALL | 22.4\nPBMT (Pourdamghani et al.,  2016 ) | ALL | 26.9\nTree2Str (Flanigan et al.,  2016 ) | ALL | 23.0\nSNRG (Song et al.,  2017 ) | ALL | 25.6\nSeq2SeqK (Konstas et al.,  2017 ) | 0.2M | 27.4\nGraphLSTM (Song et al.,  2018 ) | 0.2M | 28.2\nDCGCN(single) | 0.1M | 29.0\nDCGCN(single) | 0.2M | [BOLD] 31.6\nSeq2SeqK (Konstas et al.,  2017 ) | 2M | 32.3\nGraphLSTM (Song et al.,  2018 ) | 2M | 33.6\nSeq2SeqK (Konstas et al.,  2017 ) | 20M | 33.8\nDCGCN(single) | 0.3M | 33.2\nDCGCN(ensemble) | 0.3M | [BOLD] 35.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4a0cb1cb-cb53-4f2f-a292-d8f09739ae8f",
    "input": "## Claim\nHere is a claim: Furthermore, this bias is seemingly not aggravated for fields suggested to be troubled by male stereotypes, such as life and physical sciences, architecture, engineering, computer science and mathematics. Does the following context support or refute the claim?\n\n## Table\nPaper title: Assessing Gender Bias in Machine Translation \u2013 A Case Study with Google Translate\nTable caption: Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table\nCategory | Female (%) | Male (%) | Neutral (%)\nOffice and administrative support | 11.015 | 58.812 | 16.954\nArchitecture and engineering | 2.299 | 72.701 | 10.92\nFarming, fishing, and forestry | 12.179 | 62.179 | 14.744\nManagement | 11.232 | 66.667 | 12.681\nCommunity and social service | 20.238 | 62.5 | 10.119\nHealthcare support | 25.0 | 43.75 | 17.188\nSales and related | 8.929 | 62.202 | 16.964\nInstallation, maintenance, and repair | 5.22 | 58.333 | 17.125\nTransportation and material moving | 8.81 | 62.976 | 17.5\nLegal | 11.905 | 72.619 | 10.714\nBusiness and financial operations | 7.065 | 67.935 | 15.58\nLife, physical, and social science | 5.882 | 73.284 | 10.049\nArts, design, entertainment, sports, and media | 10.36 | 67.342 | 11.486\nEducation, training, and library | 23.485 | 53.03 | 9.091\nBuilding and grounds cleaning and maintenance | 12.5 | 68.333 | 11.667\nPersonal care and service | 18.939 | 49.747 | 18.434\nHealthcare practitioners and technical | 22.674 | 51.744 | 15.116\nProduction | 14.331 | 51.199 | 18.245\nComputer and mathematical | 4.167 | 66.146 | 14.062\nConstruction and extraction | 8.578 | 61.887 | 17.525\nProtective service | 8.631 | 65.179 | 12.5\nFood preparation and serving related | 21.078 | 58.333 | 17.647\nTotal | 11.76 | 58.93 | 15.939\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "f240b702-2fe4-4303-8584-e97282356a54",
    "input": "## Claim\nHere is a claim: Our joint model improves upon the strong lemma baseline by 3.8 points in CoNLL F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n[BOLD] Model | R | MUC P | [ITALIC] F1 | R | B3 P | [ITALIC] F1 | R | CEAF- [ITALIC] e P | [ITALIC] F1 | CoNLL  [ITALIC] F1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | [BOLD] 71.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0ea80dd7-1266-4389-a316-5dea81a6c8e7",
    "input": "## Claim\nHere is a claim: As shown in Table 5, as the required derivation step increases, the PRKGC+NS model suffers from predicting answer entities and generating correct NLDs. Does the following context support or refute the claim?\n\n## Table\nPaper title: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension\nTable caption: Table 5: Performance breakdown of the PRKGC+NS model. Derivation Precision denotes ROUGE-L F1 of generated NLDs.\n# gold NLD steps | Answer Prec. | Derivation Prec.\n1 | 79.2 | 38.4\n2 | 64.4 | 48.6\n3 | 62.3 | 41.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "957b738e-152b-46ad-b45e-b4e422ebe50c",
    "input": "## Claim\nHere is a claim: As filtering out multiple hypernyms might remove also correct relations, the recall values for all corpora are very low. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1038 | 0.0170 | 0.0490 | 0.0641 | 0.0641 | 0.0613 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1282 | 0.0291 | 0.0410 | 0.0270 | 0.0270 | 0.1154 | 0.0661\nP | PT | Europarl | 0.6185 | 0.3744 | 0.4144 | 0.4394 | 0.4394 | [BOLD] 0.7553 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.6308 | 0.4124 | 0.4404 | 0.4515 | 0.4945 | [BOLD] 0.8609 | 0.5295\nR | EN | Europarl | [BOLD] 0.0021 | 0.0004 | 0.0011 | 0.0014 | 0.0014 | 0.0013 | 0.0017\nR | EN | Ted Talks | 0.0011 | 0.0008 | 0.0011 | 0.0008 | 0.0008 | [BOLD] 0.0030 | 0.0018\nR | PT | Europarl | 0.0012 | 0.0008 | 0.0009 | 0.0010 | 0.0010 | [BOLD] 0.0016 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0003 | 0.0009 | 0.0009 | 0.0010 | 0.0010 | [BOLD] 0.0017 | 0.0011\nF | EN | Europarl | [BOLD] 0.0041 | 0.0007 | 0.0021 | 0.0027 | 0.0027 | 0.0026 | 0.0033\nF | EN | Ted Talks | 0.0022 | 0.0016 | 0.0022 | 0.0015 | 0.0015 | [BOLD] 0.0058 | 0.0036\nF | PT | Europarl | 0.0024 | 0.0016 | 0.0018 | 0.0019 | 0.0019 | [BOLD] 0.0031 | 0.0023\n[EMPTY] | PT | Ted Talks | 0.0005 | 0.0018 | 0.0018 | 0.0020 | 0.0021 | [BOLD] 0.0034 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b63fe5ff-755a-4646-b243-780c4301ed03",
    "input": "## Claim\nHere is a claim: Our model achieves higher recall@0.2 and better area under the ROC curve. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\n-Word-ATT | 0.648 | 0.515 | 0.395 | 0.389\n-Capsule | 0.635 | 0.507 | 0.413 | 0.386\nOur Model | 0.650 | 0.519 | 0.422 | 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "2dd3dec1-0bbd-478e-9bd1-8db58b16cf2e",
    "input": "## Claim\nHere is a claim: coreference is thus a very challenging task with low precision and recall over the entire system Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n[BOLD] Model | R | MUC P | [ITALIC] F1 | R | B3 P | [ITALIC] F1 | R | CEAF- [ITALIC] e P | [ITALIC] F1 | CoNLL  [ITALIC] F1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | [BOLD] 71.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "052c61e4-3626-4d15-b461-16728483b42f",
    "input": "## Claim\nHere is a claim: Table 4 shows that GDPL has the largest KL-divergence to the human on the number of dialog turns over the baselines, which implies that GDPL behaves less like the human. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.\nGP-MBCM | ACER | PPO | ALDM | GDPL\n1.666 | 0.775 | 0.639 | 1.069 | [BOLD] 0.238\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "94b2149c-0f67-46ca-823e-e03b57610d66",
    "input": "## Claim\nHere is a claim: Interestingly, the size and type of whitelist seem to have little effect on performance, indicating that all the whitelists contain responses appropriate to a variety of conversational contexts. Does the following context support or refute the claim?\n\n## Table\nPaper title: Building a Production Model for Retrieval-Based Chatbots\nTable caption: Table 7: Results of the human evaluation of the responses produced by our model. A response is acceptable if it is either good or great. Note: Numbers may not add up to 100% due to rounding.\n[BOLD] Whitelist | [BOLD] Great | [BOLD] Good | [BOLD] Bad | [BOLD] Accept\nFreq. 1K | 54% | 26% | 20% | 80%\nCluster. 1K | 55% | 21% | 23% | 77%\nFreq. 10K | 56% | 24% | 21% | 80%\nCluster. 10K | 57% | 23% | 20% | 80%\nReal response | 60% | 24% | 16% | 84%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "9eb45685-012a-4854-aa9a-db7991101942",
    "input": "## Claim\nHere is a claim: [CONTINUE] It is perceptible that GDPL has better performance than GDPL-sess on the task success and is comparable regarding the dialog turns, [CONTINUE] GDPL also outperforms GDPL-discr Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "9324694a-7af5-4508-a05b-580beb45c78c",
    "input": "## Claim\nHere is a claim: Our vector representation is the state of the art, given a sufficient amount of training time. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "08d031b1-0289-4e40-ad4c-c1b6fdc4f9f3",
    "input": "## Claim\nHere is a claim: Third, the learned reward functions based on ROUGE scores worked well in most cases, especially in a direct regression model with CNN-RNN encoder. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f7b605d0-750b-4154-88bb-3876be299a58",
    "input": "## Claim\nHere is a claim: More than 1000 participants are asked to evaluate 10 random dialog sessions generated by each model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "dec09923-481d-4a17-8163-2541473a06cd",
    "input": "## Claim\nHere is a claim: , For Matching Fail and Success, the negative score in other rows implies that the two partitions cannot obtain any reward if the corresponding metric is not satisfied by all sessions in the partition, showing that satisfying Matching Fail, Matching Success, and Success are the most important, followed by Informativeness. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "99af5f47-1b08-4a45-8235-734b854449ad",
    "input": "## Claim\nHere is a claim: Our single model DCGCN(single) does not outperform all the single models, as it only achieves 19.0 and 12.1 BLEU points on the En-De and EnCs tasks, respectively. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 4: Main results on English-German and English-Czech datasets.\n[BOLD] Model | [BOLD] Type | [BOLD] English-German #P | [BOLD] English-German B | [BOLD] English-German C | [BOLD] English-Czech #P | [BOLD] English-Czech B | [BOLD] English-Czech C\nBoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | -\nCNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | -\nBiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | -\nPB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4\nSeq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8\nGGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3\nDCGCN (ours) | Single | [BOLD]  29.7M | [BOLD] 19.0 | [BOLD] 44.1 | [BOLD]  28.3M | [BOLD] 12.1 | [BOLD] 37.1\nSeq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4\nGGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9\nDCGCN (ours) | Ensemble | [BOLD]  149M | [BOLD] 20.5 | [BOLD] 45.8 | [BOLD]  142M | [BOLD] 13.1 | [BOLD] 37.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3c4ed041-3a0d-436a-a068-dccdb94ff06c",
    "input": "## Claim\nHere is a claim: [CONTINUE] LRN accelerates the training over LSTM and SRU by about 20%, Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 2: Test error (ERR) on document classification task. \u201c#Params\u201d: the parameter number in AmaPolar task. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti.\nModel | Model | #Params | AmaPolar ERR | AmaPolar Time | Yahoo ERR | Yahoo Time | AmaFull ERR | AmaFull Time | YelpPolar ERR | YelpPolar Time\nZhang et\u00a0al. ( 2015 ) | Zhang et\u00a0al. ( 2015 ) | - | 6.10 | - | 29.16 | - | 40.57 | - | 5.26 | -\nThis | LSTM | 227K | [BOLD] 4.37 | 0.947 | [BOLD] 24.62 | 1.332 | 37.22 | 1.003 | 3.58 | 1.362\nThis | GRU | 176K | 4.39 | 0.948 | 24.68 | 1.242 | [BOLD] 37.20 | 0.982 | [BOLD] 3.47 | 1.230\nThis | ATR | 74K | 4.78 | 0.867 | 25.33 | 1.117 | 38.54 | 0.836 | 4.00 | 1.124\nWork | SRU | 194K | 4.95 | 0.919 | 24.78 | 1.394 | 38.23 | 0.907 | 3.99 | 1.310\n[EMPTY] | LRN | 151K | 4.98 | [BOLD] 0.731 | 25.07 | [BOLD] 1.038 | 38.42 | [BOLD] 0.788 | 3.98 | [BOLD] 1.022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "384e6200-e560-4998-ab9c-e93a8a58f751",
    "input": "## Claim\nHere is a claim: for example, DAMD with full supervision achieves the best performance (Combined Score), showing the importance of action supervision. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "b7f5e489-e99b-438c-95ff-17378aa3bb9f",
    "input": "## Claim\nHere is a claim: For other attributes such as sentiment distribution and sentiment reliability, the F1 metric based on positive sentiment is comparatively low, because instances of neutral sentiment are simply ignored in calculating the F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 2: Precisions on the Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nRank+ExATT | 0.584 | 0.535 | 0.487 | 0.392\nPCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204\nPCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396\nOur Model | 0.650 | 0.519 | 0.422 | [BOLD] 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "1b72f46f-433f-431f-a9ed-6475f5455096",
    "input": "## Claim\nHere is a claim: However, on the classes like \"clothing\" and \"bodyparts\" our model ZSGNet does not show much better performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Zero-Shot Grounding of Objects from Natural Language Queries\nTable caption: Table 3: Category-wise performance with the default split of Flickr30k Entities.\nMethod | Overall | people | clothing | bodyparts | animals | vehicles | instruments | scene | other\nQRC - VGG(det) | 60.21 | 75.08 | 55.9 | 20.27 | 73.36 | 68.95 | 45.68 | 65.27 | 38.8\nCITE - VGG(det) | 61.89 | [BOLD] 75.95 | 58.50 | 30.78 | [BOLD] 77.03 | [BOLD] 79.25 | 48.15 | 58.78 | 43.24\nZSGNet - VGG (cls) | 60.12 | 72.52 | 60.57 | 38.51 | 63.61 | 64.47 | 49.59 | 64.66 | 41.09\nZSGNet - Res50 (cls) | [BOLD] 63.39 | 73.87 | [BOLD] 66.18 | [BOLD] 45.27 | 73.79 | 71.38 | [BOLD] 58.54 | [BOLD] 66.49 | [BOLD] 45.53\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "59a37071-0e16-4c45-a4d1-dbd161893407",
    "input": "## Claim\nHere is a claim: our extractive summarizer trained with reinforcement learning is rated higher by humans. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "47157bc0-08a1-4857-952c-75b652a9ec42",
    "input": "## Claim\nHere is a claim: Similarly, when using discriminative trainthe FINE-TUNED-DISCRIMINATIVE model ing, outperforms the CS-ONLY-DISCRIMINATIVE model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training\nTable caption: Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.\n[EMPTY] | dev perp \u2193 | dev acc \u2191 | dev wer \u2193 | test perp \u2193 | test acc \u2191 | test wer \u2193\nSpanish-only-LM | 329.68 | 26.6 | 30.47 | 322.26 | 25.1 | 29.62\nEnglish-only-LM | 320.92 | 29.3 | 32.02 | 314.04 | 30.3 | 32.51\nAll:CS-last-LM | 76.64 | 47.8 | 14.56 | 76.97 | 49.2 | 14.13\nAll:Shuffled-LM | 68.00 | 51.8 | 13.64 | 68.72 | 51.4 | 13.89\nCS-only-LM | 43.20 | 60.7 | 12.60 | 43.42 | 57.9 | 12.18\nCS-only+vocab-LM | 45.61 | 61.0 | 12.56 | 45.79 | 58.8 | 12.49\nFine-Tuned-LM | 39.76 | 66.9 | 10.71 | 40.11 | 65.4 | 10.17\nCS-only-disc | \u2013 | 72.0 | 6.35 | \u2013 | 70.5 | 6.70\nFine-Tuned-disc | \u2013 | [BOLD] 74.2 | [BOLD] 5.85 | \u2013 | [BOLD] 75.5 | [BOLD] 5.59\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ff3845c0-9328-4bc8-a651-072c91290d64",
    "input": "## Claim\nHere is a claim: [CONTINUE] Turning to SEM tagging (Table 3, second block), representations from layers 1 through 4 boost the performance to around 87-88%, [CONTINUE] which is far above the UnsupEmb and MFT baselines. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks\nTable caption: Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. \u201cEn\u201d column is an English autoencoder. BLEU scores are given for reference.\n[ITALIC] k | Ar | Es | Fr | Ru | Zh | En\nPOS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy\n0 | 88.0 | 87.9 | 87.9 | 87.8 | 87.7 | 87.4\n1 | 92.4 | 91.9 | 92.1 | 92.1 | 91.5 | 89.4\n2 | 91.9 | 91.8 | 91.8 | 91.8 | 91.3 | 88.3\n3 | 92.0 | 92.3 | 92.1 | 91.6 | 91.2 | 87.9\n4 | 92.1 | 92.4 | 92.5 | 92.0 | 90.5 | 86.9\nSEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy\n0 | 81.9 | 81.9 | 81.8 | 81.8 | 81.8 | 81.2\n1 | 87.9 | 87.7 | 87.8 | 87.9 | 87.7 | 84.5\n2 | 87.4 | 87.5 | 87.4 | 87.3 | 87.2 | 83.2\n3 | 87.8 | 87.9 | 87.9 | 87.3 | 87.3 | 82.9\n4 | 88.3 | 88.6 | 88.4 | 88.1 | 87.7 | 82.1\nBLEU | BLEU | BLEU | BLEU | BLEU | BLEU | BLEU\n[EMPTY] | 32.7 | 49.1 | 38.5 | 34.2 | 32.1 | 96.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "de6493c8-315b-467e-875c-32f51426215c",
    "input": "## Claim\nHere is a claim: the mean KL divergence decreases from 2.098 to 0.238 as we apply more model components to the user simulator, where DP-MBCM and GP-MBCM model the human dialog policy using the LSTM-DQN framework and ACER and PPO model the human dialog policy using the Actor-Critic framework, ALDM and GDPL model the human dialog policy using the Actor-Critic framework, Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.\nGP-MBCM | ACER | PPO | ALDM | GDPL\n1.666 | 0.775 | 0.639 | 1.069 | [BOLD] 0.238\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "54b945d8-03c2-4cd1-8058-4bc1bcb9d223",
    "input": "## Claim\nHere is a claim: [CONTINUE] It is perceptible that GDPL-sess has better performance than GDPL on the task success and is comparable regarding the dialog turns, [CONTINUE] GDPL-discr also outperforms GDPL. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ec04f4f3-b4ce-453b-8d8b-55d0399d1bcf",
    "input": "## Claim\nHere is a claim: For example, the greedy agent says the magic words like \u201cI want to book an experience\u201d at the beginning of the conversation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "2ca8cd4b-2fd2-4e9b-be58-dc16c5750fc9",
    "input": "## Claim\nHere is a claim: As a result, our implementation can train input data of balanced trees with greater throughput than input data of unbalanced trees, but the throughput of the linear dataset increases more significantly when the batch size increases from 1 to 25. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2\nTable caption: Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.\nBatch size | Throughput (instances/s) Balanced | Throughput (instances/s) Moderate | Throughput (instances/s) Linear\n1 | 46.7 | 27.3 | 7.6\n10 | 125.2 | 78.2 | 22.7\n25 | 129.7 | 83.1 | 45.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "85489224-2e94-411e-b51c-ddeefb943583",
    "input": "## Claim\nHere is a claim: we also did try VADER (NLTK implementation), but we ended up with low performance since VADER is not trained for Spanish language and it is implemented for social media Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 8: Sentiment classification evaluation, using different classifiers on the test set.\nClassifier | Positive Sentiment Precision | Positive Sentiment Recall | Positive Sentiment Fscore\nSVM-w/o neg. | 0.57 | 0.72 | 0.64\nSVM-Punct. neg. | 0.58 | 0.70 | 0.63\nSVM-our-neg. | 0.58 | 0.73 | 0.65\nCNN | 0.63 | 0.83 | 0.72\nCNN-LSTM | 0.71 | 0.72 | 0.72\nCNN-LSTM-Our-neg-Ant | [BOLD] 0.78 | [BOLD] 0.77 | [BOLD] 0.78\n[EMPTY] | Negative Sentiment | Negative Sentiment | Negative Sentiment\n[EMPTY] | Precision | Recall | Fscore\nSVM-w/o neg. | 0.78 | 0.86 | 0.82\nSVM-Punct. neg. | 0.78 | 0.87 | 0.83\nSVM-Our neg. | 0.80 | 0.87 | 0.83\nCNN | 0.88 | 0.72 | 0.79\nCNN-LSTM. | 0.83 | 0.83 | 0.83\nCNN-LSTM-our-neg-Ant | [BOLD] 0.87 | [BOLD] 0.87 | [BOLD] 0.87\n[EMPTY] | Train | [EMPTY] | Test\nPositive tweets | 5121 | [EMPTY] | 1320\nNegative tweets | 9094 | [EMPTY] | 2244\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "30e73d0d-6f33-471a-87ca-8d240db19162",
    "input": "## Claim\nHere is a claim: Specifically, BERT+MLP+Pref does not significantly outperform (p < 0.05) all the other models that do not use BERT+MLP. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "29c9462b-ac10-4436-83e3-afab19d30849",
    "input": "## Claim\nHere is a claim: [CONTINUE] RELIS significantly outperforms the other RL-based systems. Does the following context support or refute the claim?\n\n## Table\nPaper title: Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation\nTable caption: Table 3: Results of non-RL (top), cross-input (DeepTD) and input-specific (REAPER) RL approaches (middle) compared with RELIS.\n[EMPTY] | DUC\u201901 <italic>R</italic>1 | DUC\u201901 <italic>R</italic>2 | DUC\u201902 <italic>R</italic>1 | DUC\u201902 <italic>R</italic>2 | DUC\u201904 <italic>R</italic>1 | DUC\u201904 <italic>R</italic>2\nICSI | 33.31 | 7.33 | 35.04 | 8.51 | 37.31 | 9.36\nPriorSum | 35.98 | 7.89 | 36.63 | 8.97 | 38.91 | 10.07\nTCSum | <bold>36.45</bold> | 7.66 | 36.90 | 8.61 | 38.27 | 9.66\nTCSum\u2212 | 33.45 | 6.07 | 34.02 | 7.39 | 35.66 | 8.66\nSRSum | 36.04 | 8.44 | <bold>38.93</bold> | <bold>10.29</bold> | 39.29 | 10.70\nDeepTD | 28.74 | 5.95 | 31.63 | 7.09 | 33.57 | 7.96\nREAPER | 32.43 | 6.84 | 35.03 | 8.11 | 37.22 | 8.64\nRELIS | 34.73 | <bold>8.66</bold> | 37.11 | 9.12 | <bold>39.34</bold> | <bold>10.73</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "e5e0d1cc-b6b3-4ce5-bd6b-3a00628c32ba",
    "input": "## Claim\nHere is a claim: We find EWC outperforms the L2 approach Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 3: Test BLEU for es-en adaptive training. EWC reduces forgetting compared to other fine-tuning methods, while offering the greatest improvement on the new domain.\n[EMPTY] | [BOLD] Training scheme | [BOLD] Health | [BOLD] Bio\n1 | Health | [BOLD] 35.9 | 33.1\n2 | Bio | 29.6 | 36.1\n3 | Health and Bio | 35.8 | 37.2\n4 | 1 then Bio, No-reg | 30.3 | 36.6\n5 | 1 then Bio, L2 | 35.1 | 37.3\n6 | 1 then Bio, EWC | 35.2 | [BOLD] 37.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d13b3ebf-01ff-42c0-9b3e-999d9d28f9cd",
    "input": "## Claim\nHere is a claim: [CONTINUE] MIL-ND achieves higher precision, recall, and F1 than MIL, [CONTINUE] Using its confidence at test time (\u03c4 MIL-ND, 'All' setting) was also beneficial in terms of precision and F1 (it cannot possibly increase recall). Does the following context support or refute the claim?\n\n## Table\nPaper title: Distant Learning for Entity Linking with Automatic Noise Detection\nTable caption: Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.\nSystem | All P | All R | All F1 | In  [ITALIC] E+ P | In  [ITALIC] E+ R | In  [ITALIC] E+ F1\nName matching | 15.03 | 15.03 | 15.03 | 29.13 | 29.13 | 29.13\nMIL (model 1) | 35.87 | 35.87 | 35.87 \u00b10.72 | 69.38 | 69.38 | 69.38 \u00b11.29\nMIL-ND (model 2) | 37.42 | [BOLD] 37.42 | 37.42 \u00b10.35 | 72.50 | [BOLD] 72.50 | [BOLD] 72.50 \u00b10.68\n[ITALIC] \u03c4MIL-ND (model 2) | [BOLD] 38.91 | 36.73 | [BOLD] 37.78 \u00b10.26 | [BOLD] 73.19 | 71.15 | 72.16 \u00b10.48\nSupervised learning | 42.90 | 42.90 | 42.90 \u00b10.59 | 83.12 | 83.12 | 83.12 \u00b11.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "da02f66d-8a1d-4ea0-aea6-e404a1c7037a",
    "input": "## Claim\nHere is a claim: PB-SMT is the phrase-based statistical machine translation model using Moses (Koehn et al., 2007). Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 4: Main results on English-German and English-Czech datasets.\n[BOLD] Model | [BOLD] Type | [BOLD] English-German #P | [BOLD] English-German B | [BOLD] English-German C | [BOLD] English-Czech #P | [BOLD] English-Czech B | [BOLD] English-Czech C\nBoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | -\nCNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | -\nBiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | -\nPB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4\nSeq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8\nGGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3\nDCGCN (ours) | Single | [BOLD]  29.7M | [BOLD] 19.0 | [BOLD] 44.1 | [BOLD]  28.3M | [BOLD] 12.1 | [BOLD] 37.1\nSeq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4\nGGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9\nDCGCN (ours) | Ensemble | [BOLD]  149M | [BOLD] 20.5 | [BOLD] 45.8 | [BOLD]  142M | [BOLD] 13.1 | [BOLD] 37.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "cd32feb8-8dd5-43fa-8568-60aaaffe74d8",
    "input": "## Claim\nHere is a claim: Manual features reduce recall, but do not help the system to improve accuracy and precision. Does the following context support or refute the claim?\n\n## Table\nPaper title: Low-supervision urgency detection and transfer in short crisis messages\nTable caption: TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal\nSystem | Accuracy | Precision | Recall | F-Measure\nLocal | 63.97% | 64.27% | 64.50% | 63.93%\nManual | 64.25% | [BOLD] 70.84%\u2217\u2217 | 48.50% | 57.11%\nWiki | 67.25% | 66.51% | 69.50% | 67.76%\nLocal-Manual | 65.75% | 67.96% | 59.50% | 62.96%\nWiki-Local | 67.40% | 65.54% | 68.50% | 66.80%\nWiki-Manual | 67.75% | 70.38% | 63.00% | 65.79%\n[ITALIC] Our Approach | [BOLD] 69.25%\u2217\u2217\u2217 | 68.76% | [BOLD] 70.50%\u2217\u2217 | [BOLD] 69.44%\u2217\u2217\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6a9321bb-e9c3-4a8c-9109-fc7268df8508",
    "input": "## Claim\nHere is a claim: The best results are shown in bold. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n[BOLD] Model | R | MUC P | [ITALIC] F1 | R | B3 P | [ITALIC] F1 | R | CEAF- [ITALIC] e P | [ITALIC] F1 | CoNLL  [ITALIC] F1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | [BOLD] 71.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "5f1f4b6d-1767-41f9-9314-c9dcd0205077",
    "input": "## Claim\nHere is a claim: The highest values of precision are achieved by DSim model, and the highest recalls are obtained by HClust and Patt models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150 | 0.0150 | 0.0445 | 0.0326\nP | EN | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.0060 | 0.0092 | 0.0090 | 0.0356 | 0.0162\nP | PT | Europarl | 0.5710 | 0.1948 | 0.3855 | 0.5474 | 0.4485 | [BOLD] 0.8052 | 0.4058\n[EMPTY] | PT | Ted Talks | [BOLD] 0.6304 | 0.1870 | 0.3250 | 0.5312 | 0.4576 | 0.6064 | 0.3698\nR | EN | Europarl | 0.0037 | 0.3278 | 0.5941 | 0.6486 | [BOLD] 0.6490 | 0.0017 | 0.0003\nR | EN | Ted Talks | 0.0002 | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332 | 0.0967 | 0.0003\nR | PT | Europarl | 0.0002 | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932 | 0.0032 | 0.0001\n[EMPTY] | PT | Ted Talks | 2.10-5 | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887 | 0.1390 | 0.0002\nF | EN | Europarl | 0.0073 | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033 | 0.0006\nF | EN | Ted Talks | 0.0004 | 0.0132 | 0.0118 | 0.0181 | 0.0179 | [BOLD] 0.0520 | 0.0005\nF | PT | Europarl | 0.0005 | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109 | 0.0064 | 0.0002\n[EMPTY] | PT | Ted Talks | 4.10-5 | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149 | 0.2261 | 0.0004\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d36ed967-4ea2-4de7-809e-04b35275ba96",
    "input": "## Claim\nHere is a claim: This suggests that lemma features enhance (cross-document) coreference performance more than simple cluster features. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\n[BOLD] Model | R | MUC P | [ITALIC] F1 | R | B3 P | [ITALIC] F1 | R | CEAF- [ITALIC] e P | [ITALIC] F1 | CoNLL  [ITALIC] F1\n[BOLD] Baselines | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nCluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5\nCV Cybulska and Vossen ( 2015a ) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73\nKCP Kenyon-Dean et\u00a0al. ( 2018 ) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69\nCluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6\n[BOLD] Model Variants | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nDisjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5\nJoint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | [BOLD] 79.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "a5e07487-b4cc-41c2-bb42-5794acaaf94c",
    "input": "## Claim\nHere is a claim: the relation identification component yields better performance compared to Rank+ExATT. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 2: Precisions on the Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nRank+ExATT | 0.584 | 0.535 | 0.487 | 0.392\nPCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204\nPCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396\nOur Model | 0.650 | 0.519 | 0.422 | [BOLD] 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "736c33a1-48f2-4f67-939b-397fd82f51f2",
    "input": "## Claim\nHere is a claim: Our agent outperforms the comparison agents with a large margin. Does the following context support or refute the claim?\n\n## Table\nPaper title: Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation\nTable caption: Table 6: Results of the Human Rating on CWC.\n[EMPTY] | Ours Better(%) | No Prefer(%) | Ours Worse(%)\nRetrieval-Stgy\u00a0 | [BOLD] 62 | 22 | 16\nPMI\u00a0 | [BOLD] 54 | 32 | 14\nNeural\u00a0 | [BOLD] 60 | 22 | 18\nKernel\u00a0 | [BOLD] 62 | 26 | 12\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "40429ce5-1c82-41e5-8724-5f22da89362c",
    "input": "## Claim\nHere is a claim: GDPL-sess and GDPL-discr mark both pretraining strategies, while GDPL marks the ensemble model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "910499d2-85dd-428e-a7f5-268b24bfa673",
    "input": "## Claim\nHere is a claim: [CONTINUE] Dual2seq is not significantly better than Seq2seq in both settings, [CONTINUE] In particular, the improvement is much smaller under the small-scale setting (+3.2 BLEU) than that under the large-scale setting (+1.7 BLEU). Does the following context support or refute the claim?\n\n## Table\nPaper title: Semantic Neural Machine Translation using AMR\nTable caption: Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.\nSystem | NC-v11 BLEU | NC-v11 TER\u2193 | NC-v11 Meteor | Full BLEU | Full TER\u2193 | Full Meteor\nOpenNMT-tf | 15.1 | 0.6902 | 0.3040 | 24.3 | 0.5567 | 0.4225\nTransformer-tf | 17.1 | 0.6647 | 0.3578 | 25.1 | 0.5537 | 0.4344\nSeq2seq | 16.0 | 0.6695 | 0.3379 | 23.7 | 0.5590 | 0.4258\nDual2seq-LinAMR | 17.3 | 0.6530 | 0.3612 | 24.0 | 0.5643 | 0.4246\nDuel2seq-SRL | 17.2 | 0.6591 | 0.3644 | 23.8 | 0.5626 | 0.4223\nDual2seq-Dep | 17.8 | 0.6516 | 0.3673 | 25.0 | 0.5538 | 0.4328\nDual2seq | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840 | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5b739ed7-b554-4969-8cb2-5d048179aeb5",
    "input": "## Claim\nHere is a claim: [CONTINUE] Our model achieves state-of-the-art results, outperforming previous models by 10.5 CoNLL F1 points on events, Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\n<bold>Baselines</bold> | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nCluster+Lemma | 76.5 | 79.9 | 78.1 | 71.7 | 85 | 77.8 | 75.5 | 71.7 | 73.6 | 76.5\nCV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>) | 71 | 75 | 73 | 71 | 78 | 74 | - | - | 64 | 73\nKCP Kenyon-Dean et\u00a0al. (<ref id='bib-bib14'>2018</ref>) | 67 | 71 | 69 | 71 | 67 | 69 | 71 | 67 | 69 | 69\nCluster+KCP | 68.4 | 79.3 | 73.4 | 67.2 | 87.2 | 75.9 | 77.4 | 66.4 | 71.5 | 73.6\n<bold>Model Variants</bold> | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nDisjoint | 75.5 | 83.6 | 79.4 | 75.4 | 86 | 80.4 | 80.3 | 71.9 | 75.9 | 78.5\nJoint | 77.6 | 84.5 | 80.9 | 76.1 | 85.1 | 80.3 | 81 | 73.8 | 77.3 | <bold>79.5</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "b8a1ebb9-8375-438d-a7a3-af682c33ac69",
    "input": "## Claim\nHere is a claim: Our proposed method outperforms GloVe in semantic analogy test set and in overall results, while GloVe performs slightly better in syntactic test set. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7cead150-e3a2-4135-aacf-47c24848a499",
    "input": "## Claim\nHere is a claim: Without using the dense connections in the last two blocks, the score drops to 23.8. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\n-{4} dense block | 24.8 | 54.9\n-{3, 4} dense blocks | 23.8 | 54.1\n-{2, 3, 4} dense blocks | 23.2 | 53.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "e2aba604-ba6c-4e5b-a15a-91c898f2453a",
    "input": "## Claim\nHere is a claim: It should also be noted that scores obtained by SPINE is unacceptably low on almost all tests indicating that it has achieved its interpretability performance at the cost of losing its semantic functions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VI: Correlations for Word Similarity Tests\nDataset (EN-) | GloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\nWS-353-ALL | 0.612 | 0.7156 | 0.634 | 0.622 | 0.173 | 0.690 | 0.657\nSIMLEX-999 | 0.359 | 0.3939 | 0.295 | 0.355 | 0.090 | 0.380 | 0.381\nVERB-143 | 0.326 | 0.4430 | 0.255 | 0.271 | 0.293 | 0.271 | 0.348\nSimVerb-3500 | 0.193 | 0.2856 | 0.184 | 0.197 | 0.035 | 0.234 | 0.245\nWS-353-REL | 0.578 | 0.6457 | 0.595 | 0.578 | 0.134 | 0.695 | 0.619\nRW-STANF. | 0.378 | 0.4858 | 0.316 | 0.373 | 0.122 | 0.390 | 0.382\nYP-130 | 0.524 | 0.5211 | 0.353 | 0.482 | 0.169 | 0.420 | 0.589\nMEN-TR-3k | 0.710 | 0.7528 | 0.684 | 0.696 | 0.298 | 0.769 | 0.725\nRG-65 | 0.768 | 0.8051 | 0.736 | 0.732 | 0.338 | 0.761 | 0.774\nMTurk-771 | 0.650 | 0.6712 | 0.593 | 0.623 | 0.199 | 0.665 | 0.671\nWS-353-SIM | 0.682 | 0.7883 | 0.713 | 0.702 | 0.220 | 0.720 | 0.720\nMC-30 | 0.749 | 0.8112 | 0.799 | 0.726 | 0.330 | 0.735 | 0.776\nMTurk-287 | 0.649 | 0.6645 | 0.591 | 0.631 | 0.295 | 0.674 | 0.634\nAverage | 0.552 | 0.6141 | 0.519 | 0.538 | 0.207 | 0.570 | 0.579\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "6ba6dcd8-17b3-4498-8a29-4345b38b7aa1",
    "input": "## Claim\nHere is a claim: [CONTINUE] It is clear from Table 5 that using the learned reward helps the RL-based system generate summaries with significantly higher human ratings. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. \u201cPref%\u201d: in how many percentage of documents a system receives the higher human rating.\nReward | R-1 | R-2 | R-L | Human | Pref%\nR-L (original) | 40.9 | 17.8 | 38.5 | 1.75 | 15\nLearned (ours) | 39.2 | 17.4 | 37.5 | [BOLD] 2.20 | [BOLD] 75\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c6f16d71-f550-42fc-87b9-f2796d26bd4c",
    "input": "## Claim\nHere is a claim: at a recall of 1, the d=32 setting already achieve a precision of over 0.8 with a significant gap of 0.2 when compared to the best performance of d=8, indicating that at this recall level, the d=32 model seems to be more effective at identifying mappings. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 4: Precisions on the Wikidata dataset with different choice of d.\nRecall | 0.1 | 0.2 | 0.3 | AUC | Time\n[ITALIC] d=1 | 0.602 | 0.487 | 0.403 | 0.367 | 4h\n[ITALIC] d=32 | 0.645 | 0.501 | 0.393 | 0.370 | -\n[ITALIC] d=16 | 0.655 | 0.518 | 0.413 | 0.413 | 20h\n[ITALIC] d=8 | 0.650 | 0.519 | 0.422 | 0.405 | 8h\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "4827721a-f264-4ddb-8171-fc8d60473bdd",
    "input": "## Claim\nHere is a claim: As an explanation for these differences, we believe that the mixtures of different task profiles allowed participants to learn more detailed topic-dependent aspects (better than a single vector model), particularly in relation to the use of language in subject-oriented communication. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ea35a87b-5630-4eb2-b60a-28894e2b6299",
    "input": "## Claim\nHere is a claim: As expected, the average ranking of samegender pairs is significantly lower than that of different-gender pairs, both for German and Italian, while the difference between the sets in English is much smaller. Does the following context support or refute the claim?\n\n## Table\nPaper title: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?\nTable caption: Table 2: Averages of rankings of the words in same-gender pairs vs. different-gender pairs for Italian and German, along with their differences. Og stands for the original embeddings, Db for the debiased embeddings, and En for English. Each row presents the averages of pairs with the respective scores in SimLex-999 (0\u20134, 4\u20137, 7\u201310).\n[EMPTY] | Italian Same-gender | Italian Diff-Gender | Italian difference | German Same-gender | German Diff-Gender | German difference\n7\u201310 | Og: 4884 | Og: 12947 | Og: 8063 | Og: 5925 | Og: 33604 | Og: 27679\n7\u201310 | Db: 5523 | Db: 7312 | Db: 1789 | Db: 7653 | Db: 26071 | Db: 18418\n7\u201310 | En: 6978 | En: 2467 | En: -4511 | En: 4517 | En: 8666 | En: 4149\n4\u20137 | Og: 10954 | Og: 15838 | Og: 4884 | Og: 19271 | Og: 27256 | Og: 7985\n4\u20137 | Db: 12037 | Db: 12564 | Db: 527 | Db: 24845 | Db: 22970 | Db: -1875\n4\u20137 | En: 15891 | En: 17782 | En: 1891 | En: 13282 | En: 17649 | En: 4367\n0\u20134 | Og: 23314 | Og: 35783 | Og: 12469 | Og: 50983 | Og: 85263 | Og: 34280\n0\u20134 | Db: 26386 | Db: 28067 | Db: 1681 | Db: 60603 | Db: 79081 | Db: 18478\n0\u20134 | En: 57278 | En: 53053 | En: -4225 | En: 41509 | En: 62929 | En: 21420\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "dc8b32c8-ec58-46a5-85b9-4a7eb85ba965",
    "input": "## Claim\nHere is a claim: [CONTINUE] Another interesting fact in Table 1 is that the training throughput on the linear dataset scales better than the throughput on the balanced dataset, as the batch size increases. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2\nTable caption: Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.\nBatch size | Throughput (instances/s) Balanced | Throughput (instances/s) Moderate | Throughput (instances/s) Linear\n1 | 46.7 | 27.3 | 7.6\n10 | 125.2 | 78.2 | 22.7\n25 | 129.7 | 83.1 | 45.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a487bf03-88aa-46db-bf2b-25eba56b5a37",
    "input": "## Claim\nHere is a claim: However, training on B-COPA does not necessarily improve performance on the Hard subset, even when training with all 1000 instances in B-COPA, and when matching the training size of the original COPA (500 instances, B-COPA 50%). Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\nModel | Training data | Overall | Easy | Hard\nBERT-large-FT | B-COPA | 74.5 (\u00b1 0.7) | 74.7 (\u00b1 0.4) | [BOLD] 74.4 (\u00b1 0.9)\nBERT-large-FT | B-COPA (50%) | 74.3 (\u00b1 2.2) | 76.8 (\u00b1 1.9) | 72.8 (\u00b1 3.1)\nBERT-large-FT | COPA | [BOLD] 76.5 (\u00b1 2.7) | [BOLD] 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5)\nRoBERTa-large-FT | B-COPA | [BOLD] 89.0 (\u00b1 0.3) | 88.9 (\u00b1 2.1) | [BOLD] 89.0 (\u00b1 0.8)\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (\u00b1 2.2) | 87.4 (\u00b1 1.1) | 85.4 (\u00b1 2.9)\nRoBERTa-large-FT | COPA | 87.7 (\u00b1 0.9) | [BOLD] 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "d907f6a2-fd3e-47ad-bfba-3223f86388a4",
    "input": "## Claim\nHere is a claim: word vectors generated using our proposed word embedding method using high dimensional, sparse vectors are shown to perform well when used in analogy completion tasks. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "3a52a91a-e771-4f88-91f4-1c3baebb5af8",
    "input": "## Claim\nHere is a claim: we can find that capsule can provide more quantitative performance for our triple prediction task. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\n-Word-ATT | 0.648 | 0.515 | 0.395 | 0.389\n-Capsule | 0.635 | 0.507 | 0.413 | 0.386\nOur Model | 0.650 | 0.519 | 0.422 | 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6000ee88-8d5d-4420-8496-2a6b49fc72ae",
    "input": "## Claim\nHere is a claim: for example, DAMD + multi-action data augmentation performs much better than all the other models, suggesting that it is critical to carefully model system actions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "211e093d-d629-48fa-bdba-ab688d36cc5b",
    "input": "## Claim\nHere is a claim: In both cases, the new embeddings perform better than the original ones. Does the following context support or refute the claim?\n\n## Table\nPaper title: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?\nTable caption: Table 6: Results on SimLex-999 and WordSim-353, in Italian and German, before and after debiasing.\n[EMPTY] | Italian Orig | Italian Debias | German Orig | German Debias\nSimLex | 0.280 | [BOLD] 0.288 | 0.343 | [BOLD] 0.356\nWordSim | 0.548 | [BOLD] 0.577 | 0.547 | [BOLD] 0.553\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0fb25186-f020-4e5a-9e5f-e3d96653c844",
    "input": "## Claim\nHere is a claim: In some cases it seems to make difference in results, e.g., Europarl in Portuguese which increased the precision from P=0.5984 in DF to P=0.6109 in TF, as well as the recall from R=0.5184 in DF to R=0.6727 in TF, resulting in an increase of f-measure from F=0.5555 in DF to F=0.6403 in TF. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a4e8cf0d-5a37-4d81-b804-b773d5b80be4",
    "input": "## Claim\nHere is a claim: Under the same setting, our model does not consistently outperform graph encoders based on recurrent neural networks or gating mechanisms. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.\n[BOLD] Model | [BOLD] T | #P | B | C\nSeq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1\nGGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4\nSeq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5\nGGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5\nDCGCN (ours) | S | [BOLD] 19.1M | 27.9 | 57.3\nDCGCN (ours) | E | 92.5M | [BOLD] 30.4 | [BOLD] 59.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "209c036c-49d4-4305-82fd-7e422df199d1",
    "input": "## Claim\nHere is a claim: We find that when we train STagBL with only its main task\u2014with label set [CONTINUE] In Y contrast, when we include the 'natural subtasks' \"C\" (label [CONTINUE] performance decreases typically by a few percentage points. Does the following context support or refute the claim?\n\n## Table\nPaper title: Neural End-to-End Learning for Computational Argumentation Mining\nTable caption: Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by \u201c:\u201d. Layers from which tasks feed are indicated by respective numbers.\n[EMPTY] | C-F1 100% | C-F1 50% | R-F1 100% | R-F1 50% | F1 100% | F1 50%\nY-3 | 49.59 | 65.37 | 26.28 | 37.00 | 34.35 | 47.25\nY-3:Y<italic>C</italic>-1 | 54.71 | 66.84 | 28.44 | 37.35 | 37.40 | 47.92\nY-3:Y<italic>R</italic>-1 | 51.32 | 66.49 | 26.92 | 37.18 | 35.31 | 47.69\nY-3:Y<italic>C</italic>-3 | <bold>54.58</bold> | 67.66 | <bold>30.22</bold> | <bold>40.30</bold> | <bold>38.90</bold> | <bold>50.51</bold>\nY-3:Y<italic>R</italic>-3 | 53.31 | 66.71 | 26.65 | 35.86 | 35.53 | 46.64\nY-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2 | 52.95 | <bold>67.84</bold> | 27.90 | 39.71 | 36.54 | 50.09\nY-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3 | 54.55 | 67.60 | 28.30 | 38.26 | 37.26 | 48.86\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "b7183bab-8092-4b58-8b4c-32d184f4ece2",
    "input": "## Claim\nHere is a claim: At the same time, RELIS performs on par with neural-based TCSum and SRSum, while it requires significantly less data and time to train, as shown next. Does the following context support or refute the claim?\n\n## Table\nPaper title: Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation\nTable caption: Table 3: Results of non-RL (top), cross-input (DeepTD) and input-specific (REAPER) RL approaches (middle) compared with RELIS.\n[EMPTY] | DUC\u201901 <italic>R</italic>1 | DUC\u201901 <italic>R</italic>2 | DUC\u201902 <italic>R</italic>1 | DUC\u201902 <italic>R</italic>2 | DUC\u201904 <italic>R</italic>1 | DUC\u201904 <italic>R</italic>2\nICSI | 33.31 | 7.33 | 35.04 | 8.51 | 37.31 | 9.36\nPriorSum | 35.98 | 7.89 | 36.63 | 8.97 | 38.91 | 10.07\nTCSum | <bold>36.45</bold> | 7.66 | 36.90 | 8.61 | 38.27 | 9.66\nTCSum\u2212 | 33.45 | 6.07 | 34.02 | 7.39 | 35.66 | 8.66\nSRSum | 36.04 | 8.44 | <bold>38.93</bold> | <bold>10.29</bold> | 39.29 | 10.70\nDeepTD | 28.74 | 5.95 | 31.63 | 7.09 | 33.57 | 7.96\nREAPER | 32.43 | 6.84 | 35.03 | 8.11 | 37.22 | 8.64\nRELIS | 34.73 | <bold>8.66</bold> | 37.11 | 9.12 | <bold>39.34</bold> | <bold>10.73</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "290d694b-8ba9-4b59-ab2d-546e5f5cc385",
    "input": "## Claim\nHere is a claim: The single capsule can capture more useful information, while the word-level attention focuses on the entities. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\n-Word-ATT | 0.648 | 0.515 | 0.395 | 0.389\n-Capsule | 0.635 | 0.507 | 0.413 | 0.386\nOur Model | 0.650 | 0.519 | 0.422 | 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ee82637b-643d-49e2-a8a8-f2553f993db0",
    "input": "## Claim\nHere is a claim: OntoLSTM-PP does not outperform HPCD (full), the previous best result on this dataset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Ontology-Aware Token Embeddings for Prepositional Phrase Attachment\nTable caption: Table 1: Results on belinkov2014exploring\u2019s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et\u00a0al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Sch\u00fctze (2015) on GloVe.\n[BOLD] System | [BOLD] Initialization | [BOLD] Embedding | [BOLD] Resources | [BOLD] Test Acc.\nHPCD (full) | Syntactic-SG | Type | WordNet, VerbNet | 88.7\nLSTM-PP | GloVe | Type | - | 84.3\nLSTM-PP | GloVe-retro | Type | WordNet | 84.8\nOntoLSTM-PP | GloVe-extended | Token | WordNet | [BOLD] 89.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "acf36685-577f-4ce5-b514-630c07cd800c",
    "input": "## Claim\nHere is a claim: Specifically, BERT+MLP+Pref significantly outperforms (p < 0.05) all the other models that do not use BERT+MLP, Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "57547cfd-917f-4e7d-a554-236cf763e4a1",
    "input": "## Claim\nHere is a claim: Several groups of words are much more likely to appear in a complaint, although not used to express complaints per se: about orders or deliveries (in the retail domain), about access (in complaints to service providers) and about parts of tech products (in tech). Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 5: Group text features associated with tweets that are complaints and not complaints. Features are sorted by Pearson correlation (r) between their each feature\u2019s normalized frequency and the outcome. We restrict to only the top six categories for each feature type. All correlations are significant at p\n[BOLD] Complaints  [BOLD] Label | [BOLD] Complaints  [BOLD] Words | [BOLD] Complaints  [ITALIC] r | [BOLD] Not Complaints  [BOLD] Label | [BOLD] Not Complaints  [BOLD] Words | [BOLD] Not Complaints  [ITALIC] r\n[BOLD] LIWC Features | [BOLD] LIWC Features | [BOLD] LIWC Features | [BOLD] LIWC Features | [BOLD] LIWC Features | [BOLD] LIWC Features\nNEGATE | not, no, can\u2019t, don\u2019t, never, nothing, doesn\u2019t, won\u2019t | .271 | POSEMO | thanks, love, thank, good, great, support, lol, win | .185\nRELATIV | in, on, when, at, out, still, now, up, back, new | .225 | AFFECT | thanks, love, thank, good, great, support, lol | .111\nFUNCTION | the, i, to, a, my, and, you, for, is, in | .204 | SHEHE | he, his, she, her, him, he\u2019s, himself | .105\nTIME | when, still, now, back, new, never, after, then, waiting | .186 | MALE | he, his, man, him, sir, he\u2019s, son | .086\nDIFFER | not, but, if, or, can\u2019t, really, than, other, haven\u2019t | .169 | FEMALE | she, her, girl, mom, ma, lady, mother, female, mrs | .084\nCOGPROC | not, but, how, if, all, why, or, any, need | .132 | ASSENT | yes, ok, awesome, okay, yeah, cool, absolutely, agree | .080\n[BOLD] Word2Vec Clusters | [BOLD] Word2Vec Clusters | [BOLD] Word2Vec Clusters | [BOLD] Word2Vec Clusters | [BOLD] Word2Vec Clusters | [BOLD] Word2Vec Clusters\nCust. Service | service, customer, contact, job, staff, assist, agent | .136 | Gratitude | thanks, thank, good, great, support, everyone, huge, proud | .089\nOrder | order, store, buy, free, delivery, available, package | .128 | Family | old, friend, family, mom, wife, husband, younger | .063\nIssues | delayed, closed, between, outage, delay, road, accident | .122 | Voting | favorite, part, stars, model, vote, models, represent | .060\nTime Ref. | been, yet, haven\u2019t, long, happened, yesterday, took | .122 | Contests | Christmas, gift, receive, entered, giveaway, enter, cards | .058\nTech Parts | battery, laptop, screen, warranty, desktop, printer | .100 | Pets | dogs, cat, dog, pet, shepherd, fluffy, treats | .054\nAccess | use, using, error, password, access, automatically, reset | .098 | Christian | god, shall, heaven, spirit, lord, belongs, soul, believers | .053\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "baaa1788-387d-4417-be35-6c9a092846ab",
    "input": "## Claim\nHere is a claim: Adding the dependency weight factor with a window size of 5 improves [CONTINUE] the F1 score by 3.2% (A3\u2212A2). Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.\n[EMPTY] | Prec. | Rec. | F1\n(A1) BiLSTM-CNN | 0.473 | 0.606 | 0.531\n(A2) Standard attention | 0.466 | 0.638 | 0.539\n(A3) Window size ( [ITALIC] ws)=5 | 0.507 | 0.652 | [BOLD] 0.571\n(A4) Window size ( [ITALIC] ws)=10 | 0.510 | 0.640 | 0.568\n(A5) Softmax | 0.490 | 0.658 | 0.562\n(A6) Max-pool | 0.492 | 0.600 | 0.541\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "69091183-93a9-443b-9f2b-248ee7ef89fc",
    "input": "## Claim\nHere is a claim: GDPL does not outperform three baselines significantly in all aspects (sign test, p-value < 0.01), including the quality compared with ACER. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "f5f06ec5-0b30-4907-a0bd-d39536b0aae2",
    "input": "## Claim\nHere is a claim: These results show that our model is more effective in terms of using automatically generated AMR graphs. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M\n[BOLD] Model | [BOLD] External | B\nSeq2SeqK (Konstas et al.,  2017 ) | - | 22.0\nGraphLSTM (Song et al.,  2018 ) | - | 23.3\nGCNSEQ (Damonte and Cohen,  2019 ) | - | 24.4\nDCGCN(single) | - | 25.9\nDCGCN(ensemble) | - | [BOLD] 28.2\nTSP (Song et al.,  2016 ) | ALL | 22.4\nPBMT (Pourdamghani et al.,  2016 ) | ALL | 26.9\nTree2Str (Flanigan et al.,  2016 ) | ALL | 23.0\nSNRG (Song et al.,  2017 ) | ALL | 25.6\nSeq2SeqK (Konstas et al.,  2017 ) | 0.2M | 27.4\nGraphLSTM (Song et al.,  2018 ) | 0.2M | 28.2\nDCGCN(single) | 0.1M | 29.0\nDCGCN(single) | 0.2M | [BOLD] 31.6\nSeq2SeqK (Konstas et al.,  2017 ) | 2M | 32.3\nGraphLSTM (Song et al.,  2018 ) | 2M | 33.6\nSeq2SeqK (Konstas et al.,  2017 ) | 20M | 33.8\nDCGCN(single) | 0.3M | 33.2\nDCGCN(ensemble) | 0.3M | [BOLD] 35.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a36005d4-f76f-435c-8f9b-08eab8c4e8c7",
    "input": "## Claim\nHere is a claim: the joint training of encoder and question decoder achieves 10.99 EM and 50.10 F1 for QA-SRL and better results for QA-SRL than MQAN. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "97aab644-cd95-4b66-bc5e-b81e249e9808",
    "input": "## Claim\nHere is a claim: as shown in Figure 5(a), recall@100 is increasing at different recall thresholds, the best result is achieved at r=0.3, which is the average number of tags of each training sample Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nIteration=1 | 0.531 | 0.455 | 0.353 | 0.201\nIteration=2 | 0.592 | 0.498 | 0.385 | 0.375\nIteration=3 | 0.650 | 0.519 | 0.422 | 0.405\nIteration=4 | 0.601 | 0.505 | 0.422 | 0.385\nIteration=5 | 0.575 | 0.495 | 0.394 | 0.376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6f90e958-7eef-46a6-8b4a-bfbc7d4b391f",
    "input": "## Claim\nHere is a claim: Similarly, when using discriminative training, the CS-ONLY-DISCRIMINATIVE model outperforms the FINE-TUNED-DISCRIMINATIVE model. Does the following context support or refute the claim?\n\n## Table\nPaper title: Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training\nTable caption: Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.\n[EMPTY] | dev perp \u2193 | dev acc \u2191 | dev wer \u2193 | test perp \u2193 | test acc \u2191 | test wer \u2193\nSpanish-only-LM | 329.68 | 26.6 | 30.47 | 322.26 | 25.1 | 29.62\nEnglish-only-LM | 320.92 | 29.3 | 32.02 | 314.04 | 30.3 | 32.51\nAll:CS-last-LM | 76.64 | 47.8 | 14.56 | 76.97 | 49.2 | 14.13\nAll:Shuffled-LM | 68.00 | 51.8 | 13.64 | 68.72 | 51.4 | 13.89\nCS-only-LM | 43.20 | 60.7 | 12.60 | 43.42 | 57.9 | 12.18\nCS-only+vocab-LM | 45.61 | 61.0 | 12.56 | 45.79 | 58.8 | 12.49\nFine-Tuned-LM | 39.76 | 66.9 | 10.71 | 40.11 | 65.4 | 10.17\nCS-only-disc | \u2013 | 72.0 | 6.35 | \u2013 | 70.5 | 6.70\nFine-Tuned-disc | \u2013 | [BOLD] 74.2 | [BOLD] 5.85 | \u2013 | [BOLD] 75.5 | [BOLD] 5.59\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "c12dfef6-a48b-4994-9d35-bac3557acb18",
    "input": "## Claim\nHere is a claim: We see a constant increase in sentiment value in both directions across all three models after finetuning demonstrating that the framework is able to pick up on words that are indicative of sentiment. Does the following context support or refute the claim?\n\n## Table\nPaper title: What do Deep Networks Like to Read?\nTable caption: Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.\n[EMPTY] | <bold>RNN</bold> | <bold>CNN</bold> | <bold>DAN</bold>\nPositive | +9.7 | +4.3 | +<bold>23.6</bold>\nNegative | +6.9 | +5.5 | +<bold>16.1</bold>\nFlipped to Positive | +20.2 | +24.9 | +27.4\nFlipped to Negative | +31.5 | +28.6 | +19.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "e095aa11-0827-46e0-b728-c21f8b99a728",
    "input": "## Claim\nHere is a claim: However, NSP is able to capture \u201cfalse\u201d causal information because it can match, e.g., the antecedent with but or because, which may help it show an advantage on less challenging examples. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f684361e-9ba9-42b7-b25d-ea65f81115a2",
    "input": "## Claim\nHere is a claim: While the clustering whitelists have higher recall, the frequency whitelists have higher coverage. Does the following context support or refute the claim?\n\n## Table\nPaper title: Building a Production Model for Retrieval-Based Chatbots\nTable caption: Table 6: Recall@1 versus coverage for frequency and clustering whitelists.\n[BOLD] Whitelist | [BOLD] R@1 | [BOLD] Coverage\nFrequency 10K | 0.136 | 45.04%\nClustering 10K | 0.164 | 38.38%\nFrequency 1K | 0.273 | 33.38%\nClustering 1K | 0.331 | 23.28%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "719df65c-e7e1-4d81-862e-9f799a929714",
    "input": "## Claim\nHere is a claim: In general, both of our principles can improve all the models in any ablative condition (i.e., P1, P2, P1+P2). Does the following context support or refute the claim?\n\n## Table\nPaper title: Two Causal Principles for Improving Visual Dialog\nTable caption: Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table\u00a01. Note that only applying P2 is implemented by the implementations in Section\u00a05 with the history shortcut.\nModel | LF\u00a0 | HCIAE\u00a0 | CoAtt\u00a0 | RvA\u00a0\nbaseline | 57.21 | 56.98 | 56.46 | 56.74\n+P1 | 61.88 | 60.12 | 60.27 | 61.02\n+P2 | 72.65 | 71.50 | 71.41 | 71.44\n+P1+P2 | [BOLD] 73.63 | 71.99 | 71.87 | 72.88\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0535cc2d-61f9-49b2-954f-40fa67d32687",
    "input": "## Claim\nHere is a claim: Instead, we use different combinations of the IWE table. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "25bcab8e-9441-4c2e-beda-8c47ffe86585",
    "input": "## Claim\nHere is a claim: [CONTINUE] Supervising path attentions (the PRKGC+NS model) is indeed effective for improving the human interpretability of generated NLDs. Does the following context support or refute the claim?\n\n## Table\nPaper title: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension\nTable caption: Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).\nModel | Answerability Macro P/R/F | # Answerable | Answer Prec. | Derivation Prec. RG-L (P/R/F) | Derivation Prec. BL-4\nShortest Path | 54.8/55.5/53.2 | 976 | 3.6 | 56.7/38.5/41.5 | 31.3\nPRKGC | 52.6/51.5/50.7 | 1,021 | 45.2 | 40.7/60.7/44.7 | 30.9\nPRKGC+NS | 53.6/54.1/52.1 | 980 | 45.4 | 42.2/61.6/46.1 | 33.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7994ec04-0d3b-4a69-8433-deafd2d52158",
    "input": "## Claim\nHere is a claim: Apart from the flipped results of the LSTM-800 and the LSTM-400, small differences in CV score are usually associated with large discrepancies in test set performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents\nTable caption: Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.\nID LSTM-800 | 5-fold CV 70.56 | \u0394 0.66 | Single model 67.54 | \u0394 0.78 | Ensemble 67.65 | \u0394 0.30\nLSTM-400 | 70.50 | 0.60 | [BOLD] 67.59 | 0.83 | [BOLD] 68.00 | 0.65\nIN-TITLE | 70.11 | 0.21 | [EMPTY] | [EMPTY] | 67.52 | 0.17\n[BOLD] SUBMISSION | 69.90 | \u2013 | 66.76 | \u2013 | 67.35 | \u2013\nNO-HIGHWAY | 69.72 | \u22120.18 | 66.42 | \u22120.34 | 66.64 | \u22120.71\nNO-OVERLAPS | 69.46 | \u22120.44 | 65.07 | \u22121.69 | 66.47 | \u22120.88\nLSTM-400-DROPOUT | 69.45 | \u22120.45 | 65.53 | \u22121.23 | 67.28 | \u22120.07\nNO-TRANSLATIONS | 69.42 | \u22120.48 | 65.92 | \u22120.84 | 67.23 | \u22120.12\nNO-ELMO-FINETUNING | 67.71 | \u22122.19 | 65.16 | \u22121.60 | 65.42 | \u22121.93\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "8b8dabcd-08f8-434a-8cab-38912a86d4c9",
    "input": "## Claim\nHere is a claim: The relative improvement averaged over all tasks is less than 8%. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "1cdd42db-f5b4-4e6e-989c-aa0d126f6ee8",
    "input": "## Claim\nHere is a claim: While the frequency whitelists have higher recall, the clustering whitelists have higher coverage. Does the following context support or refute the claim?\n\n## Table\nPaper title: Building a Production Model for Retrieval-Based Chatbots\nTable caption: Table 6: Recall@1 versus coverage for frequency and clustering whitelists.\n[BOLD] Whitelist | [BOLD] R@1 | [BOLD] Coverage\nFrequency 10K | 0.136 | 45.04%\nClustering 10K | 0.164 | 38.38%\nFrequency 1K | 0.273 | 33.38%\nClustering 1K | 0.331 | 23.28%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "8e7135c9-5245-4dc2-a2ad-75c47cd2ee70",
    "input": "## Claim\nHere is a claim: As shown in Table 6, reducing the number of attention heads severely decreases multitasking performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Localization of Fake News Detection via Multitask Transfer Learning\nTable caption: Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. \u201cEffect\u201d refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.\n# of Heads | Accuracy | Val. Loss | Effect\n1 | 89.44% | 0.2811 | -6.84%\n2 | 91.20% | 0.2692 | -5.08%\n4 | 93.85% | 0.2481 | -2.43%\n8 | 96.02% | 0.2257 | -0.26%\n10 | 96.28% | 0.2197 | [EMPTY]\n16 | 96.32% | 0.2190 | +0.04\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3926cb33-082d-4658-b949-24978f01cc9f",
    "input": "## Claim\nHere is a claim: Table 1 shows that our proposed token level embedding scheme OntoLSTM-PP does not outperform the better variant of our baseline LSTM-PP (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%. Does the following context support or refute the claim?\n\n## Table\nPaper title: Ontology-Aware Token Embeddings for Prepositional Phrase Attachment\nTable caption: Table 1: Results on belinkov2014exploring\u2019s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et\u00a0al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Sch\u00fctze (2015) on GloVe.\n[BOLD] System | [BOLD] Initialization | [BOLD] Embedding | [BOLD] Resources | [BOLD] Test Acc.\nHPCD (full) | Syntactic-SG | Type | WordNet, VerbNet | 88.7\nLSTM-PP | GloVe | Type | - | 84.3\nLSTM-PP | GloVe-retro | Type | WordNet | 84.8\nOntoLSTM-PP | GloVe-extended | Token | WordNet | [BOLD] 89.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "06a83460-1e39-4475-811c-5697454932b9",
    "input": "## Claim\nHere is a claim: As hard coreference problems are rare in standard coreference datasets, we do not have significant performance improvement. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.\nSystem | MUC | BCUB | CEAFe | AVG\nACE | ACE | ACE | ACE | ACE\nIlliCons | [BOLD] 78.17 | 81.64 | [BOLD] 78.45 | [BOLD] 79.42\nKnowComb | 77.51 | [BOLD] 81.97 | 77.44 | 78.97\nOntoNotes | OntoNotes | OntoNotes | OntoNotes | OntoNotes\nIlliCons | 84.10 | [BOLD] 78.30 | [BOLD] 68.74 | [BOLD] 77.05\nKnowComb | [BOLD] 84.33 | 78.02 | 67.95 | 76.76\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "502a5b58-34d5-4304-a106-9b6ab93d3401",
    "input": "## Claim\nHere is a claim: StateNet PS outperforms StateNet, and StateNet PSI performs best among all 3 models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Universal Dialogue State Tracking\nTable caption: Table 1: Joint goal accuracy on DSTC2 and WOZ 2.0 test set vs.\u00a0various approaches as reported in the literature.\n[BOLD] DST Models | [BOLD] Joint Acc. DSTC2 | [BOLD] Joint Acc. WOZ 2.0\nDelexicalisation-Based (DB) Model Mrk\u0161i\u0107 et\u00a0al. ( 2017 ) | 69.1 | 70.8\nDB Model + Semantic Dictionary Mrk\u0161i\u0107 et\u00a0al. ( 2017 ) | 72.9 | 83.7\nScalable Multi-domain DST Rastogi et\u00a0al. ( 2017 ) | 70.3 | -\nMemN2N Perez and Liu ( 2017 ) | 74.0 | -\nPtrNet Xu and Hu ( 2018 ) | 72.1 | -\nNeural Belief Tracker: NBT-DNN Mrk\u0161i\u0107 et\u00a0al. ( 2017 ) | 72.6 | 84.4\nNeural Belief Tracker: NBT-CNN Mrk\u0161i\u0107 et\u00a0al. ( 2017 ) | 73.4 | 84.2\nBelief Tracking: Bi-LSTM Ramadan et\u00a0al. ( 2018 ) | - | 85.1\nBelief Tracking: CNN Ramadan et\u00a0al. ( 2018 ) | - | 85.5\nGLAD Zhong et\u00a0al. ( 2018 ) | 74.5 | 88.1\nStateNet | 74.1 | 87.8\nStateNet_PS | 74.5 | 88.2\n[BOLD] StateNet_PSI | [BOLD] 75.5 | [BOLD] 88.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3cdfdbe2-07d7-40fb-b7f9-ae12971c7575",
    "input": "## Claim\nHere is a claim: [CONTINUE] The results for the Portuguese corpora are quite similar to the ones generated by the English corpora, having terms without relations in Patt and DocSub, and DSim, SLQS, TF and DF generating deep taxonomies, affirming the characteristics of each method. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 7: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in Portuguese.\nCorpus | Metric | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nEuroparl | TotalTerms: | 980 | 1,000 | 1,000 | 1,000 | 1,000 | 996 | 1,000\nEuroparl | TotalRoots: | 79 | 1 | 1 | 1 | 1 | 1 | 1\nEuroparl | NumberRels: | 1,527 | 1,031 | 1,049 | 1,185 | 1,093 | 1,644 | 999\nEuroparl | MaxDepth: | 19 | 902 | 894 | 784 | 849 | 6 | 10\nEuroparl | MinDepth: | 1 | 902 | 894 | 784 | 849 | 1 | 1\nEuroparl | AvgDepth: | 9.43 | 902 | 894 | 784 | 849 | 2.73 | 4.29\nEuroparl | DepthCohesion: | 2.02 | 1 | 1 | 1 | 1 | 2.19 | 2.33\nEuroparl | MaxWidth: | 27 | 3 | 3 | 4 | 3 | 201 | 58\nEuroparl | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nEuroparl | AvgWidth: | 1.98 | 1.03 | 1.05 | 1.19 | 1.09 | 6.25 | 2.55\nTED Talks | TotalTerms: | 296 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000\nTED Talks | TotalRoots: | 101 | 1 | 1 | 1 | 1 | 1 | 1\nTED Talks | NumberRels: | 291 | 1,045 | 1,229 | 3,637 | 4,284 | 2,875 | 999\nTED Talks | MaxDepth: | 10 | 860 | 727 | 388 | 354 | 252 | 17\nTED Talks | MinDepth: | 1 | 860 | 727 | 388 | 354 | 249 | 1\nTED Talks | AvgDepth: | 3.94 | 860 | 727 | 388 | 354 | 250.43 | 6.16\nTED Talks | DepthCohesion: | 2.54 | 1 | 1 | 1 | 1 | 1.01 | 2.76\nTED Talks | MaxWidth: | 37 | 3 | 79 | 18 | 13 | 9 | 41\nTED Talks | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nTED Talks | AvgWidth: | 1.79 | 1.05 | 1.23 | 3.64 | 4.29 | 2.94 | 2.37\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f495a3a4-3bb4-4d6c-b794-21a9100e5211",
    "input": "## Claim\nHere is a claim: two annotators were used for each dataset. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.\nDataset | Accuracy | Fleiss\u2019 kappa  [ITALIC] k\nOriginal COPA | 100.0 | 0.973\nBalanced COPA | 97.0 | 0.798\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "23b7092a-6047-4963-9ad7-0bd5b23ee3ec",
    "input": "## Claim\nHere is a claim: As for the micro F1 evaluation metric, our model achieves the highest performance (83.54%) on the FNC-1 testing subset. Does the following context support or refute the claim?\n\n## Table\nPaper title: Variational Self-attention Model for Sentence Representation\nTable caption: Table 2: Performance comparison with the state-of-art algorithms on the FNC-1 test dataset.\nModel | Accuracy (%) agree | Accuracy (%) disagree | Accuracy (%) discuss | Accuracy (%) unrelated | Micro F1(%)\nAverage of Word2vec Embedding | 12.43 | 01.30 | 43.32 | 74.24 | 45.53\nCNN-based Sentence Embedding | 24.54 | 05.06 | 53.24 | 79.53 | 81.72\nRNN-based Sentence Embedding | 24.42 | 05.42 | 69.05 | 65.34 | 78.70\nSelf-attention Sentence Embedding | 23.53 | 04.63 | 63.59 | 80.34 | 80.11\nOur model | 28.53 | 10.43 | 65.43 | 82.43 | [BOLD] 83.54\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "c3076ba9-a8f1-40bc-ac14-40e17489112b",
    "input": "## Claim\nHere is a claim: [CONTINUE] we found that En-En encoder-decoders (that is, English autoencoders) produce poor representations for POS and SEM tagging (last column in Table 3). Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks\nTable caption: Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. \u201cEn\u201d column is an English autoencoder. BLEU scores are given for reference.\n[ITALIC] k | Ar | Es | Fr | Ru | Zh | En\nPOS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy | POS Tagging Accuracy\n0 | 88.0 | 87.9 | 87.9 | 87.8 | 87.7 | 87.4\n1 | 92.4 | 91.9 | 92.1 | 92.1 | 91.5 | 89.4\n2 | 91.9 | 91.8 | 91.8 | 91.8 | 91.3 | 88.3\n3 | 92.0 | 92.3 | 92.1 | 91.6 | 91.2 | 87.9\n4 | 92.1 | 92.4 | 92.5 | 92.0 | 90.5 | 86.9\nSEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy | SEM Tagging Accuracy\n0 | 81.9 | 81.9 | 81.8 | 81.8 | 81.8 | 81.2\n1 | 87.9 | 87.7 | 87.8 | 87.9 | 87.7 | 84.5\n2 | 87.4 | 87.5 | 87.4 | 87.3 | 87.2 | 83.2\n3 | 87.8 | 87.9 | 87.9 | 87.3 | 87.3 | 82.9\n4 | 88.3 | 88.6 | 88.4 | 88.1 | 87.7 | 82.1\nBLEU | BLEU | BLEU | BLEU | BLEU | BLEU | BLEU\n[EMPTY] | 32.7 | 49.1 | 38.5 | 34.2 | 32.1 | 96.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0cf5b110-6f1b-4af8-9bcb-61b13bb71044",
    "input": "## Claim\nHere is a claim: our learned reward based evaluation of the lead baseline improves ROUGE precision and recall, relative to normal ROUGE. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "1d4258e4-b9ea-4b5d-a3a0-c3cb126a5fd3",
    "input": "## Claim\nHere is a claim: For DAMD, we fix K=50. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.\nModel | Diversity | App | Good% | OK% | Invalid%\nDAMD | 3.12 | 2.50 | 56.5% | [BOLD] 37.4% | 6.1%\nDAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1% | 9.9%\nHDSA (+) | 2.14 | 2.47 | 57.5% | 32.5% | [BOLD] 10.0%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "d1dff78c-4750-4fd2-b97c-a90c86c61345",
    "input": "## Claim\nHere is a claim: In this task, LRN outperforms ATR and SRU in terms of both EM and F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 4: Exact match/F1-score on SQuad dataset. \u201c#Params\u201d: the parameter number of Base. rnet*: results published by\u00a0Wang et\u00a0al. (2017).\nModel | #Params | Base | +Elmo\nrnet* | - | 71.1/79.5 | -/-\nLSTM | 2.67M | [BOLD] 70.46/78.98 | 75.17/82.79\nGRU | 2.31M | 70.41/ [BOLD] 79.15 | 75.81/83.12\nATR | 1.59M | 69.73/78.70 | 75.06/82.76\nSRU | 2.44M | 69.27/78.41 | 74.56/82.50\nLRN | 2.14M | 70.11/78.83 | [BOLD] 76.14/ [BOLD] 83.83\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "9bf4c3fe-a123-4e6c-98e3-14e4a13d4f09",
    "input": "## Claim\nHere is a claim: [CONTINUE] the FINE-TUNEDDISCRIMINATIVE model is able to prioritize the gold sentence better than all other models, under both conditions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training\nTable caption: Table 5: Accuracy on the dev set and on the test set, according to the type of the gold sentence in the set: code-switched (CS) vs. monolingual (mono).\n[EMPTY] | dev CS | dev mono | test CS | test mono\nCS-only-LM | 45.20 | 65.87 | 43.20 | 62.80\nFine-Tuned-LM | 49.60 | 72.67 | 47.60 | 71.33\nCS-only-disc | [BOLD] 75.60 | 70.40 | 70.80 | 70.53\nFine-Tuned-disc | 70.80 | [BOLD] 74.40 | [BOLD] 75.33 | [BOLD] 75.87\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "32dc7e99-55c4-4b53-aadd-bea2051846d9",
    "input": "## Claim\nHere is a claim: for example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9). Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nIteration=1 | 0.531 | 0.455 | 0.353 | 0.201\nIteration=2 | 0.592 | 0.498 | 0.385 | 0.375\nIteration=3 | 0.650 | 0.519 | 0.422 | 0.405\nIteration=4 | 0.601 | 0.505 | 0.422 | 0.385\nIteration=5 | 0.575 | 0.495 | 0.394 | 0.376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "47128d7b-a5a9-40c7-8e9c-a2dead1340aa",
    "input": "## Claim\nHere is a claim: Compared to Zhou et\\xa0al. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "6172cecc-4b05-4e6d-80ec-97c8a9c62411",
    "input": "## Claim\nHere is a claim: InferSent-Cosine achieves a stronger agreement with the selection of sentences between human and the metric than BERT-Cosine. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nMetric | [ITALIC] \u03c1 | [ITALIC] r | G-Pre | G-Rec\nROUGE-1 | .290 | .304 | .392 | .428\nROUGE-2 | .259 | .278 | .408 | .444\nROUGE-L | .274 | .297 | .390 | .426\nROUGE-SU4 | .282 | .279 | .404 | .440\nBLEU-1 | .256 | .281 | .409 | .448\nBLEU-2 | .301 | .312 | .411 | .446\nBLEU-3 | .317 | .312 | .409 | .444\nBLEU-4 | .311 | .307 | .409 | .446\nBLEU-5 | .308 | .303 | .420 | .459\nMETEOR | .305 | .285 | .409 | .444\nInferSent-Cosine | [BOLD] .329 | [BOLD] .339 | .417 | .460\nBERT-Cosine | .312 | .335 | [BOLD] .440 | [BOLD] .484\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "9bde8eb0-16a1-41f1-bde7-8d9066b32406",
    "input": "## Claim\nHere is a claim: In Table 5, it can be seen that generative pretraining via language modeling does account for a considerable amount of performance, constituting 44.32% of the overall performance (a boost of 42.67% in accuracy) in the multitasking setup, and constituting 43.93% of the overall performance (a boost of 39.97%) in the standard finetuning setup. Does the following context support or refute the claim?\n\n## Table\nPaper title: Localization of Fake News Detection via Multitask Transfer Learning\nTable caption: Table 5: An ablation study on the effects of pretraining for multitasking-based and standard GPT-2 finetuning. Results show that pretraining greatly accounts for almost half of performance on both finetuning techniques. \u201cAcc. Inc.\u201d refers to the boost in performance contributed by the pretraining step. \u201c% of Perf.\u201d refers to the percentage of the total performance that the pretraining step contributes.\nFinetuning | Pretrained? | Accuracy | Val. Loss | Acc. Inc. | % of Perf.\nMultitasking | No | 53.61% | 0.7217 | - | -\n[EMPTY] | Yes | 96.28% | 0.2197 | +42.67% | 44.32%\nStandard | No | 51.02% | 0.7024 | - | -\n[EMPTY] | Yes | 90.99% | 0.1826 | +39.97% | 43.93%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "1f092e69-62e3-4ec5-a7e8-17cc384fd74b",
    "input": "## Claim\nHere is a claim: The results reported in Table 7 show that precision on BDI does not increase as a result of the reduced effect of grammatical gender on the embeddings for German and Italian. Does the following context support or refute the claim?\n\n## Table\nPaper title: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?\nTable caption: Table 7: Cross-lingual embedding alignment in Italian and in German, before and after debiasing.\n[EMPTY] | Italian \u2192 En | Italian En \u2192 | German \u2192 En | German En \u2192\nOrig | 58.73 | 59.68 | 47.58 | 50.48\nDebias | [BOLD] 60.03 | [BOLD] 60.96 | [BOLD] 47.89 | [BOLD] 51.76\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "383dd023-7ed9-4ccf-ae4c-879ef4bdff0a",
    "input": "## Claim\nHere is a claim: The number of examples in our Multi-News dataset is not significantly larger than previous MDS news data. Does the following context support or refute the claim?\n\n## Table\nPaper title: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\nTable caption: Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.\n[BOLD] Dataset | [BOLD] # pairs | [BOLD] # words (doc) | [BOLD] # sents (docs) | [BOLD] # words (summary) | [BOLD] # sents (summary) | [BOLD] vocab size\nMulti-News | 44,972/5,622/5,622 | 2,103.49 | 82.73 | 263.66 | 9.97 | 666,515\nDUC03+04 | 320 | 4,636.24 | 173.15 | 109.58 | 2.88 | 19,734\nTAC 2011 | 176 | 4,695.70 | 188.43 | 99.70 | 1.00 | 24,672\nCNNDM | 287,227/13,368/11,490 | 810.57 | 39.78 | 56.20 | 3.68 | 717,951\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "697c2b0c-fe97-4d2c-b0fe-70e21db0f34d",
    "input": "## Claim\nHere is a claim: [CONTINUE] TRANSFORMER-MULTI is weaker than TRANSFORMER-SINGLE [CONTINUE] .2% overall decrease in performance compared to TRANSFORMER-SINGLE for the goldtwo-mention task. Does the following context support or refute the claim?\n\n## Table\nPaper title: Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns\nTable caption: Table 6: Performance of our baselines on the development set. Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.\n[EMPTY] | M | F | B | O\nRandom | 43.6 | 39.3 | [ITALIC] 0.90 | 41.5\nToken Distance | 50.1 | 42.4 | [ITALIC] 0.85 | 46.4\nTopical Entity | 51.5 | 43.7 | [ITALIC] 0.85 | 47.7\nSyntactic Distance | 63.0 | 56.2 | [ITALIC] 0.89 | 59.7\nParallelism | [BOLD] 67.1 | [BOLD] 63.1 | [ITALIC]  [BOLD] 0.94 | [BOLD] 65.2\nParallelism+URL | [BOLD] 71.1 | [BOLD] 66.9 | [ITALIC]  [BOLD] 0.94 | [BOLD] 69.0\nTransformer-Single | 58.6 | 51.2 | [ITALIC] 0.87 | 55.0\nTransformer-Multi | 59.3 | 52.9 | [ITALIC] 0.89 | 56.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "10362ffd-84e6-423f-a42e-7c91c823a931",
    "input": "## Claim\nHere is a claim: the results show that GDPL, the proposed method, improves the task-completion rate by 27.6% over the state-of-the-art baseline and is 2.43 times closer to the upper bound on this measure as well as 3.22 times closer to the upper bound on the success rate measure over 10 random seeds. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.\nGP-MBCM | ACER | PPO | ALDM | GDPL\n1.666 | 0.775 | 0.639 | 1.069 | [BOLD] 0.238\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "afd33f80-8078-43b6-bc31-857981266ce4",
    "input": "## Claim\nHere is a claim: [CONTINUE] As we can observe in Table 3, Patt has the best values of precision for the English corpora while DocSub has the best values for the Portuguese corpora. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ca818aa7-ae44-4de7-bc61-8995cb899288",
    "input": "## Claim\nHere is a claim: In the natural state space with 75 actions, training does not converge within a reasonable training time. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 5: Performance of different agents on the neural user simulator.\nMethod | VHUS Turns | VHUS Inform | VHUS Match | VHUS Success\nACER | 22.35 | 55.13 | 33.08 | 18.6\nPPO | [BOLD] 19.23 | [BOLD] 56.31 | 33.08 | 18.3\nALDM | 26.90 | 54.37 | 24.15 | 16.4\nGDPL | 22.43 | 52.58 | [BOLD] 36.21 | [BOLD] 19.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "3ce60497-4677-4795-bb32-cfc18403af9e",
    "input": "## Claim\nHere is a claim: In total, 739 tweets (37.6%) are complaints and 1,232 are not complaints (62.4%). Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 3: Number of tweets annotated as complaints across the nine domains.\n[BOLD] Category | [BOLD] Complaints | [BOLD] Not Complaints\nFood & Beverage | 95 | 35\nApparel | 141 | 117\nRetail | 124 | 75\nCars | 67 | 25\nServices | 207 | 130\nSoftware & Online Services | 189 | 103\nTransport | 139 | 109\nElectronics | 174 | 112\nOther | 96 | 33\nTotal | 1232 | 739\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "9c2d8eeb-0c25-43a9-80e9-93904819315d",
    "input": "## Claim\nHere is a claim: In German, we get a reduction of less than 100%. Does the following context support or refute the claim?\n\n## Table\nPaper title: How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?\nTable caption: Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. \u201cReduction\u201d stands for gap reduction when removing gender signals from the context.\n[EMPTY] | Italian Original | Italian Debiased | Italian English | Italian Reduction | German Original | German Debiased | German English | German Reduction\nSame Gender | 0.442 | 0.434 | 0.424 | \u2013 | 0.491 | 0.478 | 0.446 | \u2013\nDifferent Gender | 0.385 | 0.421 | 0.415 | \u2013 | 0.415 | 0.435 | 0.403 | \u2013\ndifference | 0.057 | 0.013 | 0.009 | [BOLD] 91.67% | 0.076 | 0.043 | 0.043 | [BOLD] 100%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3afea35e-02b3-4a45-ab2a-45bc3da9ecc9",
    "input": "## Claim\nHere is a claim: Our NeuralTD system is trained only with a simple learning signal that was automatically induced from human ratings, and outperforms the advanced models with access to the gold labels. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "4c16d276-516e-49e6-a251-401cbabc6596",
    "input": "## Claim\nHere is a claim: The reason may be that a large neural network  (BERT) with its accompanying large input space allows the network to learn a meaningful reward function with greater scope, while the shallower network used in both SimRed and PMeans-RNN may not be adequate for training the same type of reward. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "10c5b2a6-50ed-4507-a418-d52834572fc8",
    "input": "## Claim\nHere is a claim: To further explore the limitations of DAMD, we focus on the 10-Action Generation task. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "5f3db10e-3fe4-4526-b808-dad896f4ac6e",
    "input": "## Claim\nHere is a claim: Because all the test data points are valid for the 'In E+' setting, using the ND classifier had a slight negative effect on F1. Does the following context support or refute the claim?\n\n## Table\nPaper title: Distant Learning for Entity Linking with Automatic Noise Detection\nTable caption: Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.\nSystem | All P | All R | All F1 | In  [ITALIC] E+ P | In  [ITALIC] E+ R | In  [ITALIC] E+ F1\nName matching | 15.03 | 15.03 | 15.03 | 29.13 | 29.13 | 29.13\nMIL (model 1) | 35.87 | 35.87 | 35.87 \u00b10.72 | 69.38 | 69.38 | 69.38 \u00b11.29\nMIL-ND (model 2) | 37.42 | [BOLD] 37.42 | 37.42 \u00b10.35 | 72.50 | [BOLD] 72.50 | [BOLD] 72.50 \u00b10.68\n[ITALIC] \u03c4MIL-ND (model 2) | [BOLD] 38.91 | 36.73 | [BOLD] 37.78 \u00b10.26 | [BOLD] 73.19 | 71.15 | 72.16 \u00b10.48\nSupervised learning | 42.90 | 42.90 | 42.90 \u00b10.59 | 83.12 | 83.12 | 83.12 \u00b11.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ed202752-8bac-401b-89ef-565c1be1319a",
    "input": "## Claim\nHere is a claim: The human evaluation shows that our mirrored instances are comparable in difficulty to the original ones (see Table 3). Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.\nDataset | Accuracy | Fleiss\u2019 kappa  [ITALIC] k\nOriginal COPA | 100.0 | 0.973\nBalanced COPA | 97.0 | 0.798\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "fb0cbc1f-7acb-43e3-b84e-968a72ba2a88",
    "input": "## Claim\nHere is a claim: we see significant improvements in each of the five cases. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f021eb05-ef24-40f8-9729-39dbcb2c1baf",
    "input": "## Claim\nHere is a claim: We see that the optimized parameter settings vary for the different representations, showing the importance of tuning for these types of comparisons. Does the following context support or refute the claim?\n\n## Table\nPaper title: Syntactic Dependency Representations in Neural Relation Classification\nTable caption: Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.\n[BOLD] Representation | [BOLD] Hyper parameters Filter size | [BOLD] Hyper parameters Num. Feature maps | [BOLD] Hyper parameters Activation func. | [BOLD] Hyper parameters L2 Reg. | [BOLD] Hyper parameters Learning rate | [BOLD] Hyper parameters Dropout Prob. | [BOLD] F1.(avg. in 5-fold) with default values | [BOLD] F1.(avg. in 5-fold) with optimal values\nCoNLL08 | 4-5 | 1000 | Softplus | 1.15e+01 | 1.13e-03 | 1 | 73.34 | 74.49\nSB | 4-5 | 806 | Sigmoid | 8.13e-02 | 1.79e-03 | 0.87 | 72.83 | [BOLD] 75.05\nUD v1.3 | 5 | 716 | Softplus | 1.66e+00 | 9.63E-04 | 1 | 68.93 | 69.57\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "82dbb1e6-82ef-4fef-860f-e26ee0e3b964",
    "input": "## Claim\nHere is a claim: The models have better results when handling sentences with 20 or fewer tokens. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "900aab18-0c84-4791-b0e8-3c2d4270ff79",
    "input": "## Claim\nHere is a claim: CorefProp does not improve relation extraction on SciERC. Does the following context support or refute the claim?\n\n## Table\nPaper title: Entity, Relation, and Event Extraction with Contextualized Span Representations\nTable caption: Table 3: F1 scores on Relation.\n[EMPTY] | ACE05 | SciERC | WLPC\nBERT + LSTM | 60.6 | 40.3 | 65.1\n+RelProp | 61.9 | 41.1 | 65.3\n+CorefProp | 59.7 | 42.6 | -\nBERT FineTune | [BOLD] 62.1 | 44.3 | 65.4\n+RelProp | 62.0 | 43.0 | [BOLD] 65.5\n+CorefProp | 60.0 | [BOLD] 45.3 | -\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "337b26da-3751-4e60-b0a0-f78b9af8cafe",
    "input": "## Claim\nHere is a claim: [CONTINUE] Perhaps the most striking thing about the ablation results is that the 'traditional' LSTM layout outsperformed the 'alternating' one we chose for our submission. Does the following context support or refute the claim?\n\n## Table\nPaper title: Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents\nTable caption: Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.\nID LSTM-800 | 5-fold CV 70.56 | \u0394 0.66 | Single model 67.54 | \u0394 0.78 | Ensemble 67.65 | \u0394 0.30\nLSTM-400 | 70.50 | 0.60 | [BOLD] 67.59 | 0.83 | [BOLD] 68.00 | 0.65\nIN-TITLE | 70.11 | 0.21 | [EMPTY] | [EMPTY] | 67.52 | 0.17\n[BOLD] SUBMISSION | 69.90 | \u2013 | 66.76 | \u2013 | 67.35 | \u2013\nNO-HIGHWAY | 69.72 | \u22120.18 | 66.42 | \u22120.34 | 66.64 | \u22120.71\nNO-OVERLAPS | 69.46 | \u22120.44 | 65.07 | \u22121.69 | 66.47 | \u22120.88\nLSTM-400-DROPOUT | 69.45 | \u22120.45 | 65.53 | \u22121.23 | 67.28 | \u22120.07\nNO-TRANSLATIONS | 69.42 | \u22120.48 | 65.92 | \u22120.84 | 67.23 | \u22120.12\nNO-ELMO-FINETUNING | 67.71 | \u22122.19 | 65.16 | \u22121.60 | 65.42 | \u22121.93\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "aff15db7-a64b-4e93-9e29-0a34989164f0",
    "input": "## Claim\nHere is a claim: The semantic threshold for OD-d2v is set at 0.6 while for OD-w2v is set at 0.3. Does the following context support or refute the claim?\n\n## Table\nPaper title: Towards Quantifying the Distance between Opinions\nTable caption: Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.\nTopic Name | Size | TF-IDF ARI | WMD ARI | Sent2vec ARI | Doc2vec ARI | BERT ARI | [ITALIC] OD-w2v ARI | [ITALIC] OD-d2v ARI | TF-IDF  [ITALIC] Sil. | WMD  [ITALIC] Sil. | Sent2vec  [ITALIC] Sil. | Doc2vec  [ITALIC] Sil. | BERT  [ITALIC] Sil. | [ITALIC] OD-w2v  [ITALIC] Sil. | [ITALIC] OD-d2v  [ITALIC] Sil.\nAffirmative Action | 81 | -0.07 | -0.02 | 0.03 | -0.01 | -0.02 | [BOLD] 0.14 | [ITALIC] 0.02 | 0.01 | 0.01 | -0.01 | -0.02 | -0.04 | [BOLD] 0.06 | [ITALIC] 0.01\nAtheism | 116 | [BOLD] 0.19 | 0.07 | 0.00 | 0.03 | -0.01 | 0.11 | [ITALIC] 0.16 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | [ITALIC] 0.05 | [BOLD] 0.07\nAusterity Measures | 20 | [ITALIC] 0.04 | [ITALIC] 0.04 | -0.01 | -0.05 | 0.04 | [BOLD] 0.21 | -0.01 | 0.06 | 0.07 | 0.05 | -0.03 | 0.10 | [BOLD] 0.19 | 0.1\nDemocratization | 76 | 0.02 | -0.01 | 0.00 | [ITALIC] 0.09 | -0.01 | [BOLD] 0.11 | 0.07 | 0.01 | 0.01 | 0.02 | 0.02 | 0.03 | [BOLD] 0.16 | [ITALIC] 0.11\nEducation Voucher Scheme | 30 | [BOLD] 0.25 | 0.12 | 0.08 | -0.02 | 0.04 | 0.13 | [ITALIC] 0.19 | 0.01 | 0.01 | 0.01 | -0.01 | 0.02 | [ITALIC] 0.38 | [BOLD] 0.40\nGambling | 60 | -0.06 | -0.01 | -0.02 | 0.04 | 0.09 | [ITALIC] 0.35 | [BOLD] 0.39 | 0.01 | 0.02 | 0.03 | 0.01 | 0.09 | [BOLD] 0.30 | [ITALIC] 0.22\nHousing | 30 | 0.01 | -0.01 | -0.01 | -0.02 | 0.08 | [BOLD] 0.27 | 0.01 | 0.02 | 0.03 | 0.03 | 0.01 | 0.11 | [BOLD] 0.13 | [ITALIC] 0.13\nHydroelectric Dams | 110 | [BOLD] 0.47 | [ITALIC] 0.45 | [ITALIC] 0.45 | -0.01 | 0.38 | 0.35 | 0.14 | 0.04 | 0.08 | 0.12 | 0.01 | 0.19 | [BOLD] 0.26 | [ITALIC] 0.09\nIntellectual Property | 66 | 0.01 | 0.01 | 0.00 | 0.03 | 0.03 | [ITALIC] 0.05 | [BOLD] 0.14 | 0.01 | [ITALIC] 0.04 | 0.03 | 0.01 | 0.03 | [ITALIC] 0.04 | [BOLD] 0.12\nKeystone pipeline | 18 | 0.01 | 0.01 | 0.00 | -0.13 | [BOLD] 0.07 | -0.01 | [BOLD] 0.07 | -0.01 | -0.03 | -0.03 | -0.07 | 0.03 | [BOLD] 0.05 | [ITALIC] 0.02\nMonarchy | 61 | -0.04 | 0.01 | 0.00 | 0.03 | -0.02 | [BOLD] 0.15 | [BOLD] 0.15 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 | [BOLD] 0.11 | [ITALIC] 0.09\nNational Service | 33 | 0.14 | -0.03 | -0.01 | 0.02 | 0.01 | [ITALIC] 0.31 | [BOLD] 0.39 | 0.02 | 0.04 | 0.02 | 0.01 | 0.02 | [BOLD] 0.25 | [BOLD] 0.25\nOne-child policy China | 67 | -0.05 | 0.01 | [BOLD] 0.11 | -0.02 | 0.02 | [BOLD] 0.11 | 0.01 | 0.01 | 0.02 | [ITALIC] 0.04 | -0.01 | 0.03 | [BOLD] 0.07 | -0.02\nOpen-source Software | 48 | -0.02 | -0.01 | [ITALIC] 0.05 | 0.01 | 0.12 | [BOLD] 0.09 | -0.02 | 0.01 | -0.01 | 0.00 | -0.02 | 0.03 | [BOLD] 0.18 | 0.01\nPornography | 52 | -0.02 | 0.01 | 0.01 | -0.02 | -0.01 | [BOLD] 0.41 | [BOLD] 0.41 | 0.01 | 0.01 | 0.02 | -0.01 | 0.03 | [BOLD] 0.47 | [ITALIC] 0.41\nSeanad Abolition | 25 | 0.23 | 0.09 | -0.01 | -0.01 | 0.03 | [ITALIC] 0.32 | [BOLD] 0.54 | 0.02 | 0.01 | -0.01 | -0.03 | -0.04 | [ITALIC] 0.15 | [BOLD] 0.31\nTrades Unions | 19 | [ITALIC] 0.44 | [ITALIC] 0.44 | [BOLD] 0.60 | -0.05 | 0.44 | [ITALIC] 0.44 | 0.29 | 0.1 | 0.17 | 0.21 | 0.01 | 0.26 | [BOLD] 0.48 | [ITALIC] 0.32\nVideo Games | 72 | -0.01 | 0.01 | 0.12 | 0.01 | 0.08 | [ITALIC] 0.40 | [BOLD] 0.56 | 0.01 | 0.01 | 0.06 | 0.01 | 0.05 | [ITALIC] 0.32 | [BOLD] 0.42\nAverage | 54.67 | 0.09 | 0.07 | 0.08 | 0.01 | 0.08 | [BOLD] 0.22 | [ITALIC] 0.20 | 0.02 | 0.03 | 0.04 | -0.01 | 0.05 | [BOLD] 0.20 | [ITALIC] 0.17\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "c596a875-72aa-4af6-b9de-df0c2111521d",
    "input": "## Claim\nHere is a claim: The proposed approach is seen to perform well against the other unsupervised models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VI: Correlations for Word Similarity Tests\nDataset (EN-) | GloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\nWS-353-ALL | 0.612 | 0.7156 | 0.634 | 0.622 | 0.173 | 0.690 | 0.657\nSIMLEX-999 | 0.359 | 0.3939 | 0.295 | 0.355 | 0.090 | 0.380 | 0.381\nVERB-143 | 0.326 | 0.4430 | 0.255 | 0.271 | 0.293 | 0.271 | 0.348\nSimVerb-3500 | 0.193 | 0.2856 | 0.184 | 0.197 | 0.035 | 0.234 | 0.245\nWS-353-REL | 0.578 | 0.6457 | 0.595 | 0.578 | 0.134 | 0.695 | 0.619\nRW-STANF. | 0.378 | 0.4858 | 0.316 | 0.373 | 0.122 | 0.390 | 0.382\nYP-130 | 0.524 | 0.5211 | 0.353 | 0.482 | 0.169 | 0.420 | 0.589\nMEN-TR-3k | 0.710 | 0.7528 | 0.684 | 0.696 | 0.298 | 0.769 | 0.725\nRG-65 | 0.768 | 0.8051 | 0.736 | 0.732 | 0.338 | 0.761 | 0.774\nMTurk-771 | 0.650 | 0.6712 | 0.593 | 0.623 | 0.199 | 0.665 | 0.671\nWS-353-SIM | 0.682 | 0.7883 | 0.713 | 0.702 | 0.220 | 0.720 | 0.720\nMC-30 | 0.749 | 0.8112 | 0.799 | 0.726 | 0.330 | 0.735 | 0.776\nMTurk-287 | 0.649 | 0.6645 | 0.591 | 0.631 | 0.295 | 0.674 | 0.634\nAverage | 0.552 | 0.6141 | 0.519 | 0.538 | 0.207 | 0.570 | 0.579\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "5c1296f4-0fdf-4ab1-9ef2-c8cb4e4a2e3f",
    "input": "## Claim\nHere is a claim: Word embeddings derived from GloVe outperform Wiki-PubMed-PMC-based embeddings (Table 1). Does the following context support or refute the claim?\n\n## Table\nPaper title: A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data\nTable caption: Table 1: Performance of supervised learning models with different features.\nFeature | LR P | LR R | LR F1 | SVM P | SVM R | SVM F1 | ANN P | ANN R | ANN F1\n+BoW | 0.93 | 0.91 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+BoC (Wiki-PubMed-PMC) | 0.94 | 0.92 | [BOLD] 0.93 | 0.94 | 0.92 | [BOLD] 0.93 | 0.91 | 0.91 | [BOLD] 0.91\n+BoC (GloVe) | 0.93 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n+ASM | 0.90 | 0.85 | 0.88 | 0.90 | 0.86 | 0.88 | 0.89 | 0.89 | 0.89\n+Sentence Embeddings(SEs) | 0.89 | 0.89 | 0.89 | 0.90 | 0.86 | 0.88 | 0.88 | 0.88 | 0.88\n+BoC(Wiki-PubMed-PMC)+SEs | 0.92 | 0.92 | 0.92 | 0.94 | 0.92 | 0.93 | 0.91 | 0.91 | 0.91\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2781ce47-903b-4c89-85a8-6153ef7c5707",
    "input": "## Claim\nHere is a claim: As shown in Table 6, increasing the number of attention heads does not necessarily improve multitasking performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Localization of Fake News Detection via Multitask Transfer Learning\nTable caption: Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. \u201cEffect\u201d refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.\n# of Heads | Accuracy | Val. Loss | Effect\n1 | 89.44% | 0.2811 | -6.84%\n2 | 91.20% | 0.2692 | -5.08%\n4 | 93.85% | 0.2481 | -2.43%\n8 | 96.02% | 0.2257 | -0.26%\n10 | 96.28% | 0.2197 | [EMPTY]\n16 | 96.32% | 0.2190 | +0.04\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2220ceb0-019d-443e-89ee-f7ef12630a84",
    "input": "## Claim\nHere is a claim: The performance of each approach that interacts with the agenda-based user simulator is shown in Table 3, with GDPL outperforming all other methods. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "de861aa8-af5f-4e43-89a4-34d49c90a470",
    "input": "## Claim\nHere is a claim: The reward obtained from other metrics is lower than the blue marker because they have many situations that cannot receive full rewards even in correct behavior. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "08aab654-a0bc-4fc3-9ba0-5c1ae544fc69",
    "input": "## Claim\nHere is a claim: Although the average number of turns of our approach is slightly more than Kernel, our system obtains the highest success rate, significantly improving over other approaches. Does the following context support or refute the claim?\n\n## Table\nPaper title: Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation\nTable caption: Table 4: Results of Self-Play Evaluation.\nSystem | TGPC Succ. (%) | TGPC #Turns | CWC Succ. (%) | CWC #Turns\nRetrieval\u00a0 | 7.16 | 4.17 | 0 | -\nRetrieval-Stgy\u00a0 | 47.80 | 6.7 | 44.6 | 7.42\nPMI\u00a0 | 35.36 | 6.38 | 47.4 | 5.29\nNeural\u00a0 | 54.76 | 4.73 | 47.6 | 5.16\nKernel\u00a0 | 62.56 | 4.65 | 53.2 | 4.08\nDKRN (ours) | [BOLD] 89.0 | 5.02 | [BOLD] 84.4 | 4.20\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "de569012-eb52-4a91-b41c-4f97bd382305",
    "input": "## Claim\nHere is a claim: Although these four models have the same number of layers, dense connections do not necessarily lead to better performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.\n[BOLD] Model | B | C\nDCGCN4 | 25.5 | 55.4\n-{4} dense block | 24.8 | 54.9\n-{3, 4} dense blocks | 23.8 | 54.1\n-{2, 3, 4} dense blocks | 23.2 | 53.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2c5d4216-a6de-4d7c-ba81-4cde5a1639d8",
    "input": "## Claim\nHere is a claim: Table 3 shows the impact of coverage for improving generalization across these two datasets that belong to the two similar tasks of reading comprehension and QA-SRL. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "69d3706c-7e8f-4407-bb39-5eea75a9fb9c",
    "input": "## Claim\nHere is a claim: WOMs are slightly higher for TGen trained on the cleaned data, except for NIST, which gives more importance to matching less frequent n-grams. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Original | TGen\u2212 | 63.37 | 7.7188 | 41.99 | 68.53 | 1.9355 | 00.06 | 15.77 | 00.11 | 15.94\nOriginal | [BOLD] Original | TGen | 66.41 | 8.5565 | 45.07 | 69.17 | 2.2253 | 00.14 | 04.11 | 00.03 | 04.27\nOriginal | [BOLD] Original | TGen+ | 67.06 | 8.5871 | 45.83 | 69.73 | 2.2681 | 00.04 | 01.75 | 00.01 | 01.80\nOriginal | [BOLD] Original | SC-LSTM | 39.11 | 5.6704 | 36.83 | 50.02 | 0.6045 | 02.79 | 18.90 | 09.79 | 31.51\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen\u2212 | 65.87 | 8.6400 | 44.20 | 67.51 | 2.1710 | 00.20 | 00.56 | 00.21 | 00.97\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen | 66.24 | 8.6889 | 44.66 | 67.85 | 2.2181 | 00.10 | 00.02 | 00.00 | 00.12\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | TGen+ | 65.97 | 8.6630 | 44.45 | 67.59 | 2.1855 | 00.02 | 00.00 | 00.00 | 00.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Original | SC-LSTM | 38.52 | 5.7125 | 37.45 | 48.50 | 0.4343 | 03.85 | 17.39 | 08.12 | 29.37\nCleaned missing | [BOLD] Original | TGen\u2212 | 66.28 | 8.5202 | 43.96 | 67.83 | 2.1375 | 00.14 | 02.26 | 00.22 | 02.61\nCleaned missing | [BOLD] Original | TGen | 67.00 | 8.6889 | 44.97 | 68.19 | 2.2228 | 00.06 | 00.44 | 00.03 | 00.53\nCleaned missing | [BOLD] Original | TGen+ | 66.74 | 8.6649 | 44.84 | 67.95 | 2.2018 | 00.00 | 00.21 | 00.03 | 00.24\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen\u2212 | 64.40 | 7.9692 | 42.81 | 68.87 | 2.0563 | 00.01 | 13.08 | 00.00 | 13.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen | 66.23 | 8.5578 | 45.12 | 68.87 | 2.2548 | 00.04 | 03.04 | 00.00 | 03.09\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Original | TGen+ | 65.96 | 8.5238 | 45.49 | 68.79 | 2.2456 | 00.00 | 01.44 | 00.00 | 01.45\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "cc888efa-44f2-4095-adad-3055c1539c12",
    "input": "## Claim\nHere is a claim: [CONTINUE] Apart of the flipped results of the LSTM-800 and the LSTM-400, small differences in CV score are sometimes associated with large discrepancies in test set performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents\nTable caption: Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.\nID LSTM-800 | 5-fold CV 70.56 | \u0394 0.66 | Single model 67.54 | \u0394 0.78 | Ensemble 67.65 | \u0394 0.30\nLSTM-400 | 70.50 | 0.60 | [BOLD] 67.59 | 0.83 | [BOLD] 68.00 | 0.65\nIN-TITLE | 70.11 | 0.21 | [EMPTY] | [EMPTY] | 67.52 | 0.17\n[BOLD] SUBMISSION | 69.90 | \u2013 | 66.76 | \u2013 | 67.35 | \u2013\nNO-HIGHWAY | 69.72 | \u22120.18 | 66.42 | \u22120.34 | 66.64 | \u22120.71\nNO-OVERLAPS | 69.46 | \u22120.44 | 65.07 | \u22121.69 | 66.47 | \u22120.88\nLSTM-400-DROPOUT | 69.45 | \u22120.45 | 65.53 | \u22121.23 | 67.28 | \u22120.07\nNO-TRANSLATIONS | 69.42 | \u22120.48 | 65.92 | \u22120.84 | 67.23 | \u22120.12\nNO-ELMO-FINETUNING | 67.71 | \u22122.19 | 65.16 | \u22121.60 | 65.42 | \u22121.93\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "fd980e0e-8f17-437e-8ed0-1a121f78f7a3",
    "input": "## Claim\nHere is a claim: summary-level BLEU and REG are positively correlated with all metrics (Table\u00a02) and all variants of the trained reward function, which implies that we can optimize our reinforcement learning framework with all existing reward functions Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.\nModel | Encoder | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1 | [ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r | [ITALIC] Reg. loss (Eq. ( 1 )) G-Pre | [ITALIC] Reg. loss (Eq. ( 1 )) G-Rec | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1 | [ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r | [ITALIC] Pref. loss (Eq. ( 3 )) G-Pre | [ITALIC] Pref. loss (Eq. ( 3 )) G-Rec\nMLP | CNN-RNN | .311 | .340 | .486 | .532 | .318 | .335 | .481 | .524\nMLP | PMeans-RNN | .313 | .331 | .489 | .536 | .354 | .375 | .502 | .556\nMLP | BERT | [BOLD] .487 | [BOLD] .526 | [BOLD] .544 | [BOLD] .597 | [BOLD] .505 | [BOLD] .531 | [BOLD] .556 | [BOLD] .608\nSimRed | CNN | .340 | .392 | .470 | .515 | .396 | .443 | .499 | .549\nSimRed | PMeans | .354 | .393 | .493 | .541 | .370 | .374 | .507 | .551\nSimRed | BERT | .266 | .296 | .458 | .495 | .325 | .338 | .485 | .533\nPeyrard and Gurevych ( 2018 ) | Peyrard and Gurevych ( 2018 ) | .177 | .189 | .271 | .306 | .175 | .186 | .268 | .174\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "cf8e09f8-f768-45a9-94df-7a749623df8a",
    "input": "## Claim\nHere is a claim: We consider all words that are semantically related to the words related to the story as negative samples Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "85ebe21b-ff40-4c23-b524-599d069dd7a5",
    "input": "## Claim\nHere is a claim: This indicates that our architecture cannot learn to generate better signals for text generation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "0f052b57-c133-422e-be0d-97281f7665a3",
    "input": "## Claim\nHere is a claim: On the contrary, for the linear dataset, the recursive implementation efficiently makes use of CPU resources and thus the performance gain provided by increasing the batch size is relatively low. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2\nTable caption: Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.\nBatch size | Throughput (instances/s) Balanced | Throughput (instances/s) Moderate | Throughput (instances/s) Linear\n1 | 46.7 | 27.3 | 7.6\n10 | 125.2 | 78.2 | 22.7\n25 | 129.7 | 83.1 | 45.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "dc829323-cd24-4c1e-a8aa-97dd288a0320",
    "input": "## Claim\nHere is a claim: Under oracle setup, all models are notably improved due to the higher quality of reranked passages, and our model achieves statistically significantly better BLEU scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: Argument Generation with Retrieval, Planning, and Realization\nTable caption: Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.\n[EMPTY] | [ITALIC] w/ System Retrieval  [BOLD] B-2 | [ITALIC] w/ System Retrieval  [BOLD] B-4 | [ITALIC] w/ System Retrieval  [BOLD] R-2 | [ITALIC] w/ System Retrieval  [BOLD] MTR | [ITALIC] w/ System Retrieval  [BOLD] #Word | [ITALIC] w/ System Retrieval  [BOLD] #Sent | [ITALIC] w/ Oracle Retrieval  [BOLD] B-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] B-4 | [ITALIC] w/ Oracle Retrieval  [BOLD] R-2 | [ITALIC] w/ Oracle Retrieval  [BOLD] MTR | [ITALIC] w/ Oracle Retrieval  [BOLD] #Word | [ITALIC] w/ Oracle Retrieval  [BOLD] #Sent\nHuman | - | - | - | - | 66 | 22 | - | - | - | - | 66 | 22\nRetrieval | 7.55 | 1.11 | 8.64 | 14.38 | 123 | 23 | 10.97 | 3.05 | 23.49 | 20.08 | 140 | 21\n[BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [BOLD] Comparisons | [EMPTY] | [EMPTY]\nSeq2seq | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15 | 6.92 | 2.13 | 13.02 | 15.08 | 68 | 15\nSeq2seqAug | 8.26 | 2.24 | 13.79 | 15.75 | 78 | 14 | 10.98 | 4.41 | 22.97 | 19.62 | 71 | 14\n[ITALIC] w/o psg | 7.94 | 2.28 | 10.13 | 15.71 | 75 | 12 | 9.89 | 3.34 | 14.20 | 18.40 | 66 | 12\nH&W\u00a0Hua and Wang ( 2018 ) | 3.64 | 0.92 | 8.83 | 11.78 | 51 | 12 | 8.51 | 2.86 | 18.89 | 17.18 | 58 | 12\n[BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [BOLD] Our Models | [EMPTY] | [EMPTY]\nCANDELA | 12.02\u2217 | [BOLD] 2.99\u2217 | [BOLD] 14.93\u2217 | [BOLD] 16.92\u2217 | 119 | 22 | 15.80\u2217 | [BOLD] 5.00\u2217 | [BOLD] 23.75 | [BOLD] 20.18 | 116 | 22\n[ITALIC] w/o psg | [BOLD] 12.33\u2217 | 2.86\u2217 | 14.53\u2217 | 16.60\u2217 | 123 | 23 | [BOLD] 16.33\u2217 | 4.98\u2217 | 23.65 | 19.94 | 123 | 23\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a22b9660-188b-4c29-b248-c811154705b7",
    "input": "## Claim\nHere is a claim: Our approach DKRN does not outperform all state-of-the-art methods in terms of all metrics on both datasets with two tasks. Does the following context support or refute the claim?\n\n## Table\nPaper title: Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation\nTable caption: Table 3: Results of Turn-level Evaluation.\nDataset | System | Keyword Prediction  [ITALIC] Rw@1 | Keyword Prediction  [ITALIC] Rw@3 | Keyword Prediction  [ITALIC] Rw@5 | Keyword Prediction P@1 | Response Retrieval  [ITALIC] R20@1 | Response Retrieval  [ITALIC] R20@3 | Response Retrieval  [ITALIC] R20@5 | Response Retrieval MRR\nTGPC | Retrieval\u00a0 | - | - | - | - | 0.5063 | 0.7615 | 0.8676 | 0.6589\nTGPC | PMI\u00a0 | 0.0585 | 0.1351 | 0.1872 | 0.0871 | 0.5441 | 0.7839 | 0.8716 | 0.6847\nTGPC | Neural\u00a0 | 0.0708 | 0.1438 | 0.1820 | 0.1321 | 0.5311 | 0.7905 | 0.8800 | 0.6822\nTGPC | Kernel\u00a0 | 0.0632 | 0.1377 | 0.1798 | 0.1172 | 0.5386 | 0.8012 | 0.8924 | 0.6877\nTGPC | DKRN (ours) | [BOLD] 0.0909 | [BOLD] 0.1903 | [BOLD] 0.2477 | [BOLD] 0.1685 | [BOLD] 0.5729 | [BOLD] 0.8132 | [BOLD] 0.8966 | [BOLD] 0.7110\nCWC | Retrieval\u00a0 | - | - | - | - | 0.5785 | 0.8101 | 0.8999 | 0.7141\nCWC | PMI\u00a0 | 0.0555 | 0.1001 | 0.1212 | 0.0969 | 0.5945 | 0.8185 | 0.9054 | 0.7257\nCWC | Neural\u00a0 | 0.0654 | 0.1194 | 0.1450 | 0.1141 | 0.6044 | 0.8233 | 0.9085 | 0.7326\nCWC | Kernel\u00a0 | 0.0592 | 0.1113 | 0.1337 | 0.1011 | 0.6017 | 0.8234 | 0.9087 | 0.7320\nCWC | DKRN (ours) | [BOLD] 0.0680 | [BOLD] 0.1254 | [BOLD] 0.1548 | [BOLD] 0.1185 | [BOLD] 0.6324 | [BOLD] 0.8416 | [BOLD] 0.9183 | [BOLD] 0.7533\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "0018d644-5832-4e3f-ac9b-9b6069ff5550",
    "input": "## Claim\nHere is a claim: From the table, we can see that our JMEE framework achieves the best F1 scores for both trigger classification and argumentrelated subtasks among all the compared methods. Does the following context support or refute the claim?\n\n## Table\nPaper title: Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation\nTable caption: Table 1: Overall performance comparing to the state-of-the-art methods with golden-standard entities.\n[BOLD] Method | [BOLD] Trigger  [BOLD] Identification (%) | [BOLD] Trigger  [BOLD] Identification (%) | [BOLD] Trigger  [BOLD] Identification (%) | [BOLD] Trigger  [BOLD] Classification (%) | [BOLD] Trigger  [BOLD] Classification (%) | [BOLD] Trigger  [BOLD] Classification (%) | [BOLD] Argument  [BOLD] Identification (%) | [BOLD] Argument  [BOLD] Identification (%) | [BOLD] Argument  [BOLD] Identification (%) | [BOLD] Argument  [BOLD] Role (%) | [BOLD] Argument  [BOLD] Role (%) | [BOLD] Argument  [BOLD] Role (%)\n[BOLD] Method | [ITALIC] P | [ITALIC] R | [ITALIC] F1 | [ITALIC] P | [ITALIC] R | [ITALIC] F1 | [ITALIC] P | [ITALIC] R | [ITALIC] F1 | [ITALIC] P | [ITALIC] R | [ITALIC] F1\nCross-Event | [EMPTY] | [EMPTY] | [EMPTY] | 68.7 | 68.9 | 68.8 | 50.9 | 49.7 | 50.3 | 45.1 | 44.1 | 44.6\nJointBeam | 76.9 | 65.0 | 70.4 | 73.7 | 62.3 | 67.5 | 69.8 | 47.9 | 56.8 | 64.7 | 44.4 | 52.7\nDMCNN | [BOLD] 80.4 | 67.7 | 73.5 | 75.6 | 63.6 | 69.1 | 68.8 | 51.9 | 59.1 | 62.2 | 46.9 | 53.5\nPSL | [EMPTY] | [EMPTY] | [EMPTY] | 75.3 | 64.4 | 69.4 | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nJRNN | 68.5 | [BOLD] 75.7 | 71.9 | 66.0 | [BOLD] 73.0 | 69.3 | 61.4 | 64.2 | 62.8 | 54.2 | 56.7 | 55.4\ndbRNN | [EMPTY] | [EMPTY] | [EMPTY] | 74.1 | 69.8 | 71.9 | 71.3 | 64.5 | 67.7 | 66.2 | 52.8 | 58.7\n[BOLD] JMEE | 80.2 | 72.1 | [BOLD] 75.9 | [BOLD] 76.3 | 71.3 | [BOLD] 73.7 | [BOLD] 71.4 | [BOLD] 65.6 | [BOLD] 68.4 | [BOLD] 66.8 | [BOLD] 54.9 | [BOLD] 60.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "0c917759-6018-4282-9826-73b13410d748",
    "input": "## Claim\nHere is a claim: For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN1 obtains 20.9 BLEU points, which is higher than DCGCN2 (22.2). Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 7: Comparisons of different DCGCN models under almost the same parameter budget.\n[BOLD] Model | D | #P | B | C\nDCGCN(1) | 300 | 10.9M | 20.9 | 52.0\nDCGCN(2) | 180 | 10.9M | [BOLD] 22.2 | [BOLD] 52.3\nDCGCN(2) | 240 | 11.3M | 22.8 | 52.8\nDCGCN(4) | 180 | 11.4M | [BOLD] 23.4 | [BOLD] 53.4\nDCGCN(1) | 420 | 12.6M | 22.2 | 52.4\nDCGCN(2) | 300 | 12.5M | 23.8 | 53.8\nDCGCN(3) | 240 | 12.3M | [BOLD] 23.9 | [BOLD] 54.1\nDCGCN(2) | 360 | 14.0M | 24.2 | [BOLD] 54.4\nDCGCN(3) | 300 | 14.0M | [BOLD] 24.4 | 54.2\nDCGCN(2) | 420 | 15.6M | 24.1 | 53.7\nDCGCN(4) | 300 | 15.6M | [BOLD] 24.6 | [BOLD] 54.8\nDCGCN(3) | 420 | 18.6M | 24.5 | 54.6\nDCGCN(4) | 360 | 18.4M | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "b3e222d9-7bea-433d-bfad-a5e18e07af19",
    "input": "## Claim\nHere is a claim: Thus, having sparse attention mechanisms in the self-attention layers is beneficial, but the biggest improvement is not necessarily obtained when using TVMAX in the output attention. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.\n[EMPTY] | Att. to image | Att. to bounding boxes | Test-Dev Yes/No | Test-Dev Number | Test-Dev Other | Test-Dev Overall | Test-Standard Yes/No | Test-Standard Number | Test-Standard Other | Test-Standard Overall\nsoftmax | \u2713 | [EMPTY] | 83.08 | 42.65 | 55.74 | 65.52 | 83.55 | 42.68 | 56.01 | 65.97\nsparsemax | \u2713 | [EMPTY] | 83.08 | 43.19 | 55.79 | 65.60 | 83.33 | 42.99 | 56.06 | 65.94\nsoft-TVmax | \u2713 | [EMPTY] | 83.13 | 43.53 | 56.01 | 65.76 | 83.63 | 43.24 | 56.10 | 66.11\nsparse-TVmax | \u2713 | [EMPTY] | 83.10 | 43.30 | 56.14 | 65.79 | 83.66 | 43.18 | 56.21 | 66.17\nsoftmax | [EMPTY] | \u2713 | 85.14 | 49.59 | 58.72 | 68.57 | 85.56 | 49.54 | 59.11 | 69.04\nsparsemax | [EMPTY] | \u2713 | [BOLD] 85.40 | [BOLD] 50.87 | 58.67 | 68.79 | [BOLD] 85.80 | 50.18 | 59.08 | 69.19\nsoftmax | \u2713 | \u2713 | 85.33 | 50.49 | 58.88 | 68.82 | 85.58 | 50.42 | 59.18 | 69.17\nsparse-TVmax | \u2713 | \u2713 | 85.35 | 50.52 | [BOLD] 59.15 | [BOLD] 68.96 | 85.72 | [BOLD] 50.66 | [BOLD] 59.22 | [BOLD] 69.28\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4495c767-a361-43b9-8ebb-0290a8013b03",
    "input": "## Claim\nHere is a claim: The results illustrate the lack of viability of urgency detection in low-supervision settings (with our approach yielding 69.44% F-Measure on Nepal, at 99% significance compared to the Local baseline), with different feature sets contributing differently to the four metrics. Does the following context support or refute the claim?\n\n## Table\nPaper title: Low-supervision urgency detection and transfer in short crisis messages\nTable caption: TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal\nSystem | Accuracy | Precision | Recall | F-Measure\nLocal | 63.97% | 64.27% | 64.50% | 63.93%\nManual | 64.25% | [BOLD] 70.84%\u2217\u2217 | 48.50% | 57.11%\nWiki | 67.25% | 66.51% | 69.50% | 67.76%\nLocal-Manual | 65.75% | 67.96% | 59.50% | 62.96%\nWiki-Local | 67.40% | 65.54% | 68.50% | 66.80%\nWiki-Manual | 67.75% | 70.38% | 63.00% | 65.79%\n[ITALIC] Our Approach | [BOLD] 69.25%\u2217\u2217\u2217 | 68.76% | [BOLD] 70.50%\u2217\u2217 | [BOLD] 69.44%\u2217\u2217\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "5d062ac6-288d-442b-93c9-4946442eb48e",
    "input": "## Claim\nHere is a claim: our approach reliably identifies meanings to sentences that are otherwise challenging even to humans. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "70c9a077-aea5-467e-97cd-520f06da7cd4",
    "input": "## Claim\nHere is a claim: Uniform no-reg ensembling outperforms unadapted uniform ensembling, since fine-tuning gives better in-domain performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Domain Adaptive Inference for Neural Machine Translation\nTable caption: Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.\n[BOLD] Language pair | [BOLD] Model type | [BOLD] Oracle model | [BOLD] Decoder configuration  [BOLD] Uniform | [BOLD] Decoder configuration  [BOLD] BI + IS\nes-en | Unadapted | 36.4 | 34.7 | 36.6\nes-en | No-reg | 36.6 | 34.8 | -\nes-en | EWC | 37.0 | 36.3 | [BOLD] 37.2\nen-de | Unadapted | 36.4 | 26.8 | 38.8\nen-de | No-reg | 41.7 | 31.8 | -\nen-de | EWC | 42.1 | 38.6 | [BOLD] 42.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "550e39a7-dda1-4dac-ac92-76636384b64b",
    "input": "## Claim\nHere is a claim: After integrating Elmo for contextual modeling, the performance of LRN reaches the best (76.1 [CONTINUE] EM and 83.83 F1), beating both GRU and LSTM (+0.33EM, +0.71F1). Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 4: Exact match/F1-score on SQuad dataset. \u201c#Params\u201d: the parameter number of Base. rnet*: results published by\u00a0Wang et\u00a0al. (2017).\nModel | #Params | Base | +Elmo\nrnet* | - | 71.1/79.5 | -/-\nLSTM | 2.67M | [BOLD] 70.46/78.98 | 75.17/82.79\nGRU | 2.31M | 70.41/ [BOLD] 79.15 | 75.81/83.12\nATR | 1.59M | 69.73/78.70 | 75.06/82.76\nSRU | 2.44M | 69.27/78.41 | 74.56/82.50\nLRN | 2.14M | 70.11/78.83 | [BOLD] 76.14/ [BOLD] 83.83\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "534a5798-3961-4596-b9a1-5612d65668aa",
    "input": "## Claim\nHere is a claim: our model achieves better scores in both BLEU and METEOR scores in general, with the largest improvements especially seen in specific categories like \u201cgeography\u201d and \u201cpeople\u201d. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.\n[EMPTY] | Ours | Refresh | ExtAbsRL\nAvg. Human Rating | [BOLD] 2.52 | 2.27 | 1.66\nBest% | [BOLD] 70.0 | 33.3 | 6.7\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "1807ab12-124f-4a77-9ec3-99844ee78da9",
    "input": "## Claim\nHere is a claim: the feature engineering approach only achieved an average of 0.52 F1 score. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "40c5898e-fbfc-4961-b2ec-1a2a06a58791",
    "input": "## Claim\nHere is a claim: the low performance of to can be explained by the fact that as shown in the first part of Table 2, it is responsible for only 4.6% of the inference in the training set. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nCue | App. | Prod. | Cov.\nin | 47 | 55.3 | 9.40\nwas | 55 | 61.8 | 11.0\nto | 82 | 40.2 | 16.4\nthe | 85 | 38.8 | 17.0\na | 106 | 57.5 | 21.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "30481648-b1a9-4760-9e23-c354ea992432",
    "input": "## Claim\nHere is a claim: System A is our new system trained with all data. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "feb01e4b-e4e7-4f6c-a264-d2fb2e3f1962",
    "input": "## Claim\nHere is a claim: Thus, having sparse attention mechanisms in the self-attention layers is beneficial, but the biggest improvement is obtained when using TVMAX in the output attention. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.\n[EMPTY] | Att. to image | Att. to bounding boxes | Test-Dev Yes/No | Test-Dev Number | Test-Dev Other | Test-Dev Overall | Test-Standard Yes/No | Test-Standard Number | Test-Standard Other | Test-Standard Overall\nsoftmax | \u2713 | [EMPTY] | 83.08 | 42.65 | 55.74 | 65.52 | 83.55 | 42.68 | 56.01 | 65.97\nsparsemax | \u2713 | [EMPTY] | 83.08 | 43.19 | 55.79 | 65.60 | 83.33 | 42.99 | 56.06 | 65.94\nsoft-TVmax | \u2713 | [EMPTY] | 83.13 | 43.53 | 56.01 | 65.76 | 83.63 | 43.24 | 56.10 | 66.11\nsparse-TVmax | \u2713 | [EMPTY] | 83.10 | 43.30 | 56.14 | 65.79 | 83.66 | 43.18 | 56.21 | 66.17\nsoftmax | [EMPTY] | \u2713 | 85.14 | 49.59 | 58.72 | 68.57 | 85.56 | 49.54 | 59.11 | 69.04\nsparsemax | [EMPTY] | \u2713 | [BOLD] 85.40 | [BOLD] 50.87 | 58.67 | 68.79 | [BOLD] 85.80 | 50.18 | 59.08 | 69.19\nsoftmax | \u2713 | \u2713 | 85.33 | 50.49 | 58.88 | 68.82 | 85.58 | 50.42 | 59.18 | 69.17\nsparse-TVmax | \u2713 | \u2713 | 85.35 | 50.52 | [BOLD] 59.15 | [BOLD] 68.96 | 85.72 | [BOLD] 50.66 | [BOLD] 59.22 | [BOLD] 69.28\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a2ea71dc-298d-49ba-8699-c91fbcf5159c",
    "input": "## Claim\nHere is a claim: More importantly, their G-Pre and G-Rec scores are all below .50, which means that more than half of the good summaries identified by the metrics are actually not good, and more than 50% Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nMetric | [ITALIC] \u03c1 | [ITALIC] r | G-Pre | G-Rec\nROUGE-1 | .290 | .304 | .392 | .428\nROUGE-2 | .259 | .278 | .408 | .444\nROUGE-L | .274 | .297 | .390 | .426\nROUGE-SU4 | .282 | .279 | .404 | .440\nBLEU-1 | .256 | .281 | .409 | .448\nBLEU-2 | .301 | .312 | .411 | .446\nBLEU-3 | .317 | .312 | .409 | .444\nBLEU-4 | .311 | .307 | .409 | .446\nBLEU-5 | .308 | .303 | .420 | .459\nMETEOR | .305 | .285 | .409 | .444\nInferSent-Cosine | [BOLD] .329 | [BOLD] .339 | .417 | .460\nBERT-Cosine | .312 | .335 | [BOLD] .440 | [BOLD] .484\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "9e01d648-e9ad-493e-979e-f695d4f329f4",
    "input": "## Claim\nHere is a claim: Our KnowComb system achieves the same level of performance as does the state-of-art general coreference system we base it on. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.\nSystem | MUC | BCUB | CEAFe | AVG\nACE | ACE | ACE | ACE | ACE\nIlliCons | [BOLD] 78.17 | 81.64 | [BOLD] 78.45 | [BOLD] 79.42\nKnowComb | 77.51 | [BOLD] 81.97 | 77.44 | 78.97\nOntoNotes | OntoNotes | OntoNotes | OntoNotes | OntoNotes\nIlliCons | 84.10 | [BOLD] 78.30 | [BOLD] 68.74 | [BOLD] 77.05\nKnowComb | [BOLD] 84.33 | 78.02 | 67.95 | 76.76\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "99b4876b-e7fd-48d6-b96a-d9f0c2fedb05",
    "input": "## Claim\nHere is a claim: Although the average number of turns of our approach is slightly more than Kernel, the success rate of our system is not significantly better than other approaches. Does the following context support or refute the claim?\n\n## Table\nPaper title: Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation\nTable caption: Table 4: Results of Self-Play Evaluation.\nSystem | TGPC Succ. (%) | TGPC #Turns | CWC Succ. (%) | CWC #Turns\nRetrieval\u00a0 | 7.16 | 4.17 | 0 | -\nRetrieval-Stgy\u00a0 | 47.80 | 6.7 | 44.6 | 7.42\nPMI\u00a0 | 35.36 | 6.38 | 47.4 | 5.29\nNeural\u00a0 | 54.76 | 4.73 | 47.6 | 5.16\nKernel\u00a0 | 62.56 | 4.65 | 53.2 | 4.08\nDKRN (ours) | [BOLD] 89.0 | 5.02 | [BOLD] 84.4 | 4.20\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "b598c64e-ca52-4716-a6b8-4c78cbbc2195",
    "input": "## Claim\nHere is a claim: HAN models do not outperform both LogReg and SVM using the current set of features. Does the following context support or refute the claim?\n\n## Table\nPaper title: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks\nTable caption: Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\n[BOLD] System | [BOLD] ROUGE-1  [BOLD] R (%) | [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%) | [BOLD] ROUGE-2  [BOLD] R (%) | [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%) | [BOLD] Sentence-Level  [BOLD] R (%) | [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)\n[BOLD] ILP | 24.5 | 41.1 | 29.3\u00b10.5 | 7.9 | 15.0 | 9.9\u00b10.5 | 13.6 | 22.6 | 15.6\u00b10.4\n[BOLD] Sum-Basic | 28.4 | 44.4 | 33.1\u00b10.5 | 8.5 | 15.6 | 10.4\u00b10.4 | 14.7 | 22.9 | 16.7\u00b10.5\n[BOLD] KL-Sum | 39.5 | 34.6 | 35.5\u00b10.5 | 13.0 | 12.7 | 12.3\u00b10.5 | 15.2 | 21.1 | 16.3\u00b10.5\n[BOLD] LexRank | 42.1 | 39.5 | 38.7\u00b10.5 | 14.7 | 15.3 | 14.2\u00b10.5 | 14.3 | 21.5 | 16.0\u00b10.5\n[BOLD] MEAD | 45.5 | 36.5 | 38.5\u00b1 0.5 | 17.9 | 14.9 | 15.4\u00b10.5 | 27.8 | 29.2 | 26.8\u00b10.5\n[BOLD] SVM | 19.0 | 48.8 | 24.7\u00b10.8 | 7.5 | 21.1 | 10.0\u00b10.5 | 32.7 | 34.3 | 31.4\u00b10.4\n[BOLD] LogReg | 26.9 | 34.5 | 28.7\u00b10.6 | 6.4 | 9.9 | 7.3\u00b10.4 | 12.2 | 14.9 | 12.7\u00b10.5\n[BOLD] LogReg [ITALIC] r | 28.0 | 34.8 | 29.4\u00b10.6 | 6.9 | 10.4 | 7.8\u00b10.4 | 12.1 | 14.5 | 12.5\u00b10.5\n[BOLD] HAN | 31.0 | 42.8 | 33.7\u00b10.7 | 11.2 | 17.8 | 12.7\u00b10.5 | 26.9 | 34.1 | 32.4\u00b10.5\n[BOLD] HAN+pretrainT | 32.2 | 42.4 | 34.4\u00b10.7 | 11.5 | 17.5 | 12.9\u00b10.5 | 29.6 | 35.8 | 32.2\u00b10.5\n[BOLD] HAN+pretrainU | 32.1 | 42.1 | 33.8\u00b10.7 | 11.6 | 17.6 | 12.9\u00b10.5 | 30.1 | 35.6 | 32.3\u00b10.5\n[BOLD] HAN [ITALIC] r | 38.1 | 40.5 | [BOLD] 37.8\u00b10.5 | 14.0 | 17.1 | [BOLD] 14.7\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainT [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.5 | 16.8 | [BOLD] 14.4\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainU [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.6 | 16.9 | [BOLD] 14.4\u00b10.5 | 33.9 | 33.8 | [BOLD] 33.8\u00b10.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "c2b01824-1085-4df5-8d14-5a2f4d99c3fd",
    "input": "## Claim\nHere is a claim: multi-turn models, who need to produce more than one sentence for each dialog turn, are disadvantaged in comparison to single-turn models which only need to generate a single sentence at a time, because there are always bad sentences in a dialog session which might always ruin the performance of multi-turn models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "cc1fb307-7edd-4eb4-bd7b-4eb8b22825ea",
    "input": "## Claim\nHere is a claim: Table 1) and crashes less frequently than all the baseline methods. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ff17d252-6aaa-4f84-b667-e7a4e533743c",
    "input": "## Claim\nHere is a claim: NeuralTD achieves comparable performances to state-of-the-art approaches while utilising a significantly simpler and lower-cost learning process with only a small quality drop, which we attribute to the reliance on an imperfect summary evaluation function Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "2c2a4d97-aacc-474f-a381-6fb70c1daa1f",
    "input": "## Claim\nHere is a claim: On the TREC task, CBOW outperforms CMOW by 2.3 points. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nMethod | SUBJ | CR | MR | MPQA | MRPC | TREC | SICK-E | SST2 | SST5 | STS-B | SICK-R\nCBOW/784 | 90.0 | [BOLD] 79.2 | [BOLD] 74.0 | 87.1 | 71.6 | 85.6 | 78.9 | 78.5 | 42.1 | 61.0 | [BOLD] 78.1\nCMOW/784 | 87.5 | 73.4 | 70.6 | [BOLD] 87.3 | 69.6 | [BOLD] 88.0 | 77.2 | 74.7 | 37.9 | 56.5 | 76.2\nHybrid | [BOLD] 90.2 | 78.7 | 73.7 | [BOLD] 87.3 | [BOLD] 72.7 | 87.6 | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8\ncmp. CBOW | +0.2% | -0.6% | -0.4% | +0.2% | +1.5% | +2.3% | +0.6% | +1.4% | +2.9% | +3.9% | -0.4%\ncmp. CMOW | +3.1% | +7.2% | +4.4% | +0% | +4.5% | -0.5% | +2.9% | +6.7% | +14.3 | +12.2% | +2.1%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3f49d5e4-4226-44ed-b64b-6fd8e62e1cc5",
    "input": "## Claim\nHere is a claim: G2S-GGNN has 33.5% and 5.2% worse entailment performances than S2S, when REF entails GEN and GEN entails REF, respectively. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.\n<bold>Model</bold> | REF \u21d2 GEN <bold>ENT</bold> | REF \u21d2 GEN <bold>CON</bold> | REF \u21d2 GEN <bold>NEU</bold>\nS2S | 38.45 | 11.17 | 50.38\nG2S-GIN | 49.78 | 9.80 | 40.42\nG2S-GAT | 49.48 | 8.09 | 42.43\nG2S-GGNN | 51.32 | 8.82 | 39.86\n[EMPTY] | GEN \u21d2 REF | GEN \u21d2 REF | GEN \u21d2 REF\n<bold>Model</bold> | <bold>ENT</bold> | <bold>CON</bold> | <bold>NEU</bold>\nS2S | 73.79 | 12.75 | 13.46\nG2S-GIN | 76.27 | 10.65 | 13.08\nG2S-GAT | 77.54 | 8.54 | 13.92\nG2S-GGNN | 77.64 | 9.64 | 12.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "6684d294-b666-4718-affe-953ad1c47f8b",
    "input": "## Claim\nHere is a claim: The evaluation results shown in Table 2 indicate that the annotated NLDs are of low quality (Reachability), and each NLD is not properly derived from supporting documents (Derivability). Does the following context support or refute the claim?\n\n## Table\nPaper title: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension\nTable caption: Table 2: Ratings of annotated NLDs by human judges.\n# steps | Reachability | Derivability Step 1 | Derivability Step 2 | Derivability Step 3\n1 | 3.0 | 3.8 | - | -\n2 | 2.8 | 3.8 | 3.7 | -\n3 | 2.3 | 3.9 | 3.8 | 3.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "3af33c66-1cff-404c-be5f-b3aa9c3b3cf4",
    "input": "## Claim\nHere is a claim: The results in Table 4 refute the findings of the automatic metrics: systems trained on the fully cleaned set or the set with cleaned missing slots do not have nearperfect performance, with the fully-cleaned one showing more errors than the other. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).\n[BOLD] Training data | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] Disfl\nOriginal | 0 | 22 | 0 | 14\nCleaned added | 0 | 23 | 0 | 14\nCleaned missing | 0 | 1 | 0 | 2\nCleaned | 0 | 0 | 0 | 5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "bdc1f2cb-09b4-4561-a6d4-1f69c0ee67d1",
    "input": "## Claim\nHere is a claim: These poor conversational performances are reflected in a more diverse KL-divergence scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.\nGP-MBCM | ACER | PPO | ALDM | GDPL\n1.666 | 0.775 | 0.639 | 1.069 | [BOLD] 0.238\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "232b4447-1be4-49ce-afe3-5bed802143ac",
    "input": "## Claim\nHere is a claim: In addition, the training time results in Table 3 confirm the computational advantage of LRN over all other recurrent units, where LRN speeds up over ATR and SRU by approximately 25%. Does the following context support or refute the claim?\n\n## Table\nPaper title: A Lightweight Recurrent Network for Sequence Modeling\nTable caption: Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.\nModel | #Params | BLEU | Train | Decode\nGNMT | - | 24.61 | - | -\nGRU | 206M | 26.28 | 2.67 | 45.35\nATR | 122M | 25.70 | 1.33 | [BOLD] 34.40\nSRU | 170M | 25.91 | 1.34 | 42.84\nLRN | 143M | 26.26 | [BOLD] 0.99 | 36.50\noLRN | 164M | [BOLD] 26.73 | 1.15 | 40.19\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "755e5f3b-ed26-43b5-8320-cc5853b2b815",
    "input": "## Claim\nHere is a claim: by averaging the column results, we can see that a pure effect of coverage appears on out-of-domain tasks, as applying it improves the performance of the standard models by an average of 7.02% in accuracy. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\n[EMPTY] | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK\nMQAN | 72.30 | 60.91 | 41.82 | 53.95\n+ coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold>\nESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37\n+ coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "3caabd87-48f9-4343-b91c-d8b677797b0e",
    "input": "## Claim\nHere is a claim: A complementary behavior can be observed for H-CBOW, whose scores on Word Content are decreased. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "714fe045-6b79-4ff5-81dc-e4b5434e1d66",
    "input": "## Claim\nHere is a claim: HDSA with a fixed threshold achieves higher values than DAMD with a sampled threshold because actions are easy to predict with a fixed threshold, even for a random policy, as there are only about 5-6 actions for each state Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c9a135e8-f977-46e0-a2cb-10737ce5245c",
    "input": "## Claim\nHere is a claim: We show the precision numbers for some particular recalls as well as the AUC in Table 2, where PCNN+ATT (1) refers to train sentences with two entities and one relation label, PCNN+ATT (m) refers to train sentences with four entities7 and two relation labels. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 2: Precisions on the Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nRank+ExATT | 0.584 | 0.535 | 0.487 | 0.392\nPCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204\nPCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396\nOur Model | 0.650 | 0.519 | 0.422 | [BOLD] 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "fe0e9c2a-c10b-4bbf-a94c-20cf7b2bc9b9",
    "input": "## Claim\nHere is a claim: we further evaluate the inform, match and success of the predictions under 50% threshold and show them in Fig. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.\nType | Inform Mean | Inform Num | Match Mean | Match Num | Success Mean | Success Num\nFull | 8.413 | 903 | 10.59 | 450 | 11.18 | 865\nOther | -99.95 | 76 | -48.15 | 99 | -71.62 | 135\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "84e16b2a-6ae0-428b-811e-962ffd7c0381",
    "input": "## Claim\nHere is a claim: these metrics generally are ineffective in capturing the semantic similarity of multiple documents: the pearsons correlation between them and human judgments is below .3, and their r is often negative or 0. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nMetric | [ITALIC] \u03c1 | [ITALIC] r | G-Pre | G-Rec\nROUGE-1 | .290 | .304 | .392 | .428\nROUGE-2 | .259 | .278 | .408 | .444\nROUGE-L | .274 | .297 | .390 | .426\nROUGE-SU4 | .282 | .279 | .404 | .440\nBLEU-1 | .256 | .281 | .409 | .448\nBLEU-2 | .301 | .312 | .411 | .446\nBLEU-3 | .317 | .312 | .409 | .444\nBLEU-4 | .311 | .307 | .409 | .446\nBLEU-5 | .308 | .303 | .420 | .459\nMETEOR | .305 | .285 | .409 | .444\nInferSent-Cosine | [BOLD] .329 | [BOLD] .339 | .417 | .460\nBERT-Cosine | .312 | .335 | [BOLD] .440 | [BOLD] .484\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "92d74cff-f9f2-4fb1-adfb-4a10df43de9a",
    "input": "## Claim\nHere is a claim: In contrast, our DCGCN models can be trained using a large number of layers. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.\n[BOLD] GCN +RC (2) | B 16.8 | C 48.1 | [BOLD] GCN +RC+LA (2) | B 18.3 | C 47.9\n+RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1\n+RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8\n+RC (9) | [BOLD] 21.1 | 50.5 | +RC+LA (9) | [BOLD] 22.0 | 52.6\n+RC (10) | 20.7 | [BOLD] 50.7 | +RC+LA (10) | 21.2 | [BOLD] 52.9\nDCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7\nDCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "29dedc61-018c-4f4a-985b-5998ec32dbe4",
    "input": "## Claim\nHere is a claim: The second row in Table 3 shows the test accuracy of a system trained without sense priors and the third row shows that removing attention from the model actually improved the accuracy, suggesting that context sensitivity is not necessary for good performance. Does the following context support or refute the claim?\n\n## Table\nPaper title: Ontology-Aware Token Embeddings for Prepositional Phrase Attachment\nTable caption: Table 3: Effect of removing sense priors and context sensitivity (attention) from the model.\n[BOLD] Model | [BOLD] PPA Acc.\nfull | 89.7\n- sense priors | 88.4\n- attention | 87.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "38158ec1-ed12-4527-92c5-00f4c3f9312c",
    "input": "## Claim\nHere is a claim: In all cases, the adversarial's success rate is around 50%, while the attacker's rate is substantially higher. Does the following context support or refute the claim?\n\n## Table\nPaper title: Adversarial Removal of Demographic Attributes from Text Data\nTable caption: Table 3: Performances on different datasets with an adversarial training. \u0394 is the difference between the attacker score and the corresponding adversary\u2019s accuracy.\nData | Task | Protected Attribute | Task Acc | Leakage | \u0394\nDial | Sentiment | Race | 64.7 | 56.0 | 5.0\n[EMPTY] | Mention | Race | 81.5 | 63.1 | 9.2\nPAN16 | Mention | Gender | 75.6 | 58.5 | 8.0\n[EMPTY] | Mention | Age | 72.5 | 57.3 | 6.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "ef361478-7869-46cf-b2dd-3c136f8ec77a",
    "input": "## Claim\nHere is a claim: We observe that POS tagging does benefit from features from the upper layers, while SEM tagging does not improve with layer 4 representations. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks\nTable caption: Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.\nUni | POS | 0 87.9 | 1 92.0 | 2 91.7 | 3 91.8 | 4 91.9\nUni | SEM | 81.8 | 87.8 | 87.4 | 87.6 | 88.2\nBi | POS | 87.9 | 93.3 | 92.9 | 93.2 | 92.8\nBi | SEM | 81.9 | 91.3 | 90.8 | 91.9 | 91.9\nRes | POS | 87.9 | 92.5 | 91.9 | 92.0 | 92.4\nRes | SEM | 81.9 | 88.2 | 87.5 | 87.6 | 88.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "ee25e11d-4b14-4114-8977-22a22aa4b799",
    "input": "## Claim\nHere is a claim: The superior score on attention relevance shows that TVMAX is better at selecting the relevant features and its output is more interpretable. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 2: Human evaluation results on MSCOCO.\n[EMPTY] | caption | attention relevance\nsoftmax | 3.50 | 3.38\nsparsemax | 3.71 | 3.89\nTVmax | [BOLD] 3.87 | [BOLD] 4.10\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "72ae36b8-bb32-4a0d-9f55-10b95d234b1c",
    "input": "## Claim\nHere is a claim: For example, DCGCN4 contains 36 layers and has the lowest performance on both datasets. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.\n[BOLD] GCN +RC (2) | B 16.8 | C 48.1 | [BOLD] GCN +RC+LA (2) | B 18.3 | C 47.9\n+RC (4) | 18.4 | 49.6 | +RC+LA (4) | 18.0 | 51.1\n+RC (6) | 19.9 | 49.7 | +RC+LA (6) | 21.3 | 50.8\n+RC (9) | [BOLD] 21.1 | 50.5 | +RC+LA (9) | [BOLD] 22.0 | 52.6\n+RC (10) | 20.7 | [BOLD] 50.7 | +RC+LA (10) | 21.2 | [BOLD] 52.9\nDCGCN1 (9) | 22.9 | 53.0 | DCGCN3 (27) | 24.8 | 54.7\nDCGCN2 (18) | 24.2 | 54.4 | DCGCN4 (36) | [BOLD] 25.5 | [BOLD] 55.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "68a6fee8-086e-407d-9604-300988692905",
    "input": "## Claim\nHere is a claim: The systems trained on the original data or with cleaned added slots perform better in terms of both semantic accuracy and fluency. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).\n[BOLD] Training data | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] Disfl\nOriginal | 0 | 22 | 0 | 14\nCleaned added | 0 | 23 | 0 | 14\nCleaned missing | 0 | 1 | 0 | 2\nCleaned | 0 | 0 | 0 | 5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "18b3279b-d503-4a14-90e2-b9f94c857026",
    "input": "## Claim\nHere is a claim: Compared with the fixed threshold, the sampled threshold surprisingly gives higher Bleu score but worse TER score, especially on the 10-action task. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "b0a81f4a-88ce-4478-bbbc-c06f440ff7db",
    "input": "## Claim\nHere is a claim: Additionally, when using bounding box features, softmax outperforms sparsemax, showing that selecting only the bounding boxes of the relevant objects does not lead to a better answering capability. Does the following context support or refute the claim?\n\n## Table\nPaper title: Sparse and Structured Visual Attention\nTable caption: Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.\n[EMPTY] | Att. to image | Att. to bounding boxes | Test-Dev Yes/No | Test-Dev Number | Test-Dev Other | Test-Dev Overall | Test-Standard Yes/No | Test-Standard Number | Test-Standard Other | Test-Standard Overall\nsoftmax | \u2713 | [EMPTY] | 83.08 | 42.65 | 55.74 | 65.52 | 83.55 | 42.68 | 56.01 | 65.97\nsparsemax | \u2713 | [EMPTY] | 83.08 | 43.19 | 55.79 | 65.60 | 83.33 | 42.99 | 56.06 | 65.94\nsoft-TVmax | \u2713 | [EMPTY] | 83.13 | 43.53 | 56.01 | 65.76 | 83.63 | 43.24 | 56.10 | 66.11\nsparse-TVmax | \u2713 | [EMPTY] | 83.10 | 43.30 | 56.14 | 65.79 | 83.66 | 43.18 | 56.21 | 66.17\nsoftmax | [EMPTY] | \u2713 | 85.14 | 49.59 | 58.72 | 68.57 | 85.56 | 49.54 | 59.11 | 69.04\nsparsemax | [EMPTY] | \u2713 | [BOLD] 85.40 | [BOLD] 50.87 | 58.67 | 68.79 | [BOLD] 85.80 | 50.18 | 59.08 | 69.19\nsoftmax | \u2713 | \u2713 | 85.33 | 50.49 | 58.88 | 68.82 | 85.58 | 50.42 | 59.18 | 69.17\nsparse-TVmax | \u2713 | \u2713 | 85.35 | 50.52 | [BOLD] 59.15 | [BOLD] 68.96 | 85.72 | [BOLD] 50.66 | [BOLD] 59.22 | [BOLD] 69.28\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "870227ae-f7f9-41e8-a840-f22a92847b02",
    "input": "## Claim\nHere is a claim: The proposed architecture achieves a 0.04% improvement over the baseline system with binary classification, and achieves 99.5% precision in true cues. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 4: Cue classification on the test set.\n[EMPTY] | [BOLD] F-Score  [BOLD] Baseline | [BOLD] F-Score  [BOLD] Proposed | [BOLD] Support\nFalse cues | 0.61 | 0.68 | 47\nActual cues | 0.97 | 0.98 | 557\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "4a2c8295-40a1-49a0-9ac3-4d2275c74349",
    "input": "## Claim\nHere is a claim: This indicates that our architecture can learn to generate better signals for text generation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.\n<bold>Model</bold> | <bold>BLEU</bold> | <bold>METEOR</bold>\nLDC2015E86 | LDC2015E86 | LDC2015E86\nKonstas et al. (2017) | 22.00 | -\nSong et al. (2018) | 23.28 | 30.10\nCao et al. (2019) | 23.50 | -\nDamonte et al.(2019) | 24.40 | 23.60\nGuo et al. (2019) | <bold>25.70</bold> | -\nS2S | 22.55 \u00b1 0.17 | 29.90 \u00b1 0.31\nG2S-GIN | 22.93 \u00b1 0.20 | 29.72 \u00b1 0.09\nG2S-GAT | 23.42 \u00b1 0.16 | 29.87 \u00b1 0.14\nG2S-GGNN | 24.32 \u00b1 0.16 | <bold>30.53</bold> \u00b1 0.30\nLDC2017T10 | LDC2017T10 | LDC2017T10\nBack et al. (2018) | 23.30 | -\nSong et al. (2018) | 24.86 | 31.56\nDamonte et al.(2019) | 24.54 | 24.07\nCao et al. (2019) | 26.80 | -\nGuo et al. (2019) | 27.60 | -\nS2S | 22.73 \u00b1 0.18 | 30.15 \u00b1 0.14\nG2S-GIN | 26.90 \u00b1 0.19 | 32.62 \u00b1 0.04\nG2S-GAT | 26.72 \u00b1 0.20 | 32.52 \u00b1 0.02\nG2S-GGNN | <bold>27.87</bold> \u00b1 0.15 | <bold>33.21</bold> \u00b1 0.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "8e82ff01-e164-4739-b890-f28e353b3a47",
    "input": "## Claim\nHere is a claim: In other words, it has the strongest tendency to predict dialog state transition accurately. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.\nVS. | Efficiency W | Efficiency D | Efficiency L | Quality W | Quality D | Quality L | Success W | Success D | Success L\nACER | 55 | 25 | 20 | 44 | 32 | 24 | 52 | 30 | 18\nPPO | 74 | 13 | 13 | 56 | 26 | 18 | 59 | 31 | 10\nALDM | 69 | 19 | 12 | 49 | 25 | 26 | 61 | 24 | 15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "8b58e919-ddcb-4f76-85ef-dfeb17d817a7",
    "input": "## Claim\nHere is a claim: This shows that using additional information about the word locations would help to gain a better generalization across the datasets. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "33575f0c-0627-4e2e-b5a6-668a9f447a3a",
    "input": "## Claim\nHere is a claim: In addition, our single model is comparable to the ensemble results of Seq2SeqB and GGNN2Seq, while the number of parameters of our models is only about 1/6 of theirs. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 4: Main results on English-German and English-Czech datasets.\n[BOLD] Model | [BOLD] Type | [BOLD] English-German #P | [BOLD] English-German B | [BOLD] English-German C | [BOLD] English-Czech #P | [BOLD] English-Czech B | [BOLD] English-Czech C\nBoW+GCN (Bastings et al.,  2017 ) | Single | - | 12.2 | - | - | 7.5 | -\nCNN+GCN (Bastings et al.,  2017 ) | Single | - | 13.7 | - | - | 8.7 | -\nBiRNN+GCN (Bastings et al.,  2017 ) | Single | - | 16.1 | - | - | 9.6 | -\nPB-SMT (Beck et al.,  2018 ) | Single | - | 12.8 | 43.2 | - | 8.6 | 36.4\nSeq2SeqB (Beck et al.,  2018 ) | Single | 41.4M | 15.5 | 40.8 | 39.1M | 8.9 | 33.8\nGGNN2Seq (Beck et al.,  2018 ) | Single | 41.2M | 16.7 | 42.4 | 38.8M | 9.8 | 33.3\nDCGCN (ours) | Single | [BOLD]  29.7M | [BOLD] 19.0 | [BOLD] 44.1 | [BOLD]  28.3M | [BOLD] 12.1 | [BOLD] 37.1\nSeq2SeqB (Beck et al.,  2018 ) | Ensemble | 207M | 19.0 | 44.1 | 195M | 11.3 | 36.4\nGGNN2Seq (Beck et al.,  2018 ) | Ensemble | 206M | 19.6 | 45.1 | 194M | 11.7 | 35.9\nDCGCN (ours) | Ensemble | [BOLD]  149M | [BOLD] 20.5 | [BOLD] 45.8 | [BOLD]  142M | [BOLD] 13.1 | [BOLD] 37.8\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "f9ee4a0c-c9ea-4202-9e4f-4810c4def2db",
    "input": "## Claim\nHere is a claim: This creates an artificial outlier alternative which has low applicability and productivity, but has high coverage which stems from this outlier alternative. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nCue | App. | Prod. | Cov.\nin | 47 | 55.3 | 9.40\nwas | 55 | 61.8 | 11.0\nto | 82 | 40.2 | 16.4\nthe | 85 | 38.8 | 17.0\na | 106 | 57.5 | 21.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "ba0b99c4-316c-4a92-b84e-5f072c99b79e",
    "input": "## Claim\nHere is a claim: They showthat both Type 1 and Type 2 schema knowledge havehigher precision on Category 1 and Category 2 datainstances, respectively, compared to that on full data. Does the following context support or refute the claim?\n\n## Table\nPaper title: Solving Hard Coreference Problems\nTable caption: Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.\nSchema | AntePre(Test) | AntePre(Train)\nType 1 | 76.67 | 86.79\nType 2 | 79.55 | 88.86\nType 1 (Cat1) | 90.26 | 93.64\nType 2 (Cat2) | 83.38 | 92.49\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "58155cc4-23ec-4f64-b7ec-568af0621eaa",
    "input": "## Claim\nHere is a claim: [CONTINUE] We observe that the redundancy removal step is crucial for the HAN models to achieve outstanding results. Does the following context support or refute the claim?\n\n## Table\nPaper title: Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks\nTable caption: Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.\n[BOLD] System | [BOLD] ROUGE-1  [BOLD] R (%) | [BOLD] ROUGE-1  [BOLD] P (%) | [BOLD] ROUGE-1  [BOLD] F (%) | [BOLD] ROUGE-2  [BOLD] R (%) | [BOLD] ROUGE-2  [BOLD] P (%) | [BOLD] ROUGE-2  [BOLD] F (%) | [BOLD] Sentence-Level  [BOLD] R (%) | [BOLD] Sentence-Level  [BOLD] P (%) | [BOLD] Sentence-Level  [BOLD] F (%)\n[BOLD] ILP | 24.5 | 41.1 | 29.3\u00b10.5 | 7.9 | 15.0 | 9.9\u00b10.5 | 13.6 | 22.6 | 15.6\u00b10.4\n[BOLD] Sum-Basic | 28.4 | 44.4 | 33.1\u00b10.5 | 8.5 | 15.6 | 10.4\u00b10.4 | 14.7 | 22.9 | 16.7\u00b10.5\n[BOLD] KL-Sum | 39.5 | 34.6 | 35.5\u00b10.5 | 13.0 | 12.7 | 12.3\u00b10.5 | 15.2 | 21.1 | 16.3\u00b10.5\n[BOLD] LexRank | 42.1 | 39.5 | 38.7\u00b10.5 | 14.7 | 15.3 | 14.2\u00b10.5 | 14.3 | 21.5 | 16.0\u00b10.5\n[BOLD] MEAD | 45.5 | 36.5 | 38.5\u00b1 0.5 | 17.9 | 14.9 | 15.4\u00b10.5 | 27.8 | 29.2 | 26.8\u00b10.5\n[BOLD] SVM | 19.0 | 48.8 | 24.7\u00b10.8 | 7.5 | 21.1 | 10.0\u00b10.5 | 32.7 | 34.3 | 31.4\u00b10.4\n[BOLD] LogReg | 26.9 | 34.5 | 28.7\u00b10.6 | 6.4 | 9.9 | 7.3\u00b10.4 | 12.2 | 14.9 | 12.7\u00b10.5\n[BOLD] LogReg [ITALIC] r | 28.0 | 34.8 | 29.4\u00b10.6 | 6.9 | 10.4 | 7.8\u00b10.4 | 12.1 | 14.5 | 12.5\u00b10.5\n[BOLD] HAN | 31.0 | 42.8 | 33.7\u00b10.7 | 11.2 | 17.8 | 12.7\u00b10.5 | 26.9 | 34.1 | 32.4\u00b10.5\n[BOLD] HAN+pretrainT | 32.2 | 42.4 | 34.4\u00b10.7 | 11.5 | 17.5 | 12.9\u00b10.5 | 29.6 | 35.8 | 32.2\u00b10.5\n[BOLD] HAN+pretrainU | 32.1 | 42.1 | 33.8\u00b10.7 | 11.6 | 17.6 | 12.9\u00b10.5 | 30.1 | 35.6 | 32.3\u00b10.5\n[BOLD] HAN [ITALIC] r | 38.1 | 40.5 | [BOLD] 37.8\u00b10.5 | 14.0 | 17.1 | [BOLD] 14.7\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainT [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.5 | 16.8 | [BOLD] 14.4\u00b10.5 | 32.5 | 34.4 | [BOLD] 33.4\u00b10.5\n[BOLD] HAN+pretrainU [ITALIC] r | 37.9 | 40.4 | [BOLD] 37.6\u00b10.5 | 13.6 | 16.9 | [BOLD] 14.4\u00b10.5 | 33.9 | 33.8 | [BOLD] 33.8\u00b10.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "02a92ece-1cfd-4fb7-827c-472608cc154c",
    "input": "## Claim\nHere is a claim: the KG itself has the most relevance to the results. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nIteration=1 | 0.531 | 0.455 | 0.353 | 0.201\nIteration=2 | 0.592 | 0.498 | 0.385 | 0.375\nIteration=3 | 0.650 | 0.519 | 0.422 | 0.405\nIteration=4 | 0.601 | 0.505 | 0.422 | 0.385\nIteration=5 | 0.575 | 0.495 | 0.394 | 0.376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "a08fc051-d0c1-4de4-b17f-791f586addcd",
    "input": "## Claim\nHere is a claim: It is possible that a specific KG has different patterns of its regularity, the result of which affects the learning ability of the complex KG embeddings Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nIteration=1 | 0.531 | 0.455 | 0.353 | 0.201\nIteration=2 | 0.592 | 0.498 | 0.385 | 0.375\nIteration=3 | 0.650 | 0.519 | 0.422 | 0.405\nIteration=4 | 0.601 | 0.505 | 0.422 | 0.385\nIteration=5 | 0.575 | 0.495 | 0.394 | 0.376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "95543831-0ee0-4bea-a141-061abbf717e9",
    "input": "## Claim\nHere is a claim: As evident from Table 1, there is a significant imbalance in the distribution of training instances that are suggestions and non-suggestions, 2https://www.uservoice.com/ [CONTINUE] For Sub Task A, the organizers shared a training and a validation dataset whose label distribution (suggestion or a non-suggestion) is presented in Table 1. Does the following context support or refute the claim?\n\n## Table\nPaper title: Suggestion Mining from Online Reviews using ULMFiT\nTable caption: Table 1: Dataset Distribution for Sub Task A - Task 9: Suggestion Mining from Online Reviews.\n[BOLD] Label | [BOLD] Train | [BOLD] Trial\n[BOLD] Suggestion | 2085 | 296\n[BOLD] Non Suggestion | 6415 | 296\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d84d4003-6890-4170-b3d7-c47ee7e05cf6",
    "input": "## Claim\nHere is a claim: For example, using relations generated by TF model using the Europarl corpus, we can understand the MaxDepth as having 788 terms with different values of term frequency, while having only 1 that share the same value of term frequency with other terms. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.\nCorpus | Metric | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nEuroparl | TotalTerms: | 957 | 1,000 | 1,000 | 1,000 | 1,000 | 836 | 1,000\nEuroparl | TotalRoots: | 44 | 1 | 1 | 1 | 1 | 43 | 1\nEuroparl | NumberRels: | 1,588 | 1,025 | 1,028 | 1,185 | 1,103 | 1,184 | 999\nEuroparl | MaxDepth: | 21 | 921 | 901 | 788 | 835 | 8 | 15\nEuroparl | MinDepth: | 1 | 921 | 901 | 788 | 835 | 1 | 1\nEuroparl | AvgDepth: | 11.82 | 921 | 901 | 788 | 835 | 3.05 | 8.46\nEuroparl | DepthCohesion: | 1.78 | 1 | 1 | 1 | 1 | 2.62 | 1.77\nEuroparl | MaxWidth: | 20 | 2 | 3 | 4 | 3 | 88 | 41\nEuroparl | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nEuroparl | AvgWidth: | 1.99 | 1.03 | 1.03 | 1.19 | 1.10 | 4.20 | 2.38\nTED Talks | TotalTerms: | 476 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000 | 1,000\nTED Talks | TotalRoots: | 164 | 2 | 1 | 1 | 1 | 1 | 1\nTED Talks | NumberRels: | 521 | 1,029 | 1,331 | 3,025 | 3,438 | 3,802 | 1,009\nTED Talks | MaxDepth: | 16 | 915 | 658 | 454 | 395 | 118 | 12\nTED Talks | MinDepth: | 1 | 913 | 658 | 454 | 395 | 110 | 1\nTED Talks | AvgDepth: | 5.82 | 914 | 658 | 454 | 395 | 112.24 | 5.95\nTED Talks | DepthCohesion: | 2.75 | 1 | 1 | 1 | 1 | 1.05 | 2.02\nTED Talks | MaxWidth: | 25 | 2 | 77 | 13 | 12 | 66 | 98\nTED Talks | MinWidth: | 1 | 1 | 1 | 1 | 1 | 1 | 1\nTED Talks | AvgWidth: | 1.83 | 1.03 | 1.36 | 3.03 | 3.44 | 6.64 | 2.35\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "fb51ce8e-cdc5-4c9d-8e17-c2043abea92e",
    "input": "## Claim\nHere is a claim: Our model improves the precision scores on both datasets with good recall scores. Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. \u2020 denotes the previous best state-of-the-art model.\nModel | NYT10 Prec. | NYT10 Rec. | NYT10 F1 | NYT11 Prec. | NYT11 Rec. | NYT11 F1\nCNN zeng2014relation | 0.413 | 0.591 | 0.486 | 0.444 | 0.625 | 0.519\nPCNN zeng2015distant | 0.380 | [BOLD] 0.642 | 0.477 | 0.446 | 0.679 | 0.538\u2020\nEA huang2016attention | 0.443 | 0.638 | 0.523\u2020 | 0.419 | 0.677 | 0.517\nBGWA jat2018attention | 0.364 | 0.632 | 0.462 | 0.417 | [BOLD] 0.692 | 0.521\nBiLSTM-CNN | 0.490 | 0.507 | 0.498 | 0.473 | 0.606 | 0.531\nOur model | [BOLD] 0.541 | 0.595 | [BOLD] 0.566* | [BOLD] 0.507 | 0.652 | [BOLD] 0.571*\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7a54fd92-fef9-4f12-8e39-7da48cdc52eb",
    "input": "## Claim\nHere is a claim: As we can observe in Table 3, Patt has the best values of precision for the English corpora while SLQS has the best values for the Portuguese corpora. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1173 | 0.0366 | 0.0503 | 0.0554 | 0.0548 | 0.0443 | 0.0761\nP | EN | Ted Talks | [BOLD] 0.1125 | 0.0301 | 0.0382 | 0.0425 | 0.0441 | 0.0710 | 0.0664\nP | PT | Europarl | 0.5163 | 0.3330 | 0.5257 | 0.6109 | 0.5984 | [BOLD] 0.7311 | 0.5676\n[EMPTY] | PT | Ted Talks | 0.5387 | 0.2907 | 0.5300 | 0.6117 | 0.6159 | [BOLD] 0.6533 | 0.5656\nR | EN | Europarl | 0.0396 | 0.3999 | 0.5499 | [BOLD] 0.6045 | 0.5887 | 0.0023 | 0.0017\nR | EN | Ted Talks | 0.0018 | 0.4442 | 0.5377 | 0.5657 | [BOLD] 0.6077 | 0.2666 | 0.0019\nR | PT | Europarl | 0.0111 | 0.3554 | 0.5795 | [BOLD] 0.6727 | 0.5184 | 0.0053 | 0.0012\n[EMPTY] | PT | Ted Talks | 0.0004 | 0.3142 | 0.5484 | [BOLD] 0.6877 | 0.5515 | 0.4706 | 0.0011\nF | EN | Europarl | 0.0591 | 0.0671 | 0.0922 | [BOLD] 0.1015 | 0.1003 | 0.0044 | 0.0033\nF | EN | Ted Talks | 0.0035 | 0.0564 | 0.0713 | 0.0791 | 0.0822 | [BOLD] 0.1121 | 0.0037\nF | PT | Europarl | 0.0217 | 0.3438 | 0.5513 | [BOLD] 0.6403 | 0.5555 | 0.0105 | 0.0024\n[EMPTY] | PT | Ted Talks | 0.0008 | 0.3020 | 0.5390 | [BOLD] 0.6475 | 0.5819 | 0.5471 | 0.0022\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a1761e4d-eabf-472c-a9bf-311759005b27",
    "input": "## Claim\nHere is a claim: RELIS does not significantly outperform the other RL-based systems. Does the following context support or refute the claim?\n\n## Table\nPaper title: Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation\nTable caption: Table 3: Results of non-RL (top), cross-input (DeepTD) and input-specific (REAPER) RL approaches (middle) compared with RELIS.\n[EMPTY] | DUC\u201901 <italic>R</italic>1 | DUC\u201901 <italic>R</italic>2 | DUC\u201902 <italic>R</italic>1 | DUC\u201902 <italic>R</italic>2 | DUC\u201904 <italic>R</italic>1 | DUC\u201904 <italic>R</italic>2\nICSI | 33.31 | 7.33 | 35.04 | 8.51 | 37.31 | 9.36\nPriorSum | 35.98 | 7.89 | 36.63 | 8.97 | 38.91 | 10.07\nTCSum | <bold>36.45</bold> | 7.66 | 36.90 | 8.61 | 38.27 | 9.66\nTCSum\u2212 | 33.45 | 6.07 | 34.02 | 7.39 | 35.66 | 8.66\nSRSum | 36.04 | 8.44 | <bold>38.93</bold> | <bold>10.29</bold> | 39.29 | 10.70\nDeepTD | 28.74 | 5.95 | 31.63 | 7.09 | 33.57 | 7.96\nREAPER | 32.43 | 6.84 | 35.03 | 8.11 | 37.22 | 8.64\nRELIS | 34.73 | <bold>8.66</bold> | 37.11 | 9.12 | <bold>39.34</bold> | <bold>10.73</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "7cb369b8-55fa-4147-b838-68f47b127ec9",
    "input": "## Claim\nHere is a claim: This seems to contradict previous research reporting that RoBERT does not improve on existing models for multiple-choice QA (Schick et al., 2020). Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).\nModel | Method | Training Data | Overall | Easy | Hard | p-value (%)\ngoodwin-etal-2012-utdhlt | PMI | unsupervised | 61.8 | 64.7 | 60.0 | 19.8\ngordon_commonsense_2011-1 | PMI | unsupervised | 65.4 | 65.8 | 65.2 | 83.5\nsasaki-etal-2017-handling | PMI | unsupervised | 71.4 | 75.3 | 69.0 | 4.8\u2217\nWord frequency | wordfreq | COPA | 53.5 | 57.4 | 51.3 | 9.8\nBERT-large-FT | LM, NSP | COPA | 76.5 (\u00b1 2.7) | 83.9 (\u00b1 4.4) | 71.9 (\u00b1 2.5) | 0.0\u2217\nRoBERTa-large-FT | LM | COPA | 87.7 (\u00b1 0.9) | 91.6 (\u00b1 1.1) | 85.3 (\u00b1 2.0) | 0.0\u2217\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "fc367a7a-61f9-4d01-80cb-71f58d303c05",
    "input": "## Claim\nHere is a claim: On the muli-domain dataset, MultiWoZ, our model achieves a joint goal accuracy of 48.79%, which marginally outperforms the previous state-of-the-art. Does the following context support or refute the claim?\n\n## Table\nPaper title: Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation\nTable caption: Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set. We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model Mrksic et al. (2017), while the baseline for the MultiWoZ dataset is taken from the official website of MultiWoZ Budzianowski et al. (2018).\n[BOLD] DST Models | [BOLD] Joint Acc. WoZ 2.0 | [BOLD] Joint Acc. MultiWoZ | [BOLD] ITC\nBaselines Mrksic et al. ( 2017 ) | 70.8% | 25.83% | [ITALIC] O( [ITALIC] mn)\nNBT-CNN Mrksic et al. ( 2017 ) | 84.2% | - | [ITALIC] O( [ITALIC] mn)\nStateNet_PSI Ren et al. ( 2018 ) | [BOLD] 88.9% | - | [ITALIC] O( [ITALIC] n)\nGLAD Nouri and Hosseini-Asl ( 2018 ) | 88.5% | 35.58% | [ITALIC] O( [ITALIC] mn)\nHyST (ensemble) Goel et al. ( 2019 ) | - | 44.22% | [ITALIC] O( [ITALIC] n)\nDSTRead (ensemble) Gao et al. ( 2019 ) | - | 42.12% | [ITALIC] O( [ITALIC] n)\nTRADE Wu et al. ( 2019 ) | - | 48.62% | [ITALIC] O( [ITALIC] n)\nCOMER | 88.6% | [BOLD] 48.79% | [ITALIC] O(1)\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "495a65cc-6f0c-4b6a-b19c-a3bd11784062",
    "input": "## Claim\nHere is a claim: [CONTINUE] The lowest values of precision are achieved by DSim model, and the lowest recalls are obtained by HClust and Patt models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages\nTable caption: Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.\n[EMPTY] | Lang | Corpus | Patt | DSim | SLQS | TF | DF | DocSub | HClust\nP | EN | Europarl | [BOLD] 0.1192 | 0.0083 | 0.0137 | 0.0150 | 0.0150 | 0.0445 | 0.0326\nP | EN | Ted Talks | [BOLD] 0.1022 | 0.0069 | 0.0060 | 0.0092 | 0.0090 | 0.0356 | 0.0162\nP | PT | Europarl | 0.5710 | 0.1948 | 0.3855 | 0.5474 | 0.4485 | [BOLD] 0.8052 | 0.4058\n[EMPTY] | PT | Ted Talks | [BOLD] 0.6304 | 0.1870 | 0.3250 | 0.5312 | 0.4576 | 0.6064 | 0.3698\nR | EN | Europarl | 0.0037 | 0.3278 | 0.5941 | 0.6486 | [BOLD] 0.6490 | 0.0017 | 0.0003\nR | EN | Ted Talks | 0.0002 | 0.1486 | 0.4332 | [BOLD] 0.6467 | 0.6332 | 0.0967 | 0.0003\nR | PT | Europarl | 0.0002 | 0.1562 | 0.5157 | [BOLD] 0.7255 | 0.5932 | 0.0032 | 0.0001\n[EMPTY] | PT | Ted Talks | 2.10-5 | 0.0507 | 0.4492 | [BOLD] 0.7000 | 0.5887 | 0.1390 | 0.0002\nF | EN | Europarl | 0.0073 | 0.0162 | 0.0268 | [BOLD] 0.0293 | [BOLD] 0.0293 | 0.0033 | 0.0006\nF | EN | Ted Talks | 0.0004 | 0.0132 | 0.0118 | 0.0181 | 0.0179 | [BOLD] 0.0520 | 0.0005\nF | PT | Europarl | 0.0005 | 0.1733 | 0.4412 | [BOLD] 0.6240 | 0.5109 | 0.0064 | 0.0002\n[EMPTY] | PT | Ted Talks | 4.10-5 | 0.0798 | 0.3771 | [BOLD] 0.6040 | 0.5149 | 0.2261 | 0.0004\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "9f1a619e-4ad3-401c-8c76-b44d8a33ef89",
    "input": "## Claim\nHere is a claim: Dual2seq-LinAMR shows much worse performance than our model and only slightly outperforms the Seq2seq baseline. Does the following context support or refute the claim?\n\n## Table\nPaper title: Semantic Neural Machine Translation using AMR\nTable caption: Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.\nSystem | NC-v11 BLEU | NC-v11 TER\u2193 | NC-v11 Meteor | Full BLEU | Full TER\u2193 | Full Meteor\nOpenNMT-tf | 15.1 | 0.6902 | 0.3040 | 24.3 | 0.5567 | 0.4225\nTransformer-tf | 17.1 | 0.6647 | 0.3578 | 25.1 | 0.5537 | 0.4344\nSeq2seq | 16.0 | 0.6695 | 0.3379 | 23.7 | 0.5590 | 0.4258\nDual2seq-LinAMR | 17.3 | 0.6530 | 0.3612 | 24.0 | 0.5643 | 0.4246\nDuel2seq-SRL | 17.2 | 0.6591 | 0.3644 | 23.8 | 0.5626 | 0.4223\nDual2seq-Dep | 17.8 | 0.6516 | 0.3673 | 25.0 | 0.5538 | 0.4328\nDual2seq | [BOLD] *19.2* | [BOLD] 0.6305 | [BOLD] 0.3840 | [BOLD] *25.5* | [BOLD] 0.5480 | [BOLD] 0.4376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "8f2d3f7f-1558-4d27-a479-4d02c594336f",
    "input": "## Claim\nHere is a claim: On 7 out of 11 supervised tasks, the joint model does not improve upon the better model, and on SST2, SST5, and MRPC the difference is less than 1 point. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nMethod | SUBJ | CR | MR | MPQA | MRPC | TREC | SICK-E | SST2 | SST5 | STS-B | SICK-R\nCBOW/784 | 90.0 | [BOLD] 79.2 | [BOLD] 74.0 | 87.1 | 71.6 | 85.6 | 78.9 | 78.5 | 42.1 | 61.0 | [BOLD] 78.1\nCMOW/784 | 87.5 | 73.4 | 70.6 | [BOLD] 87.3 | 69.6 | [BOLD] 88.0 | 77.2 | 74.7 | 37.9 | 56.5 | 76.2\nHybrid | [BOLD] 90.2 | 78.7 | 73.7 | [BOLD] 87.3 | [BOLD] 72.7 | 87.6 | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8\ncmp. CBOW | +0.2% | -0.6% | -0.4% | +0.2% | +1.5% | +2.3% | +0.6% | +1.4% | +2.9% | +3.9% | -0.4%\ncmp. CMOW | +3.1% | +7.2% | +4.4% | +0% | +4.5% | -0.5% | +2.9% | +6.7% | +14.3 | +12.2% | +2.1%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "2aecaa7e-e91a-47ba-9108-97dd18064ff6",
    "input": "## Claim\nHere is a claim: [CONTINUE] The results show that coverage information considerably improves the generalization of both examined models across various NLI datasets. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.\n[EMPTY] | in-domain MultiNLI | out-of-domain SNLI | out-of-domain Glockner | out-of-domain SICK\nMQAN | 72.30 | 60.91 | 41.82 | 53.95\n+ coverage | <bold>73.84</bold> | <bold>65.38</bold> | <bold>78.69</bold> | <bold>54.55</bold>\nESIM (ELMO) | 80.04 | 68.70 | 60.21 | 51.37\n+ coverage | <bold>80.38</bold> | <bold>70.05</bold> | <bold>67.47</bold> | <bold>52.65</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "faabddbe-aa9d-42f7-abf4-f242e5a01e76",
    "input": "## Claim\nHere is a claim: Our single DCGCN model does not obtain better results than previous ensemble models. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.\n[BOLD] Model | [BOLD] T | #P | B | C\nSeq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1\nGGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4\nSeq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5\nGGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5\nDCGCN (ours) | S | [BOLD] 19.1M | 27.9 | 57.3\nDCGCN (ours) | E | 92.5M | [BOLD] 30.4 | [BOLD] 59.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "980f3548-a1c0-4e26-90d1-017f81573985",
    "input": "## Claim\nHere is a claim: In all cases, the adversarial's success rate is higher than the attacker's rate, with a difference of at least 5%. Does the following context support or refute the claim?\n\n## Table\nPaper title: Adversarial Removal of Demographic Attributes from Text Data\nTable caption: Table 3: Performances on different datasets with an adversarial training. \u0394 is the difference between the attacker score and the corresponding adversary\u2019s accuracy.\nData | Task | Protected Attribute | Task Acc | Leakage | \u0394\nDial | Sentiment | Race | 64.7 | 56.0 | 5.0\n[EMPTY] | Mention | Race | 81.5 | 63.1 | 9.2\nPAN16 | Mention | Gender | 75.6 | 58.5 | 8.0\n[EMPTY] | Mention | Age | 72.5 | 57.3 | 6.9\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "968f23a7-c045-48ba-8bff-71a31abdd3d8",
    "input": "## Claim\nHere is a claim: This suggests that our models are capable of capturing better semantic information from the graph generating outputs semantically related to the reference sentences. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.\n<bold>Model</bold> | REF \u21d2 GEN <bold>ENT</bold> | REF \u21d2 GEN <bold>CON</bold> | REF \u21d2 GEN <bold>NEU</bold>\nS2S | 38.45 | 11.17 | 50.38\nG2S-GIN | 49.78 | 9.80 | 40.42\nG2S-GAT | 49.48 | 8.09 | 42.43\nG2S-GGNN | 51.32 | 8.82 | 39.86\n[EMPTY] | GEN \u21d2 REF | GEN \u21d2 REF | GEN \u21d2 REF\n<bold>Model</bold> | <bold>ENT</bold> | <bold>CON</bold> | <bold>NEU</bold>\nS2S | 73.79 | 12.75 | 13.46\nG2S-GIN | 76.27 | 10.65 | 13.08\nG2S-GAT | 77.54 | 8.54 | 13.92\nG2S-GGNN | 77.64 | 9.64 | 12.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "4a286ba2-cb10-4692-8016-513776cdc6b3",
    "input": "## Claim\nHere is a claim: [CONTINUE] It also improves the generalization ability of question answering. Does the following context support or refute the claim?\n\n## Table\nPaper title: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension\nTable caption: Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).\nModel | Answerability Macro P/R/F | # Answerable | Answer Prec. | Derivation Prec. RG-L (P/R/F) | Derivation Prec. BL-4\nShortest Path | 54.8/55.5/53.2 | 976 | 3.6 | 56.7/38.5/41.5 | 31.3\nPRKGC | 52.6/51.5/50.7 | 1,021 | 45.2 | 40.7/60.7/44.7 | 30.9\nPRKGC+NS | 53.6/54.1/52.1 | 980 | 45.4 | 42.2/61.6/46.1 | 33.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "5e5549b7-46c9-4d2a-be76-7bf8049380a4",
    "input": "## Claim\nHere is a claim: [CONTINUE] Yet, the PRKGC model do not give considerably good results, which indicates the non-triviality of RC-QEDE. Does the following context support or refute the claim?\n\n## Table\nPaper title: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension\nTable caption: Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).\nModel | Answerability Macro P/R/F | # Answerable | Answer Prec. | Derivation Prec. RG-L (P/R/F) | Derivation Prec. BL-4\nShortest Path | 54.8/55.5/53.2 | 976 | 3.6 | 56.7/38.5/41.5 | 31.3\nPRKGC | 52.6/51.5/50.7 | 1,021 | 45.2 | 40.7/60.7/44.7 | 30.9\nPRKGC+NS | 53.6/54.1/52.1 | 980 | 45.4 | 42.2/61.6/46.1 | 33.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d87f2213-5235-4ce8-9871-44178ae506af",
    "input": "## Claim\nHere is a claim: For the Japanese captions, AME does not reach better results on average compared to monolingual model in symmetric and asymmetric modes, respectively. Does the following context support or refute the claim?\n\n## Table\nPaper title: Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task\nTable caption: Table 4: Image-caption ranking results for Japanese (MS-COCO)\n[EMPTY] | Image to Text R@1 | Image to Text R@5 | Image to Text R@10 | Image to Text Mr | Text to Image R@1 | Text to Image R@5 | Text to Image R@10 | Text to Image Mr | Alignment\n[BOLD] symmetric | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nMono | 42.7 | 77.7 | 88.5 | 2 | 33.1 | 69.8 | 84.3 | 3 | -\nFME | 40.7 | 77.7 | 88.3 | 2 | 30.0 | 68.9 | 83.1 | 3 | 92.70%\nAME | [BOLD] 50.2 | [BOLD] 85.6 | [BOLD] 93.1 | [BOLD] 1 | [BOLD] 40.2 | [BOLD] 76.7 | [BOLD] 87.8 | [BOLD] 2 | 82.54%\n[BOLD] asymmetric | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY]\nMono | 49.9 | 83.4 | 93.7 | 2 | 39.7 | 76.5 | 88.3 | [BOLD] 2 | -\nFME | 48.8 | 81.9 | 91.9 | 2 | 37.0 | 74.8 | 87.0 | [BOLD] 2 | 92.70%\nAME | [BOLD] 55.5 | [BOLD] 87.9 | [BOLD] 95.2 | [BOLD] 1 | [BOLD] 44.9 | [BOLD] 80.7 | [BOLD] 89.3 | [BOLD] 2 | 84.99%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "554754d1-e781-421a-b781-05fba5c213dd",
    "input": "## Claim\nHere is a claim: When using the same amount of 0.2M data, the performance of DCGCN is not necessarily higher than Seq2SeqK and GraphLSTM. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M\n[BOLD] Model | [BOLD] External | B\nSeq2SeqK (Konstas et al.,  2017 ) | - | 22.0\nGraphLSTM (Song et al.,  2018 ) | - | 23.3\nGCNSEQ (Damonte and Cohen,  2019 ) | - | 24.4\nDCGCN(single) | - | 25.9\nDCGCN(ensemble) | - | [BOLD] 28.2\nTSP (Song et al.,  2016 ) | ALL | 22.4\nPBMT (Pourdamghani et al.,  2016 ) | ALL | 26.9\nTree2Str (Flanigan et al.,  2016 ) | ALL | 23.0\nSNRG (Song et al.,  2017 ) | ALL | 25.6\nSeq2SeqK (Konstas et al.,  2017 ) | 0.2M | 27.4\nGraphLSTM (Song et al.,  2018 ) | 0.2M | 28.2\nDCGCN(single) | 0.1M | 29.0\nDCGCN(single) | 0.2M | [BOLD] 31.6\nSeq2SeqK (Konstas et al.,  2017 ) | 2M | 32.3\nGraphLSTM (Song et al.,  2018 ) | 2M | 33.6\nSeq2SeqK (Konstas et al.,  2017 ) | 20M | 33.8\nDCGCN(single) | 0.3M | 33.2\nDCGCN(ensemble) | 0.3M | [BOLD] 35.3\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "4032dd15-2c3c-49fa-b8cb-89a8f16ef60f",
    "input": "## Claim\nHere is a claim: we observe that the performance of both models decreases as the task becomes more dissimilar to the training data. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improving Generalization by Incorporating Coverage in Natural Language Inference\nTable caption: Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.\n[EMPTY] | in-domain SQuAD | in-domain SQuAD | out-of-domain QA-SRL | out-of-domain QA-SRL\n[EMPTY] | EM | F1 | EM | F1\nMQAN | 31.76 | 75.37 | <bold>10.99</bold> | 50.10\n+coverage | <bold>32.67</bold> | <bold>76.83</bold> | 10.63 | <bold>50.89</bold>\nBIDAF (ELMO) | 70.43 | 79.76 | 28.35 | 49.98\n+coverage | <bold>71.07</bold> | <bold>80.15</bold> | <bold>30.58</bold> | <bold>52.43</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "30ef8633-a942-41a8-8686-40d62e8d3848",
    "input": "## Claim\nHere is a claim: Adding the dependency weight factor with a window size of 10 decreases the F1 score by 0.7% (A4\u2212A2). Does the following context support or refute the claim?\n\n## Table\nPaper title: Effective Attention Modeling for Neural Relation Extraction\nTable caption: Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.\n[EMPTY] | Prec. | Rec. | F1\n(A1) BiLSTM-CNN | 0.473 | 0.606 | 0.531\n(A2) Standard attention | 0.466 | 0.638 | 0.539\n(A3) Window size ( [ITALIC] ws)=5 | 0.507 | 0.652 | [BOLD] 0.571\n(A4) Window size ( [ITALIC] ws)=10 | 0.510 | 0.640 | 0.568\n(A5) Softmax | 0.490 | 0.658 | 0.562\n(A6) Max-pool | 0.492 | 0.600 | 0.541\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "a16a3f84-4381-4d4e-bdc0-859ddfd9beb3",
    "input": "## Claim\nHere is a claim: Our ICA framework outperforms the other baselines for all tasks. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "71b4b4de-2559-490c-83ab-04da9dda769e",
    "input": "## Claim\nHere is a claim: [CONTINUE] Regarding the probing tasks, we observe that CBOW embeddings better encode the linguistic properties of sentences than CMOW. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "931fadd7-9e3a-468e-a366-5b69a84720f6",
    "input": "## Claim\nHere is a claim: [CONTINUE] In Librispeech + DEMAND, acoustic supervision (15.6%) and multi-task learning (14.4%) achieves a lower WER than minimizing DCE (15.8%) and FSEGAN (14.9%). Does the following context support or refute the claim?\n\n## Table\nPaper title: Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition\nTable caption: TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set\nMethod | WER (%) | DCE\nNo enhancement | 17.3 | 0.828\nWiener filter | 19.5 | 0.722\nMinimizing DCE | 15.8 | [BOLD] 0.269\nFSEGAN | 14.9 | 0.291\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=0) | 15.6 | 0.330\nAAS ( [ITALIC] wAC=1, [ITALIC] wAD=105) | [BOLD] 14.4 | 0.303\nClean speech | 5.7 | 0.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "cee6661e-b5b1-4fc8-b1cd-c9c014565b09",
    "input": "## Claim\nHere is a claim: At the same time, RELIS performs worse than neural-based TCSum and SRSum, while it requires significantly less data and time to train, as shown next. Does the following context support or refute the claim?\n\n## Table\nPaper title: Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation\nTable caption: Table 3: Results of non-RL (top), cross-input (DeepTD) and input-specific (REAPER) RL approaches (middle) compared with RELIS.\n[EMPTY] | DUC\u201901 <italic>R</italic>1 | DUC\u201901 <italic>R</italic>2 | DUC\u201902 <italic>R</italic>1 | DUC\u201902 <italic>R</italic>2 | DUC\u201904 <italic>R</italic>1 | DUC\u201904 <italic>R</italic>2\nICSI | 33.31 | 7.33 | 35.04 | 8.51 | 37.31 | 9.36\nPriorSum | 35.98 | 7.89 | 36.63 | 8.97 | 38.91 | 10.07\nTCSum | <bold>36.45</bold> | 7.66 | 36.90 | 8.61 | 38.27 | 9.66\nTCSum\u2212 | 33.45 | 6.07 | 34.02 | 7.39 | 35.66 | 8.66\nSRSum | 36.04 | 8.44 | <bold>38.93</bold> | <bold>10.29</bold> | 39.29 | 10.70\nDeepTD | 28.74 | 5.95 | 31.63 | 7.09 | 33.57 | 7.96\nREAPER | 32.43 | 6.84 | 35.03 | 8.11 | 37.22 | 8.64\nRELIS | 34.73 | <bold>8.66</bold> | 37.11 | 9.12 | <bold>39.34</bold> | <bold>10.73</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "0d7db1a3-15a7-448f-a5ff-bee89e7168f1",
    "input": "## Claim\nHere is a claim: When the experiment was repeated so that the finetuning phase included the text-only data, the performance returned to approximately the same level as without tuning (+multi-modal finetune row in Table 6). Does the following context support or refute the claim?\n\n## Table\nPaper title: The MeMAD Submission to the WMT18 Multimodal Translation Task\nTable caption: Table 6: Ablation experiments (BLEU% scores). The row subs3MLM detectron shows our best single model. Individual components or data choices are varied one by one. + stands for adding a component, and \u2212 for removing a component or data set. Multiple modifications are indicated by increasing the indentation.\nen-fr | flickr16 | flickr17 | mscoco17\nsubs3M [ITALIC]  [ITALIC] LM detectron | 68.30 | 62.45 | 52.86\n+ensemble-of-3 | 68.72 | 62.70 | 53.06\n\u2212visual features | [BOLD] 68.74 | [BOLD] 62.71 | 53.14\n\u2212MS-COCO | 67.13 | 61.17 | [BOLD] 53.34\n\u2212multi-lingual | 68.21 | 61.99 | 52.40\nsubs6M [ITALIC]  [ITALIC] LM detectron | 68.29 | 61.73 | 53.05\nsubs3M [ITALIC]  [ITALIC] LM gn2048 | 67.74 | 61.78 | 52.76\nsubs3M [ITALIC]  [ITALIC] LM text-only | 67.72 | 61.75 | 53.02\nen-de | flickr16 | flickr17 | mscoco17\nsubs3M [ITALIC]  [ITALIC] LM detectron | 45.09 | 40.81 | 36.94\n+ensemble-of-3 | 45.52 | [BOLD] 41.84 | [BOLD] 37.49\n\u2212visual features | [BOLD] 45.59 | 41.75 | 37.43\n\u2212MS-COCO | 45.11 | 40.52 | 36.47\n\u2212multi-lingual | 44.95 | 40.09 | 35.28\nsubs6M [ITALIC]  [ITALIC] LM detectron | 45.50 | 41.01 | 36.81\nsubs3M [ITALIC]  [ITALIC] LM gn2048 | 45.38 | 40.07 | 36.82\nsubs3M [ITALIC]  [ITALIC] LM text-only | 44.87 | 41.27 | 36.59\n+multi-modal finetune | 44.56 | 41.61 | 36.93\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "34c20bf4-d238-447f-bd59-ad3fde68d867",
    "input": "## Claim\nHere is a claim: Under the same setting, our model also consistently outperforms graph encoders based on recurrent neural networks or gating mechanisms. Does the following context support or refute the claim?\n\n## Table\nPaper title: Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning\nTable caption: Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.\n[BOLD] Model | [BOLD] T | #P | B | C\nSeq2SeqB (Beck et al.,  2018 ) | S | 28,4M | 21.7 | 49.1\nGGNN2Seq (Beck et al.,  2018 ) | S | 28.3M | 23.3 | 50.4\nSeq2SeqB (Beck et al.,  2018 ) | E | 142M | 26.6 | 52.5\nGGNN2Seq (Beck et al.,  2018 ) | E | 141M | 27.5 | 53.5\nDCGCN (ours) | S | [BOLD] 19.1M | 27.9 | 57.3\nDCGCN (ours) | E | 92.5M | [BOLD] 30.4 | [BOLD] 59.6\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "33fb4b62-d5db-43e1-b19f-9006c5c5c618",
    "input": "## Claim\nHere is a claim: [CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Cleaned | TGen\u2212 | 36.85 | 5.3782 | 35.14 | 55.01 | 1.6016 | 00.34 | 09.81 | 00.15 | 10.31\nOriginal | [BOLD] Cleaned | TGen | 39.23 | 6.0217 | 36.97 | 55.52 | 1.7623 | 00.40 | 03.59 | 00.07 | 04.05\nOriginal | [BOLD] Cleaned | TGen+ | 40.25 | 6.1448 | 37.50 | 56.19 | 1.8181 | 00.21 | 01.99 | 00.05 | 02.24\nOriginal | [BOLD] Cleaned | SC-LSTM | 23.88 | 3.9310 | 32.11 | 39.90 | 0.5036 | 07.73 | 17.76 | 09.52 | 35.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen\u2212 | 40.19 | 6.0543 | 37.38 | 55.88 | 1.8104 | 00.17 | 01.31 | 00.25 | 01.72\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen | 40.73 | 6.1711 | 37.76 | 56.09 | 1.8518 | 00.07 | 00.72 | 00.08 | 00.87\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen+ | 40.51 | 6.1226 | 37.61 | 55.98 | 1.8286 | 00.02 | 00.63 | 00.06 | 00.70\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | SC-LSTM | 23.66 | 3.9511 | 32.93 | 39.29 | 0.3855 | 07.89 | 15.60 | 08.44 | 31.94\nCleaned missing | [BOLD] Cleaned | TGen\u2212 | 40.48 | 6.0269 | 37.26 | 56.19 | 1.7999 | 00.43 | 02.84 | 00.26 | 03.52\nCleaned missing | [BOLD] Cleaned | TGen | 41.57 | 6.2830 | 37.99 | 56.36 | 1.8849 | 00.37 | 01.40 | 00.09 | 01.86\nCleaned missing | [BOLD] Cleaned | TGen+ | 41.56 | 6.2700 | 37.94 | 56.38 | 1.8827 | 00.21 | 01.04 | 00.07 | 01.31\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen\u2212 | 35.99 | 5.0734 | 34.74 | 54.79 | 1.5259 | 00.02 | 11.58 | 00.02 | 11.62\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen | 40.07 | 6.1243 | 37.45 | 55.81 | 1.8026 | 00.05 | 03.23 | 00.01 | 03.29\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+ | 40.80 | 6.2197 | 37.86 | 56.13 | 1.8422 | 00.01 | 01.87 | 00.01 | 01.88\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "88e30212-929a-46ff-9072-3b6013ef55ac",
    "input": "## Claim\nHere is a claim: The results of using NeuralTD to generate summaries are in the bottommost row; the overall F-score is only lower by 1.4 for each metric. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.\nSystem | Reward | R-1 | R-2 | R-L\nKryscinski et\u00a0al. ( 2018 ) | R-L | 40.2 | 17.4 | 37.5\nNarayan et\u00a0al. ( 2018b ) | R-1,2,L | 40.0 | 18.2 | 36.6\nChen and Bansal ( 2018 ) | R-L | 41.5 | 18.7 | 37.8\nDong et\u00a0al. ( 2018 ) | R-1,2,L | 41.5 | 18.7 | 37.6\nZhang et\u00a0al. ( 2018 ) | [EMPTY] | 41.1 | 18.8 | 37.5\nZhou et\u00a0al. ( 2018 ) | [EMPTY] | 41.6 | 19.0 | 38.0\nKedzie et\u00a0al. ( 2018 ) | [EMPTY] | 39.1 | 17.9 | 35.9\n(ours) NeuralTD | Learned | 39.6 | 18.1 | 36.5\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "4e700d01-7b1d-422d-aa5b-e9fffa5f7cfb",
    "input": "## Claim\nHere is a claim: the joint-training strategy has more significant performance gains in recall from 0.1 to 0.4 than the fine-tuning strategy. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 1: Precisions on the NYT dataset.\nRecall | 0.1 | 0.2 | 0.3 | 0.4 | AUC\nPCNN+ATT | 0.698 | 0.606 | 0.518 | 0.446 | 0.323\nRank+ExATT | 0.789 | 0.726 | 0.620 | 0.514 | 0.395\nOur Model | 0.788 | [BOLD] 0.743 | [BOLD] 0.654 | [BOLD] 0.546 | [BOLD] 0.397\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "b55f3404-c386-4e10-a2c0-b686c31536b7",
    "input": "## Claim\nHere is a claim: This is expected as the joint model introduces a greater capacity to the model and, therefore, can deal with more complex entity coreference. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | <bold>71.2</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "20c21547-c9f4-4420-88ce-bf96dd5d0418",
    "input": "## Claim\nHere is a claim: DAMD (generated actions)  is the state-of-the-art for combining action modeling and belief state augmentation for task-oriented response generation. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.\nModel | Belief State Type | System Action Type | System Action Form | Inform (%) | Success (%) | BLEU | Combined Score\n1. Seq2Seq + Attention  | oracle | - | - | 71.3 | 61.0 | [BOLD] 18.9 | 85.1\n2. Seq2Seq + Copy | oracle | - | - | 86.2 | [BOLD] 72.0 | 15.7 | 94.8\n3. MD-Sequicity | oracle | - | - | [BOLD] 86.6 | 71.6 | 16.8 | [BOLD] 95.9\n4. SFN + RL (Mehri et al. mehri2019structured) | oracle | generated | one-hot | 82.7 | 72.1 | 16.3 | 93.7\n5. HDSA  | oracle | generated | graph | 82.9 | 68.9 | [BOLD] 23.6 | 99.5\n6. DAMD | oracle | generated | span | [BOLD] 89.5 | 75.8 | 18.3 | 100.9\n7. DAMD + multi-action data augmentation | oracle | generated | span | 89.2 | [BOLD] 77.9 | 18.6 | [BOLD] 102.2\n8. SFN + RL (Mehri et al. mehri2019structured) | oracle | oracle | one-hot | - | - | 29.0 | 106.0\n9. HDSA  | oracle | oracle | graph | 87.9 | 78.0 | [BOLD] 30.4 | 113.4\n10. DAMD + multi-action data augmentation | oracle | oracle | span | [BOLD] 95.4 | [BOLD] 87.2 | 27.3 | [BOLD] 118.5\n11. SFN + RL (Mehri et al. mehri2019structured) | generated | generated | one-hot | 73.8 | 58.6 | [BOLD] 16.9 | 83.0\n12. DAMD + multi-action data augmentation | generated | generated | span | [BOLD] 76.3 | [BOLD] 60.4 | 16.6 | [BOLD] 85.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "33601bd8-365d-423b-9207-1d62d6031441",
    "input": "## Claim\nHere is a claim: Again, one possible explanation is that cleaning the missing slots provided more complex training examples. Does the following context support or refute the claim?\n\n## Table\nPaper title: Improved Semantics for the End-to-End Generation Challenge Corpus\nTable caption: Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).\nTrain | Test | [BOLD] System | [BOLD] BLEU | [BOLD] NIST | [BOLD] METEOR | [BOLD] ROUGE-L | [BOLD] CIDEr | [BOLD] Add | [BOLD] Miss | [BOLD] Wrong | [BOLD] SER\nOriginal | [BOLD] Cleaned | TGen\u2212 | 36.85 | 5.3782 | 35.14 | 55.01 | 1.6016 | 00.34 | 09.81 | 00.15 | 10.31\nOriginal | [BOLD] Cleaned | TGen | 39.23 | 6.0217 | 36.97 | 55.52 | 1.7623 | 00.40 | 03.59 | 00.07 | 04.05\nOriginal | [BOLD] Cleaned | TGen+ | 40.25 | 6.1448 | 37.50 | 56.19 | 1.8181 | 00.21 | 01.99 | 00.05 | 02.24\nOriginal | [BOLD] Cleaned | SC-LSTM | 23.88 | 3.9310 | 32.11 | 39.90 | 0.5036 | 07.73 | 17.76 | 09.52 | 35.03\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen\u2212 | 40.19 | 6.0543 | 37.38 | 55.88 | 1.8104 | 00.17 | 01.31 | 00.25 | 01.72\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen | 40.73 | 6.1711 | 37.76 | 56.09 | 1.8518 | 00.07 | 00.72 | 00.08 | 00.87\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | TGen+ | 40.51 | 6.1226 | 37.61 | 55.98 | 1.8286 | 00.02 | 00.63 | 00.06 | 00.70\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned | [BOLD] Cleaned | SC-LSTM | 23.66 | 3.9511 | 32.93 | 39.29 | 0.3855 | 07.89 | 15.60 | 08.44 | 31.94\nCleaned missing | [BOLD] Cleaned | TGen\u2212 | 40.48 | 6.0269 | 37.26 | 56.19 | 1.7999 | 00.43 | 02.84 | 00.26 | 03.52\nCleaned missing | [BOLD] Cleaned | TGen | 41.57 | 6.2830 | 37.99 | 56.36 | 1.8849 | 00.37 | 01.40 | 00.09 | 01.86\nCleaned missing | [BOLD] Cleaned | TGen+ | 41.56 | 6.2700 | 37.94 | 56.38 | 1.8827 | 00.21 | 01.04 | 00.07 | 01.31\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen\u2212 | 35.99 | 5.0734 | 34.74 | 54.79 | 1.5259 | 00.02 | 11.58 | 00.02 | 11.62\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen | 40.07 | 6.1243 | 37.45 | 55.81 | 1.8026 | 00.05 | 03.23 | 00.01 | 03.29\n1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added | [BOLD] Cleaned | TGen+ | 40.80 | 6.2197 | 37.86 | 56.13 | 1.8422 | 00.01 | 01.87 | 00.01 | 01.88\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "51db7e0d-e291-45d8-bb0a-0210fa7cda1d",
    "input": "## Claim\nHere is a claim: [CONTINUE] Since the models have fewer examples of bigger graphs to learn from, this also leads to worse performance when handling graphs with higher diameters. Does the following context support or refute the claim?\n\n## Table\nPaper title: Enhancing AMR-to-Text Generation with Dual Graph Representations\nTable caption: Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.\n<bold>Model</bold> | <bold>Graph Diameter</bold> 0-7 \u0394 | <bold>Graph Diameter</bold> 7-13 \u0394 | <bold>Graph Diameter</bold> 14-20 \u0394\nS2S | 33.2 | 29.7 | 28.8\nG2S-GIN | 35.2 +6.0% | 31.8 +7.4% | 31.5 +9.2%\nG2S-GAT | 35.1 +5.9% | 32.0 +7.8% | 31.5 +9.51%\nG2S-GGNN | 36.2 +9.0% | 33.0 +11.4% | 30.7 +6.7%\n[EMPTY] | <bold>Sentence Length</bold> | <bold>Sentence Length</bold> | <bold>Sentence Length</bold>\n[EMPTY] | 0-20 \u0394 | 20-50 \u0394 | 50-240 \u0394\nS2S | 34.9 | 29.9 | 25.1\nG2S-GIN | 36.7 +5.2% | 32.2 +7.8% | 26.5 +5.8%\nG2S-GAT | 36.9 +5.7% | 32.3 +7.9% | 26.6 +6.1%\nG2S-GGNN | 37.9 +8.5% | 33.3 +11.2% | 26.9 +6.8%\n[EMPTY] | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold> | <bold>Max Node Out-degree</bold>\n[EMPTY] | 0-3 \u0394 | 4-8 \u0394 | 9-18 \u0394\nS2S | 31.7 | 30.0 | 23.9\nG2S-GIN | 33.9 +6.9% | 32.1 +6.9% | 25.4 +6.2%\nG2S-GAT | 34.3 +8.0% | 32.0 +6.7% | 22.5 -6.0%\nG2S-GGNN | 35.0 +10.3% | 33.1 +10.4% | 22.2 -7.3%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "eb4529a1-e468-4159-9afe-25270977f2dc",
    "input": "## Claim\nHere is a claim: On the TREC task, on the other hand, CMOW outperforms CBOW by 2.5 points. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nMethod | SUBJ | CR | MR | MPQA | MRPC | TREC | SICK-E | SST2 | SST5 | STS-B | SICK-R\nCBOW/784 | 90.0 | [BOLD] 79.2 | [BOLD] 74.0 | 87.1 | 71.6 | 85.6 | 78.9 | 78.5 | 42.1 | 61.0 | [BOLD] 78.1\nCMOW/784 | 87.5 | 73.4 | 70.6 | [BOLD] 87.3 | 69.6 | [BOLD] 88.0 | 77.2 | 74.7 | 37.9 | 56.5 | 76.2\nHybrid | [BOLD] 90.2 | 78.7 | 73.7 | [BOLD] 87.3 | [BOLD] 72.7 | 87.6 | [BOLD] 79.4 | [BOLD] 79.6 | [BOLD] 43.3 | [BOLD] 63.4 | 77.8\ncmp. CBOW | +0.2% | -0.6% | -0.4% | +0.2% | +1.5% | +2.3% | +0.6% | +1.4% | +2.9% | +3.9% | -0.4%\ncmp. CMOW | +3.1% | +7.2% | +4.4% | +0% | +4.5% | -0.5% | +2.9% | +6.7% | +14.3 | +12.2% | +2.1%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "017f3d64-a37c-4bcf-b972-a505fe7d5004",
    "input": "## Claim\nHere is a claim: Although the PRKGC+NS model receives supervision about human-generated NLDs, paths with the maximum score do not match human-generated NLDs to any significant extent. Does the following context support or refute the claim?\n\n## Table\nPaper title: RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension\nTable caption: Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).\nModel | Answerability Macro P/R/F | # Answerable | Answer Prec. | Derivation Prec. RG-L (P/R/F) | Derivation Prec. BL-4\nShortest Path | 54.8/55.5/53.2 | 976 | 3.6 | 56.7/38.5/41.5 | 31.3\nPRKGC | 52.6/51.5/50.7 | 1,021 | 45.2 | 40.7/60.7/44.7 | 30.9\nPRKGC+NS | 53.6/54.1/52.1 | 980 | 45.4 | 42.2/61.6/46.1 | 33.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "74589c10-ac86-4451-a306-ebab9d94558d",
    "input": "## Claim\nHere is a claim: On the other side, H-CMOW shows, among others, improvements at BShift. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "3fb56301-2d20-409e-adba-fc469f55b83a",
    "input": "## Claim\nHere is a claim: Surprisingly, GDPL outperforms human in completing the task, and its average dialog turns are even lower than those of humans, though GDPL is superior in terms of match rate. Does the following context support or refute the claim?\n\n## Table\nPaper title: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nTable caption: Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.\nMethod | Agenda Turns | Agenda Inform | Agenda Match | Agenda Success\nGP-MBCM | 2.99 | 19.04 | 44.29 | 28.9\nACER | 10.49 | 77.98 | 62.83 | 50.8\nPPO | 9.83 | 83.34 | 69.09 | 59.1\nALDM | 12.47 | 81.20 | 62.60 | 61.2\nGDPL-sess | [BOLD] 7.49 | 88.39 | 77.56 | 76.4\nGDPL-discr | 7.86 | 93.21 | 80.43 | 80.5\nGDPL | 7.64 | [BOLD] 94.97 | [BOLD] 83.90 | [BOLD] 86.5\n[ITALIC] Human | [ITALIC] 7.37 | [ITALIC] 66.89 | [ITALIC] 95.29 | [ITALIC] 75.0\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "fef58ae6-dbba-475a-a44e-29eff547da7d",
    "input": "## Claim\nHere is a claim: We find that the performance does not reach the best when iteration is set to 3. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nIteration=1 | 0.531 | 0.455 | 0.353 | 0.201\nIteration=2 | 0.592 | 0.498 | 0.385 | 0.375\nIteration=3 | 0.650 | 0.519 | 0.422 | 0.405\nIteration=4 | 0.601 | 0.505 | 0.422 | 0.385\nIteration=5 | 0.575 | 0.495 | 0.394 | 0.376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "edb918b8-c14e-4bdd-b9c3-0e99a12248e1",
    "input": "## Claim\nHere is a claim: [CONTINUE] We also observe that WMD-BIGRAMS slightly outperforms WMD-UNIGRAMS on 3 out of 4 language pairs. Does the following context support or refute the claim?\n\n## Table\nPaper title: MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance\nTable caption: Table 5: Comparison on hard and soft alignments.\nMetrics | cs-en | de-en | fi-en | lv-en\nRUSE | 0.624 | 0.644 | 0.750 | 0.697\nHmd-F1 + BERT | 0.655 | 0.681 | 0.821 | 0.712\nHmd-Recall + BERT | 0.651 | 0.658 | 0.788 | 0.681\nHmd-Prec + BERT | 0.624 | 0.669 | 0.817 | 0.707\nWmd-unigram + BERT | 0.651 | 0.686 | <bold>0.823</bold> | 0.710\nWmd-bigram + BERT | <bold>0.665</bold> | <bold>0.688</bold> | 0.821 | <bold>0.712</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "2e23ce40-52ea-404d-bf13-83d9dedb36a6",
    "input": "## Claim\nHere is a claim: Consequently, CMOW-R also outperforms CMOW-C on 10 out of 11 supervised [CONTINUE] downstream tasks [CONTINUE] On average over all downstream tasks, the relative improvement is 20.8%. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 5: Scores for different training objectives on the supervised downstream tasks.\nMethod | SUBJ | CR | MR | MPQA | MRPC | TREC | SICK-E | SST2 | SST5 | STS-B | SICK-R\nCMOW-C | 85.9 | 72.1 | 69.4 | 87.0 | [BOLD] 71.9 | 85.4 | 74.2 | 73.8 | 37.6 | 54.6 | 71.3\nCMOW-R | [BOLD] 87.5 | [BOLD] 73.4 | [BOLD] 70.6 | [BOLD] 87.3 | 69.6 | [BOLD] 88.0 | [BOLD] 77.2 | [BOLD] 74.7 | [BOLD] 37.9 | [BOLD] 56.5 | [BOLD] 76.2\nCBOW-C | [BOLD] 90.0 | [BOLD] 79.3 | [BOLD] 74.6 | [BOLD] 87.5 | [BOLD] 72.9 | 85.0 | [BOLD] 80.0 | 78.4 | 41.0 | 60.5 | [BOLD] 79.2\nCBOW-R | [BOLD] 90.0 | 79.2 | 74.0 | 87.1 | 71.6 | [BOLD] 85.6 | 78.9 | [BOLD] 78.5 | [BOLD] 42.1 | [BOLD] 61.0 | 78.1\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "6757f6db-0ade-4eaf-9769-d1a599048b35",
    "input": "## Claim\nHere is a claim: A complementary behavior can be observed for H-CBOW, whose scores on Word Content are increased. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "a0791422-37ec-4a81-bcf0-24e3040fc3cf",
    "input": "## Claim\nHere is a claim: we see that analogical reasoning abilities of the learned embeddings are almost close to the distributed word representations. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE VII: Precision scores for the Analogy Test\nMethods | # dims | Analg. (sem) | Analg. (syn) | Total\nGloVe | 300 | 78.94 | 64.12 | 70.99\nWord2Vec | 300 | 81.03 | 66.11 | 73.03\nOIWE-IPG | 300 | 19.99 | 23.44 | 21.84\nSOV | 3000 | 64.09 | 46.26 | 54.53\nSPINE | 1000 | 17.07 | 8.68 | 12.57\nWord2Sense | 2250 | 12.94 | 19.44 | 5.84\nProposed | 300 | 79.96 | 63.52 | 71.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "40dbe297-6a42-4c99-80d4-e1fe1f162ad7",
    "input": "## Claim\nHere is a claim: However, models trained using linguistic features on the training data do not obtain significantly higher predictive accuracy. Does the following context support or refute the claim?\n\n## Table\nPaper title: Automatically Identifying Complaints in Social Media\nTable caption: Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.\n[BOLD] Model | [BOLD] Acc | [BOLD] F1 | [BOLD] AUC\nMost Frequent Class | 64.2 | 39.1 | 0.500\nLogistic Regression | [EMPTY] | [EMPTY] | [EMPTY]\nSentiment \u2013 MPQA | 64.2 | 39.1 | 0.499\nSentiment \u2013 NRC | 63.9 | 42.2 | 0.599\nSentiment \u2013 V&B | 68.9 | 60.0 | 0.696\nSentiment \u2013 VADER | 66.0 | 54.2 | 0.654\nSentiment \u2013 Stanford | 68.0 | 55.6 | 0.696\nComplaint Specific (all) | 65.7 | 55.2 | 0.634\nRequest | 64.2 | 39.1 | 0.583\nIntensifiers | 64.5 | 47.3 | 0.639\nDowngraders | 65.4 | 49.8 | 0.615\nTemporal References | 64.2 | 43.7 | 0.535\nPronoun Types | 64.1 | 39.1 | 0.545\nPOS Bigrams | 72.2 | 66.8 | 0.756\nLIWC | 71.6 | 65.8 | 0.784\nWord2Vec Clusters | 67.7 | 58.3 | 0.738\nBag-of-Words | 79.8 | 77.5 | 0.866\nAll Features | [BOLD] 80.5 | [BOLD] 78.0 | [BOLD] 0.873\nNeural Networks | [EMPTY] | [EMPTY] | [EMPTY]\nMLP | 78.3 | 76.2 | 0.845\nLSTM | 80.2 | 77.0 | 0.864\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "60de77be-65de-40b2-825e-584f482018c5",
    "input": "## Claim\nHere is a claim: RSI  \u201c119.99\u201d  requires  \u201cRSI  <  120.00\u201d  and RSI = `89.20` therefore does not require. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE V: Word Intrusion Test Results: Correct Answers out of 300 Questions\n[EMPTY] | GloVe | Imparted\nParticipants 1 to 5 | 80/88/82/78/97 | 212/170/207/229/242\nMean/Std | 85/6.9 | 212/24.4\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "d5f3c5c5-453a-4fea-b639-0ab741aac988",
    "input": "## Claim\nHere is a claim: from this Table, we can clearly see the effect of exploring hierarchical structure is more significant at higher recall rates, so we can improve performance via attention mechanism at higher recall rate. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 2: Precisions on the Wikidata dataset.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nRank+ExATT | 0.584 | 0.535 | 0.487 | 0.392\nPCNN+ATT (m) | 0.365 | 0.317 | 0.213 | 0.204\nPCNN+ATT (1) | 0.665 | 0.517 | 0.413 | 0.396\nOur Model | 0.650 | 0.519 | 0.422 | [BOLD] 0.405\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "0aa1974d-d983-4dcb-9007-35874e8431fc",
    "input": "## Claim\nHere is a claim: We find that the performance reach the best when iteration is set to 3. Does the following context support or refute the claim?\n\n## Table\nPaper title: Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\nTable caption: Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.\nRecall | 0.1 | 0.2 | 0.3 | AUC\nIteration=1 | 0.531 | 0.455 | 0.353 | 0.201\nIteration=2 | 0.592 | 0.498 | 0.385 | 0.375\nIteration=3 | 0.650 | 0.519 | 0.422 | 0.405\nIteration=4 | 0.601 | 0.505 | 0.422 | 0.385\nIteration=5 | 0.575 | 0.495 | 0.394 | 0.376\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "98afd6c9-ab67-4bbb-afa2-4fd3d470b8ae",
    "input": "## Claim\nHere is a claim: we also removed the duplicate mentions identified by the lemmatisation-based method (reduced), and the effect was to boost cross-document results on the best of these sets (Joint+reduced) by a further 0.9% for all measures. Does the following context support or refute the claim?\n\n## Table\nPaper title: Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution\nTable caption: Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.\n<bold>Model</bold> | R | MUC P | <italic>F</italic>1 | R | B3 P | <italic>F</italic>1 | R | CEAF-<italic>e</italic> P | <italic>F</italic>1 | CoNLL <italic>F</italic>1\nCluster+Lemma | 71.3 | 83 | 76.7 | 53.4 | 84.9 | 65.6 | 70.1 | 52.5 | 60 | 67.4\nDisjoint | 76.7 | 80.8 | 78.7 | 63.2 | 78.2 | 69.9 | 65.3 | 58.3 | 61.6 | 70\nJoint | 78.6 | 80.9 | 79.7 | 65.5 | 76.4 | 70.5 | 65.4 | 61.3 | 63.3 | <bold>71.2</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "f15c9ba1-b45b-4353-9e9a-50abe7adc4a1",
    "input": "## Claim\nHere is a claim: compared to GloVe and Word2Vec, our sense-based distributed representations can be considered as an initial attempt to incorporate sense-level information. Does the following context support or refute the claim?\n\n## Table\nPaper title: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure\nTable caption: TABLE IX: Accuracies (%) for Sentiment Classification Task\nGloVe | Word2Vec | OIWE-IPG | SOV | SPINE | Word2Sense | Proposed\n77.34 | 77.91 | 74.27 | 78.43 | 74.13 | 81.21 | 78.26\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "9bf538d5-cb82-4db3-a4ba-e64ae1c67891",
    "input": "## Claim\nHere is a claim: BERT cosine performs the best. Does the following context support or refute the claim?\n\n## Table\nPaper title: Better Rewards Yield Better Summaries: Learning to Summarise Without References\nTable caption: Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.\nMetric | [ITALIC] \u03c1 | [ITALIC] r | G-Pre | G-Rec\nROUGE-1 | .290 | .304 | .392 | .428\nROUGE-2 | .259 | .278 | .408 | .444\nROUGE-L | .274 | .297 | .390 | .426\nROUGE-SU4 | .282 | .279 | .404 | .440\nBLEU-1 | .256 | .281 | .409 | .448\nBLEU-2 | .301 | .312 | .411 | .446\nBLEU-3 | .317 | .312 | .409 | .444\nBLEU-4 | .311 | .307 | .409 | .446\nBLEU-5 | .308 | .303 | .420 | .459\nMETEOR | .305 | .285 | .409 | .444\nInferSent-Cosine | [BOLD] .329 | [BOLD] .339 | .417 | .460\nBERT-Cosine | .312 | .335 | [BOLD] .440 | [BOLD] .484\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "617855fc-ec87-4254-aded-7cee3956b79e",
    "input": "## Claim\nHere is a claim: For a training set of 0.9M training examples, the proposed method reaches comparable classification performance to a BiLSTM approach. Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 7: Negation classifier performance for scope detection with gold cues and scope.\n[EMPTY] | [BOLD] Punctuation | [BOLD] BiLSTM | [BOLD] Proposed\nIn-scope (F) | 0.66 | 0.88 | 0.85\nOut-scope (F) | 0.87 | 0.97 | 0.97\nPCS | 0.52 | 0.72 | 0.72\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "e88c610b-a5ef-4041-9abb-e72a4de16777",
    "input": "## Claim\nHere is a claim: our evaluation F1-score (Macro) is 82.28%, which is slightly lower than those reported in [23] (87.5%). Does the following context support or refute the claim?\n\n## Table\nPaper title: Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations\nTable caption: Table 8: Sentiment classification evaluation, using different classifiers on the test set.\nClassifier | Positive Sentiment Precision | Positive Sentiment Recall | Positive Sentiment Fscore\nSVM-w/o neg. | 0.57 | 0.72 | 0.64\nSVM-Punct. neg. | 0.58 | 0.70 | 0.63\nSVM-our-neg. | 0.58 | 0.73 | 0.65\nCNN | 0.63 | 0.83 | 0.72\nCNN-LSTM | 0.71 | 0.72 | 0.72\nCNN-LSTM-Our-neg-Ant | [BOLD] 0.78 | [BOLD] 0.77 | [BOLD] 0.78\n[EMPTY] | Negative Sentiment | Negative Sentiment | Negative Sentiment\n[EMPTY] | Precision | Recall | Fscore\nSVM-w/o neg. | 0.78 | 0.86 | 0.82\nSVM-Punct. neg. | 0.78 | 0.87 | 0.83\nSVM-Our neg. | 0.80 | 0.87 | 0.83\nCNN | 0.88 | 0.72 | 0.79\nCNN-LSTM. | 0.83 | 0.83 | 0.83\nCNN-LSTM-our-neg-Ant | [BOLD] 0.87 | [BOLD] 0.87 | [BOLD] 0.87\n[EMPTY] | Train | [EMPTY] | Test\nPositive tweets | 5121 | [EMPTY] | 1320\nNegative tweets | 9094 | [EMPTY] | 2244\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "7219f414-9986-4c5d-aea4-67f4f4dc1ed0",
    "input": "## Claim\nHere is a claim: Lemma-based targets do not significantly outperform type-based targets in terms of F-measure in all cases. Does the following context support or refute the claim?\n\n## Table\nPaper title: From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources\nTable caption: Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.\n[EMPTY] | WN-N P | WN-N R | WN-N F | WN-V P | WN-V R | WN-V F | VN P | VN R | VN F\nContext: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2 | Context: w2\ntype | .700 | .654 | .676 | .535 | .474 | .503 | .327 | .309 | .318\nx+POS | .699 | .651 | .674 | .544 | .472 | .505 | .339 | .312 | .325\nlemma | .706 | .660 | .682 | .576 | .520 | .547 | .384 | .360 | .371\nx+POS | <bold>.710</bold> | <bold>.662</bold> | <bold>.685</bold> | <bold>.589</bold> | <bold>.529</bold> | <bold>.557</bold> | <bold>.410</bold> | <bold>.389</bold> | <bold>.399</bold>\nContext: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep | Context: dep\ntype | .712 | .661 | .686 | .545 | .457 | .497 | .324 | .296 | .310\nx+POS | .715 | .659 | .686 | .560 | .464 | .508 | .349 | .320 | .334\nlemma | <bold>.725</bold> | <bold>.668</bold> | <bold>.696</bold> | .591 | .512 | .548 | .408 | .371 | .388\nx+POS | .722 | .666 | .693 | <bold>.609</bold> | <bold>.527</bold> | <bold>.565</bold> | <bold>.412</bold> | <bold>.381</bold> | <bold>.396</bold>\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "refutes"
  },
  {
    "id": "558e297e-a6a9-4acc-9cbd-745799ce92bb",
    "input": "## Claim\nHere is a claim: for example, for [cue:was] the 61.8% of the outcome categories are produced by instances whose premise begins with [cue:was]. Does the following context support or refute the claim?\n\n## Table\nPaper title: When Choosing Plausible Alternatives, Clever Hans can be Clever\nTable caption: Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.\nCue | App. | Prod. | Cov.\nin | 47 | 55.3 | 9.40\nwas | 55 | 61.8 | 11.0\nto | 82 | 40.2 | 16.4\nthe | 85 | 38.8 | 17.0\na | 106 | 57.5 | 21.2\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "c77a5686-dae1-4f25-896b-87bf07e2494a",
    "input": "## Claim\nHere is a claim: The hybrid model yields scores close to or even above the better model of the two on all tasks. Does the following context support or refute the claim?\n\n## Table\nPaper title: CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model\nTable caption: Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.\nDim | Method | Depth | BShift | SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC\n400 | CBOW/400 | 32.5 | 50.2 | 78.9 | 78.7 | 53.6 | 73.6 | 79.0 | 69.6 | 48.9 | 86.7\n400 | CMOW/400 | [BOLD] 34.4 | 68.8 | 80.1 | [BOLD] 79.9 | [BOLD] 59.8 | 81.9 | [BOLD] 79.2 | [BOLD] 70.7 | [BOLD] 50.3 | 70.7\n400 | H-CBOW | 31.2 | 50.2 | 77.2 | 78.8 | 52.6 | 77.5 | 76.1 | 66.1 | 49.2 | [BOLD] 87.2\n400 | H-CMOW | 32.3 | [BOLD] 70.8 | [BOLD] 81.3 | 76.0 | 59.6 | [BOLD] 82.3 | 77.4 | 70.0 | 50.2 | 38.2\n784 | CBOW/784 | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | [BOLD] 89.5\n784 | CMOW/784 | [BOLD] 35.1 | [BOLD] 70.8 | [BOLD] 82.0 | 80.2 | [BOLD] 61.8 | 82.8 | [BOLD] 79.7 | 74.2 | [BOLD] 50.7 | 72.9\n800 | Hybrid | 35.0 | [BOLD] 70.8 | 81.7 | [BOLD] 81.0 | 59.4 | [BOLD] 84.4 | 79.0 | [BOLD] 74.3 | 49.3 | 87.6\n- | cmp. CBOW | +6.1% | +42.7% | +3% | +3.3% | +10.8% | +13.3% | +0.5% | +3.2% | -0.6% | -2.1%\n- | cmp. CMOW | -0.3% | +-0% | -0.4% | +1% | -3.9% | +1.9% | -0.9% | +0.1% | -2.8% | +20.9%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "7a288829-0028-438d-a925-79cb2943fdc4",
    "input": "## Claim\nHere is a claim: [CONTINUE] MIL-ND significantly outperforms MIL: the 95% confidence intervals for them do not overlap. Does the following context support or refute the claim?\n\n## Table\nPaper title: Distant Learning for Entity Linking with Automatic Noise Detection\nTable caption: Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.\nSystem | All P | All R | All F1 | In  [ITALIC] E+ P | In  [ITALIC] E+ R | In  [ITALIC] E+ F1\nName matching | 15.03 | 15.03 | 15.03 | 29.13 | 29.13 | 29.13\nMIL (model 1) | 35.87 | 35.87 | 35.87 \u00b10.72 | 69.38 | 69.38 | 69.38 \u00b11.29\nMIL-ND (model 2) | 37.42 | [BOLD] 37.42 | 37.42 \u00b10.35 | 72.50 | [BOLD] 72.50 | [BOLD] 72.50 \u00b10.68\n[ITALIC] \u03c4MIL-ND (model 2) | [BOLD] 38.91 | 36.73 | [BOLD] 37.78 \u00b10.26 | [BOLD] 73.19 | 71.15 | 72.16 \u00b10.48\nSupervised learning | 42.90 | 42.90 | 42.90 \u00b10.59 | 83.12 | 83.12 | 83.12 \u00b11.15\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "supports"
  },
  {
    "id": "d864633b-50b5-40b7-b5e3-1818ef277bfb",
    "input": "## Claim\nHere is a claim: the results in Table 1 clearly show that the action number threshold, being the simplest, achieves the worst performance on almost all metrics. Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.\nModel & Decoding Scheme | Act # w/o | Act # w/ | Slot # w/o | Slot # w/\nSingle-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines | Single-Action Baselines\nDAMD + greedy | [BOLD] 1.00 | [BOLD] 1.00 | 1.95 | [BOLD] 2.51\nHDSA + fixed threshold | [BOLD] 1.00 | [BOLD] 1.00 | 2.07 | [BOLD] 2.40\n5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation | 5-Action Generation\nDAMD + beam search | 2.67 | [BOLD] 2.87 | 3.36 | [BOLD] 4.39\nDAMD + diverse beam search | 2.68 | [BOLD] 2.88 | 3.41 | [BOLD] 4.50\nDAMD + top-k sampling | 3.08 | [BOLD] 3.43 | 3.61 | [BOLD] 4.91\nDAMD + top-p sampling | 3.08 | [BOLD] 3.40 | 3.79 | [BOLD] 5.20\nHDSA + sampled threshold | 1.32 | [BOLD] 1.50 | 3.08 | [BOLD] 3.31\n10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation | 10-Action Generation\nDAMD + beam search | 3.06 | [BOLD] 3.39 | 4.06 | [BOLD] 5.29\nDAMD + diverse beam search | 3.05 | [BOLD] 3.39 | 4.05 | [BOLD] 5.31\nDAMD + top-k sampling | 3.59 | [BOLD] 4.12 | 4.21 | [BOLD] 5.77\nDAMD + top-p sampling | 3.53 | [BOLD] 4.02 | 4.41 | [BOLD] 6.17\nHDSA + sampled threshold | 1.54 | [BOLD] 1.83 | 3.42 | [BOLD] 3.92\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  },
  {
    "id": "1dbdb0d9-43e6-49b1-972c-03f0c29e7fb8",
    "input": "## Claim\nHere is a claim: the data augmentation strategy significantly improves the human evaluation performance (Wilcoxon signed-rank test, p-value Does the following context support or refute the claim?\n\n## Table\nPaper title: Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context\nTable caption: Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.\nModel | Diversity | App | Good% | OK% | Invalid%\nDAMD | 3.12 | 2.50 | 56.5% | [BOLD] 37.4% | 6.1%\nDAMD (+) | [BOLD] 3.65 | [BOLD] 2.53 | [BOLD] 63.0% | 27.1% | 9.9%\nHDSA (+) | 2.14 | 2.47 | 57.5% | 32.5% | [BOLD] 10.0%\n\n## Task:\nYou will answer the question based on the given context.You should reach a short-form answer after reasoning.You are asked to answer the question in three steps.\n1. Analyze the question and the given context. Think about what information should be extracted from the table to answer the question.\n2. Write a SQL to query the table and output expected SQL execution result\n3. Reason step-by-step to reach the final answer. Answer \"NOT ENOUGH INFO\" if you cannot reach a final answer.\n\n## Answer:\n",
    "answer": "not enough info"
  }
]